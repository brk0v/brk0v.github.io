[{"id":0,"href":"/docs/fd-pipe-session-terminal/0-sre-should-know-about-gnu-linux-shell-related-internals-file-descriptors-pipes-terminals-user-sessions-process-groups-and-daemons/","title":"GNU/Linux shell related internals","section":"GNU/Linux shell related internals","content":" What every SRE should know about GNU/Linux shell related internals: file descriptors, pipes, terminals, user sessions, process groups and daemons # Despite the era of containers, virtualization, and the rising number of UI of all kinds, SREs often spend a significant part of their time in GNU/Linux shells. It could be debugging, testing, developing, or preparing the new infrastructure. It may be the good old bash, the more recent and fancy zsh, or even fish or tcsh with their interesting and unique features.\nBut it is common nowadays how little people know about the internals of their shells, terminals, and relations between processes. All are taken primarily for granted without really thinking about such aspects.\nHave you ever thought about how a shell pipe works, how pressing the CTRL+C combination delivers the interrupt signal to the currently running shell processes, or how vim rewrites the content of the console when you change the size of your terminal window?\nI want to show you some indeed neat parts of pipes, file descriptors, shells, terminals, processes, jobs, and signals in this series of posts. We’ll touch on how all of them interact with each other to build a responsible, simple, and reliable environment. And all of this, of course, will be shown in the context of the Linux kernel, its internals, and various debugging tools and approaches.\nWe are going to play with file descriptors, pipes, different tools such as nohup and pv, experiment with background and foreground processes, understand how tmux gives us the ability to continue where we stopped, why and how the CTRL+C interrupts the currently running pipeline of commands and much much more. Also, we will use strace to trace syscalls, read the Linux kernel source code, and use bpftrace to get under the hood of arbitrary kernel functions.\nPrepare environment # During the series, I’ll mix python and golang for my examples. Also, we’ll need a file for our experiments. I use /var/tmp/file1.db. You can easily generate it using the following command:\n$ dd if=/dev/random of=/var/tmp/file1.db count=100 bs=1M Dive # With all that said, let\u0026rsquo;s learn, experiment, and have fun.\nRead next chapter → "},{"id":1,"href":"/docs/page-cache/0-linux-page-cache-for-sre/","title":"Linux Page Cache for SRE","section":"Linux Page Cache series","content":" SRE deep dive into Linux Page Cache # In this series of articles, I would like to talk about Linux Page Cache. I believe that the following knowledge of the theory and tools is essential and crucial for every SRE. This understanding can help both in usual and routine everyday DevOps-like tasks and in emergency debugging and firefighting. Page Cache is often left unattended, and its better understanding leads to the following:\nmore precise capacity planning and container limit calculations; better debugging and investigation skills for memory and disk intensive applications such as database management system and file sharing storages; building safe and predictable runtimes for memory and/or IO-bound ad-hoc tasks (for instance: backups and restore scripts, rsync one-liners, etc.). I’ll display what utils you should keep in mind when you\u0026rsquo;re dealing with Page Cache related tasks and problems, how to use them properly to understand real memory usage, and how to reveal issues with them. I will try to give you some examples of using these tools that are close to real life situations. Here are some of these tools I\u0026rsquo;m talking about below: vmtouch, perf, cgtouch, strace , sar and page-type.\nAlso, as the title says, “deep dive”, the internals of these utils will be shown with an emphasis on the Page Cache stats, events, syscalls and kernel interfaces. Here are some examples of what I’m touching on in the following post:\nprocfs files: /proc/PID/smaps, /proc/pid/pagemap, /proc/kpageflags, /proc/kpagecgroup and sysfs file: /sys/kernel/mm/page_idle; system calls: mincore(), mmap(), fsync(), msync(), posix_fadvise(), madvise() and others; different open and advise flags O_SYNC, FADV_DONTNEED, POSIX_FADV_RANDOM, MADV_DONTNEED, etc. I’ll try to be as verbose as possible with simple (almost all the way) code examples in Python, Go and a tiny bit of C.\nAnd finally, any conversations about modern GNU/Linux systems can’t be fully conducted without touching the cgroup (v2 in our case) and the systemd topics. I\u0026rsquo;ll show you how to leverage them to get the most out of the systems, build reliable, well-observed, controlled services, and sleep well at night while on-call.\nReaders should be confident if they have middle GNU/Linux knowledge and basic programming skills.\nAll code examples larger than 5 lines can be found on github: https://github.com/brk0v/sre-page-cache-article.\nRead next chapter → "},{"id":2,"href":"/docs/fd-pipe-session-terminal/1-file-descriptor-and-open-file-description/","title":"File descriptor and open file description","section":"GNU/Linux shell related internals","content":" File descriptor and open file description # First of all, I want to touch on the two fundamental concepts of working with files:\nfile descriptor; open file description. These two abstractions are crucial for understanding the internals of a process creation, communication, and data transition.\nThe first concept is a file descriptor or fd. It’s a positive integer number used by file system calls instead of a file path in order to make a variety of operations. Every process has its own file descriptor table (see Image 1 below). The main idea of a file descriptor is to decouple a file path (or, more correctly, an inode with minor and major device numbers) from a file object inside a process and the Linux kernel. This allows software developers to open the same file an arbitrary number of times for different purposes, with various flags (for instance: O_DIRECT, O_SYNC, O_APPEND, etc.), and at different offsets.\nFor example, a program wants to read from and write to one file in two separate places. In this case, it needs to open the file twice. Thus, two new file descriptors will refer to 2 different entries in the system-wide open file description table.\nIn its turn, the open file description table is a system-wide kernel abstraction. It stores the file open status flags (man 2 open) and the file positions (we can use man 2 lseek to change this position).\nFrankly speaking, there is no such thing inside the Linux kernel where we can find the open file description table. To be more accurate, every created process in the kernel has a per-thread struct task_struct. This struct has a pointer to another structure called the files_struct, and that contains an array of pointers to a file struct. This final struct is actually what holds all file flags, a current position, and a lot of other information about the open file: such as its type, inode, device, etc. All such entries among all running threads are what we call the open file descriptor table.\nSo, now let’s see how we can create entities in these two tables. In order to create a new entry in the open file description table we need to open a file with one of the following syscalls: open, openat, create, open2 (man 2 open). These functions also add a corresponding entry in the file descriptor table of the calling process, build a reference between the open file description table entry and the file descriptor table, and return the lowest positive number not currently opened by the calling process. The latest statement is very important to remember and understand because it means that a fd number can be reused during the process life if it opens and closes files in an arbitrary order.\nLinux kernel also provides an API to create copies of a file descriptor within a process. We will discuss why this technique can be helpful in a few minutes. For now, let’s just list them here: dup, dup2, dup3 (man 2 dup) and fcntl (man 2 fcntl) with F_DUPFD flag. All these syscalls create a new reference in the fd table of the process to the existing entry in the system-wide open file description table.\nLet’s take a closer look at an example in the image below with a snapshot of a system state. The image shows us possible relations among all the above components.\nImage 1. – Relations between process file descriptors, system-wide open file description table and files ❶ – The first three file descriptors (stdin, stdout and stderr) are special file descriptors. We will work with them later in this post. This example shows that all three point to a pseudoterminal (/dev/pts/0). These files don’t have positions due to their character device type. Thus process_1 and process_2 must be running under the terminal sessions. Please, note that the stdout of the process_2 (fd 1) points to the file on a disk /tmp/out.log. This is an example of shell redirection; we will discuss it later.\n❷ – Some file descriptors can have per-process flags. Nowadays, there is only one such flag: close-on-exec (O_CLOEXEC). We will discuss it later in this section and answer why it’s so unique. But for now, you should understand that some file descriptors may have it for the same system-wide open file description table entries. For instance: process_1 and its fd 9 and process_2 and its fd 3.\n❸ – Even though the file descriptor algorithm constantly reuses the file descriptors and allocates them sequentially from the lowest available, it doesn’t mean that there can be no gaps. For example, the fd 9 of the process_1 goes after fd 3. Some files, which used fd 4, 5, 6 and 7, could already be closed. Another way of achieving such a picture can be an explicit duplication of a file descriptor with dup2, dup3 or fcntl with F_DUPFD. Using these syscalls, we can specify the wanted file descriptor number. We will see later how it works in the chapter about the duplication of fds.\n❹ – A process can have more than one file descriptor that points to the same entry in the open file descriptions. System calls dup, dup2, dup3 and fcntl with F_DUPFD help with that. The fd 0 and fd 2 of the process_2 refer to the same pseudo terminal entry.\n❺ – Sometimes, one of the standard file descriptors might be pointed to a regular file (or pipe) and not a terminal. In this example, the stdout of the process_2 refers to a file on disk /tmp/out.txt.\n❻ – It’s possible to point file descriptors from various processes to the same entry in the system-wide open file description table. This is usually achieved by a fork call and inheriting file descriptors from the parent to its child. But there are other ways, which we’ll see later in this chapter. These descriptors could also have different int fd numbers inside processes and different process flags (O_CLOEXEC). For instance, fd 9 of process_1 and fd 3 of process_2.\n❼ – I put the file path here for simplicity. Instead, Linux kernel uses inode numbers, minor and major numbers of a device.\n❽ – Often, for a shell, the 0,1 and 2 file descriptors are pointed to a pseudo-terminal.\n❾ – Multiple open file descriptor entries can be linked with the same file on disk. The kernel allows us to open a file with different flags and at various offset positions.\nstdin, stdout and stderr # The first three file descriptors of processes are treated differently by shells and other programs. These fds also have well-known aliases:\n0 – stdin 1 – stdout 2 – stderr For a process started and running within a terminal session, these fds can be pointed to a pseudoterminal, a terminal, a file, a pipe, etc. For classical-UNIX-style daemons, they usually refer to a /dev/null device.\nLater in this series, I\u0026rsquo;ll show how this works in shells and why we must be careful with these three fds when working with long-running background processes.\nProcfs and file descriptors # The kernel exposes all open file descriptors of a process with the virtual procfs file system. So in order to get information about the open files for the current shell process, we can use a shell variable $$ with its PID. For instance:\n$ ls -l /proc/$$/fd/ lrwx------ 1 vagrant vagrant 64 Jul 9 21:15 0 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 9 21:15 1 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 9 21:15 2 -\u0026gt; /dev/pts/0 We can see only pseudoterminal /dev/pts/0 here. We will talk more about them a bit later.\nAnother useful directory in the procfs is the fdinfo folder under the process directory. It contains per file descriptor info. For example, for the stdin of the current shell process:\n$ cat /proc/$$/fdinfo/0 pos:\t0 flags: 02 mnt_id: 28 Keep in mind that the flags section here contains only the status flags (man 2 open). Let’s use it to write a tool to decode this flag mask to human-readable flags:\nimport os import sys pid = sys.argv[1] fd = sys.argv[2] with open(f\u0026#34;/proc/{pid}/fdinfo/{fd}\u0026#34;, \u0026#34;r\u0026#34;) as f: flags = f.readlines()[1].split(\u0026#34;\\t\u0026#34;)[1].strip() print(f\u0026#34;Flags mask: {flags}\u0026#34;) flags = int(flags, 8) # check status flags if flags \u0026amp; os.O_RDONLY: print(\u0026#34;os.O_RDONLY is set\u0026#34;) if flags \u0026amp; os.O_WRONLY: print(\u0026#34;os.O_WRONLY is set\u0026#34;) if flags \u0026amp; os.O_RDWR: print(\u0026#34;os.O_RDWR is set\u0026#34;) if flags \u0026amp; os.O_APPEND: print(\u0026#34;os.O_APPEND is set\u0026#34;) if flags \u0026amp; os.O_DSYNC: print(\u0026#34;os.O_DSYNC is set\u0026#34;) if flags \u0026amp; os.O_RSYNC: print(\u0026#34;os.O_RSYNC is set\u0026#34;) if flags \u0026amp; os.O_SYNC: print(\u0026#34;os.O_SYNC is set\u0026#34;) if flags \u0026amp; os.O_NDELAY: print(\u0026#34;os.O_NDELAY is set\u0026#34;) if flags \u0026amp; os.O_NONBLOCK: print(\u0026#34;os.O_NONBLOCK is set\u0026#34;) if flags \u0026amp; os.O_ASYNC: print(\u0026#34;os.O_ASYNC is set\u0026#34;) if flags \u0026amp; os.O_DIRECT: print(\u0026#34;os.O_DIRECT is set\u0026#34;) if flags \u0026amp; os.O_NOATIME: print(\u0026#34;os.O_NOATIME is set\u0026#34;) if flags \u0026amp; os.O_PATH: print(\u0026#34;os.O_PATH is set\u0026#34;) # check close on exec if flags \u0026amp; os.O_CLOEXEC: print(\u0026#34;os.O_CLOEXEC is set\u0026#34;) Out test program, which opens a file with some status flags:\nimport os import sys import time file_path = sys.argv[1] print(os.getpid()) fd = os.open(file_path, os.O_APPEND | os.O_RSYNC | os.O_NOATIME ) with os.fdopen(fd, \u0026#34;r+\u0026#34;) as f: print(f.fileno()) time.sleep(9999) Let’s run it:\n$ python3 ./open.py /tmp/123.txt 925 3 And run our tool:\n$ python3 ./flags.py 925 3 Flags mask: 07112000 os.O_APPEND is set os.O_DSYNC is set os.O_RSYNC is set os.O_SYNC is set os.O_NOATIME is set os.O_CLOEXEC is set Some flags in the kernel are aliases to other flags. That’s why we see more flags here.\nAnother example is if we run our tool with a socket fd (I used nginx process):\n$ sudo python3 ./flags.py 943 6 Flags mask: 02004002 os.O_RDWR is set os.O_NDELAY is set os.O_NONBLOCK is set os.O_CLOEXEC is set We can see that the socket is in nonblocking mode: O_NONBLOCK is set.\nSharing file descriptors between parent and child after fork() # Another important concept of file descriptors is how they behave with fork() (man 2 fork) and clone() (man 2 clone) system calls.\nAfter a fork() or a clone() (without CLONE_FILES set) call, a child and a parent have an equal set of file descriptors, which refer to the same entries in the system-wide open file description table. It means they share identical file positions, status flags and process fd flags (O_CLOEXEC)\nLet’s start with an example where 2 processes are not relatives. Both open the same file and get the same integer number for their fd. But because they both call open() independently, these two references to the open file description table will differ. After the file opening, the first example process makes a lseek() (man 2 lseek) at one position, and another program makes a lseek() call for the same file but at a different place. These actions don’t affect each other.\nCode:\nimport time import os import sys print(f\u0026#34;pid: {os.getpid()}\u0026#34;) with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;r\u0026#34;) as f: print(f.fileno()) f.seek(int(sys.argv[1])) time.sleep(99999) Run them in 2 different terminals:\n$ python3 ./file1.py 100 # \u0026lt;----------- lseek() to 100 bytes pid: 826 3 $ python3 ./file1.py 200 # \u0026lt;----------- lseek() to 200 bytes pid: 827 3 Now check procfs:\n$ ls -l /proc/826/fd lrwx------ 1 vagrant vagrant 64 Jul 9 21:18 0 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 9 21:18 1 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 9 21:18 2 -\u0026gt; /dev/pts/0 lr-x------ 1 vagrant vagrant 64 Jul 9 21:18 3 -\u0026gt; /var/tmp/file1.db \u0026lt;--------- $ ls -l /proc/827/fd lrwx------ 1 vagrant vagrant 64 Jul 9 21:18 0 -\u0026gt; /dev/pts/1 lrwx------ 1 vagrant vagrant 64 Jul 9 21:18 1 -\u0026gt; /dev/pts/1 lrwx------ 1 vagrant vagrant 64 Jul 9 21:18 2 -\u0026gt; /dev/pts/1 lr-x------ 1 vagrant vagrant 64 Jul 9 21:18 3 -\u0026gt; /var/tmp/file1.db \u0026lt;--------- We have the same file path and the same file descriptor number. Now verify that the positions are different because we have unrelated open file descriptions:\n$ cat /proc/826/fdinfo/3 pos:\t100 \u0026lt;------------------------ flags: 02100000 mnt_id: 26 $ cat /proc/827/fdinfo/3 pos:\t200 \u0026lt;------------------------ flags: 02100000 mnt_id: 26 Let’s now see how the file positions will behave after a fork() call between a parent process and its child. We open a file in a parent process, fork(), make lseek() in the child, and check whether the positions are the same or not.\nimport time import os import sys with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;r\u0026#34;) as f: print(f.fileno()) print(f\u0026#34;parent pid: {os.getpid()}\u0026#34;) pid = os.fork() if not pid: # child print(f\u0026#34;child pid: {os.getpid()}\u0026#34;) f.seek(int(sys.argv[1])) time.sleep(99999) os.waitpid(pid, 0) Run it:\n$ python3 ./file2.py 100 # \u0026lt;----------- lseek() to 100 bytes 3 parent pid: 839 child pid: 840 Our procfs picture:\n$ ls -l /proc/839/fd/ lrwx------ 1 vagrant vagrant 64 Jul 9 21:23 0 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 9 21:23 1 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 9 21:23 2 -\u0026gt; /dev/pts/0 lr-x------ 1 vagrant vagrant 64 Jul 9 21:23 3 -\u0026gt; /var/tmp/file1.db \u0026lt;--------- $ ls -l /proc/840/fd/ lrwx------ 1 vagrant vagrant 64 Jul 9 21:23 0 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 9 21:23 1 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 9 21:23 2 -\u0026gt; /dev/pts/0 lr-x------ 1 vagrant vagrant 64 Jul 9 21:23 3 -\u0026gt; /var/tmp/file1.db \u0026lt;--------- $ cat /proc/839/fdinfo/3 pos:\t100 \u0026lt;--------- 100 bytes flags: 02100000 mnt_id: 26 $ cat /proc/840/fdinfo/3 pos:\t100 \u0026lt;--------- 100 bytes flags: 02100000 mnt_id: 26 The primary purpose of such sharing is to protect files from being overwritten by children and its parent process. If all relatives start writing to a file simultaneously, the Linux kernel will sort this out and won’t lose any data because it’ll hold the lock and update the offset after each write. It’s worth mentioning that the data can appear in the file in a mixed way due to the CPU scheduler, arbitrary sizes of write buffers, and the amount of data to write.\nIf it’s not what you want, you should close all file descriptors after a successful fork(), including the three standard ones. This is basically how the classical daemons usually start. We will talk about them later in this series of posts.\nDuplication of file descriptors # We already know that we can open a new file in order to create a new file descriptor within the process. But it’s not always needed. Usually it’s handy to copy the existing fd to another one.\nLet’s start with the existing kernel API. We have a bunch of syscalls to duplicate fd:\ndup() – creates a new fd using the lowest unused int number. It usually follows the close() syscall for the one of standard fd (stdin, stdout, stderr) in order to replace it. dup2() – does the same as above but has a second argument. Here we can specify the target fd. If the target fd already exists, the dup2() closes it first. All dup2() operations are atomic. dup3() – does the same as the dup2() but has a third parameter, where the O_CLOEXEC flag can be set. fcntl() with F_DUPFD flag behaves as dup2() with one exception: if the target fd exists, it uses the next one instead of closing it. When dup(), dup2(), or fcntl() are used to create a duplicate of a file descriptor, the close-on-exec (O_CLOEXEC) flag is always reset for the duplicate fd.\nWe can in theory open the file twice with the O_APPEND flag and don’t use the duplication syscalls at all. In the following example O_APPEND flag preserves the strace tool from overwriting data in the results.log file by its concurrent writes from the stdout and stderr:\n$ strace 1\u0026gt;\u0026gt;results.log 2\u0026gt;\u0026gt;results.log where 1\u0026gt;\u0026gt; and 2\u0026gt;\u0026gt; are append shell redirections for stdout and stderr.\nBut if we use a shell pipe, the following example will only work with fd duplication logic. Pipes don’t have O_APPEND open flag, and they are much convenient for the redirection task (I’m covering the power of pipes later in the chapter 2 where you can find more justifications for the below technique):\n$ strace 2\u0026gt;\u0026amp;1 | less Let’s write an example that shows all the power of fd duplication:\nimport os import time print(f\u0026#34;{os.getpid()}\u0026#34;) fd1 = os.open(\u0026#34;/var/tmp/file1.db\u0026#34;, os.O_RDONLY, 777) fd2 = os.dup(fd1) fd3 = os.dup2(fd1, 999) os.lseek(fd3, 100, 0) time.sleep(9999) We opened one file, duplicate it several times, change the file position and it’s changed for all of the fs:\n$ ls -la /proc/2129/fd lrwx------ 1 vagrant vagrant 64 Aug 6 19:52 0 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Aug 6 19:52 1 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Aug 6 19:52 2 -\u0026gt; /dev/pts/0 lr-x------ 1 vagrant vagrant 64 Aug 6 19:52 3 -\u0026gt; /var/tmp/file1.db lr-x------ 1 vagrant vagrant 64 Aug 6 19:52 4 -\u0026gt; /var/tmp/file1.db lr-x------ 1 vagrant vagrant 64 Aug 6 19:52 999 -\u0026gt; /var/tmp/file1.db $ cat /proc/2129/fdinfo/999 pos:\t100 \u0026lt;------------ position flags: 0100000 mnt_id: 26 $ cat /proc/2129/fdinfo/3 pos:\t100 \u0026lt;------------ position flags: 02100000 mnt_id: 26 $ cat /proc/2129/fdinfo/4 pos:\t100 \u0026lt;------------ position flags: 02100000 mnt_id: 26 Execve() and file descriptors # Now let’s talk what may happen with file descriptors during the execve() system call (man 2 execve).\nJust to start, execve() is the only way the Linux kernel can start a new program. This syscall executes a binary file if the first argument is an ELF compiled file and has an executable bit set, or starts an interpreter with the content of the file if the argument has a hashbang (for example: #!/usr/bin/python) on the first line of the file and has an exec bit set.\nAfter an execve() call a file offsets and flags are copied and shared if the close-on-exec (O_CLOEXEC) flag is not set.\nLet’s prove it with an example. We need 2 files: sleep.py and exec.py. The second one will execute the first one.\n#!/usr/bin/python3 import time print(\u0026#34;sleep\u0026#34;) time.sleep(99999) Don’t forget to set an exec bit on it:\n$ chmod +x ./sleep.py The exec.py opens a file, duplicates it with dup2() syscall, clearing the close-on-exec (O_CLOEXEC) flag.\nimport os with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;r\u0026#34;) as f: print(f.fileno()) print(f\u0026#34;parent {os.getpid()}\u0026#34;) os.dup2(f.fileno(), 123) pid = os.fork() if not pid: # child print(f\u0026#34;child {os.getpid()}\u0026#34;) os.execve(\u0026#34;./sleep.py\u0026#34;, [\u0026#34;./sleep.py\u0026#34;], os.environ) f.seek(234) os.waitpid(-1, 0) If we run it in the console, the output should be something like the following:\n$ python3 ./exec.py 3 parent 6851 child 6852 sleep If we check the procfs. First, we will not be able to see the fd 3 for the child. This happens because python, by default, opens all files with the O_CLOEXEC flag (but dup2 resets this flag for 123 fd) We can get this info by running out script under strace:\n$ strace -s0 -f python3 ./exec.py And in the output we can find the following:\nopenat(AT_FDCWD, \u0026#34;/var/tmp/file1.db\u0026#34;, O_RDONLY|O_CLOEXEC) = 3 That’s why we used dup2(). It resets the O_CLOEXEC flag and allows us to check whether the fd sharing is established.\nThe parent process:\n$ ls -l /proc/6851/fd/ lrwx------ 1 vagrant vagrant 64 Jul 11 20:07 0 -\u0026gt; /dev/pts/1 lrwx------ 1 vagrant vagrant 64 Jul 11 20:07 1 -\u0026gt; /dev/pts/1 lr-x------ 1 vagrant vagrant 64 Jul 11 20:07 123 -\u0026gt; /var/tmp/file1.db \u0026lt;--- lrwx------ 1 vagrant vagrant 64 Jul 11 20:07 2 -\u0026gt; /dev/pts/1 lr-x------ 1 vagrant vagrant 64 Jul 11 20:07 3 -\u0026gt; /var/tmp/file1.db \u0026lt;--- The child has only fd 123:\n$ ls -l /proc/6852/fd/ lrwx------ 1 vagrant vagrant 64 Jul 11 20:07 0 -\u0026gt; /dev/pts/1 lrwx------ 1 vagrant vagrant 64 Jul 11 20:07 1 -\u0026gt; /dev/pts/1 lr-x------ 1 vagrant vagrant 64 Jul 11 20:07 123 -\u0026gt; /var/tmp/file1.db \u0026lt;---- lrwx------ 1 vagrant vagrant 64 Jul 11 20:07 2 -\u0026gt; /dev/pts/1 Check the positions in the parent’s fds:\n$ cat /proc/6851/fdinfo/3 pos:\t234 \u0026lt;------------------- flags: 02100000 mnt_id: 26 $ cat /proc/6851/fdinfo/123 pos:\t234 \u0026lt;------------------- flags: 0100000 mnt_id: 26 And the child:\n$ cat /proc/6852/fdinfo/123 pos:\t234 \u0026lt;------------------- flags: 0100000 mnt_id: 26 The reasonable question you may ask now is how we can protect ourselves from leaking file descriptors from a parent to children, keeping in mind that we usually execute a binary that we didn’t write. For instance, a shell starts programs like ls, ping, strace, etc.\nBack in the past (before Linux 5.9), people iterated over all possible file descriptors and tried to close them. In order to find out the upper boundary, the ulimit limit for open files was used (RLIMIT_NOFILE).\nSome people open the /proc/self/fd/ in their programs after fork() and close all fd from it.\nBut there is a more elegant way of doing this in the modern Linux kernels. It’s a close_range() syscall (man 2 close_range). It allows us to avoid heavy user space iterations and use a kernel help instead.\nThe fixed version:\n... pid = os.fork() if not pid: # child print(f\u0026#34;child {os.getpid()}\u0026#34;) max_fd = os.sysconf(\u0026#34;SC_OPEN_MAX\u0026#34;) # \u0026lt;---- added os.closerange(3, max_fd) # \u0026lt;---/ os.execve(\u0026#34;./sleep.py\u0026#34;, [\u0026#34;./sleep.py\u0026#34;], os.environ) ... O_CLOEXEC # And finally 2 sentences about the O_CLOEXEC flag, and why we need it in the first place if we can close all unneeded file descriptors? The main issue is libraries. You should always open all files with it because it’s hard to track opened files from the main program.\nAnother crucial case is a situation when the exec() fails (due to permissiom issues, wrong path, etc), and we still need some previously opened files (for instance, to write logs). Usually, reopening them after such an error is quite hard.\nAs I showed earlier, for some modern programming language it\u0026rsquo;s a default behavior for their open file functions.\nCheck if 2 file descriptors share the same open file description with kcmp() # Let’s continue our journey with more unusual and elegant system calls.\nYou can use the kcmp() syscall (man 2 kcmp) to test whether 2 fds refer to the same open file description.\nNOTE\nWe have this syscall instead of a full view of the open file description table due to security reasons. The kernel developers don’t feel good about exporting all this information to the user space https://lwn.net/Articles/845448/.\nLet’s write a tool that we can use to identifies identical file descriptors for two processes. This system call is not widely used, so many programming languages don’t have a wrapper in their standard libraries. But it’s not a problem for us. First of all, we need to find a number of this syscall. For example, we can find it in the Linux kernel sources syscall_64.tbl:\n... 311\t64\tprocess_vm_writev\tsys_process_vm_writev 312\tcommon\tkcmp\tsys_kcmp 313\tcommon\tfinit_module\tsys_finit_module ... The full code of our tool (if something is not clear, please read the man 2 kcmp):\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;syscall\u0026#34; ) const ( SYS_KCMP = 312 KCMP_FILE = 0 ) func kcmp_files(pid1, pid2, fd1, fd2 int) (int, error) { r1, _, err := syscall.Syscall6(SYS_KCMP, uintptr(pid1), uintptr(pid2), KCMP_FILE, uintptr(fd1), uintptr(fd2), 0) return int(r1), err } func main() { var ( pid1, pid2, fd1, fd2 int err error ) pid1, err = strconv.Atoi(os.Args[1]) pid2, err = strconv.Atoi(os.Args[2]) fd1, err = strconv.Atoi(os.Args[3]) fd2, err = strconv.Atoi(os.Args[4]) if err != nil { panic(err) } r1, err := kcmp_files(pid1, pid2, fd1, fd2) fmt.Println(r1, err) } For the targets, we will use the exec.py program from the previous chapter:\n$ go run ./kcmp.go 1957 1958 123 123 0 errno 0 $ go run ./kcmp.go 1957 1958 3 123 0 errno 0 $ go run ./kcmp.go 1957 1958 3 2 1 errno 0 $ go run ./kcmp.go 1957 1958 1 1 0 errno 0 As we can see, the parent and the child shared the fd 123, and the fd 3 in the parent is the copy of the 123 in the child. Also both stdout refer to the same shell pseudoterminal.\nMore ways to transfer file descriptors between processes: pidfd_getfd() and Unix datagrams. # So far, we’ve seen file descriptors sharing only from the parent to the child with the fork() call.\nOn some occasions, we want to send an fd to a target process or processes. For example, for a zero downtime program upgrades, where we want to preserve the file descriptor of a listening socket and transfer it to the new process with a new binary.\nWe have two options to do that in modern Linux kernels.\nThe first one is pretty standard and old. It works over a Unix socket. With a special UDP message, one process can pass an fd to another process. This, of course, works only locally (that’s why it’s a UNIX domain socket). The code for such transferring is massive and if you’re wondering how to write such a tool, please check out this detailed blog post.\nThe second option is quite new and allows a process to steal an fd from another process. I’m talking about the pidfd_getfd() system call (man 2 pidfd_getfd).\nIn order to leverage it, we need to open a process with another syscall: pidfd_open() (man 2 pidfd_open). Also, we would need a special set of ptrace permission: PTRACE_MODE_ATTACH_REALCREDS.\nWe can allow it system-wide in your test box, but please don’t do it in production. For production environments, please review the man 2 ptrace.\necho 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope Let’s run our old python example which opens a file with fd 3:\n$ python3 ./file2.py 123 parent pid: 3155 3 child pid: 3156 And our stealing fd tool:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;syscall\u0026#34; \u0026#34;time\u0026#34; ) const ( sys_pidfd_open = 434 // from kernel table sys_pidfd_getfd = 438 ) func pidfd_open(pid int) (int, error) { r1, _, err := syscall.Syscall(sys_pidfd_open, uintptr(pid), 0, 0) if err != 0 { return -1, err } return int(r1), nil } func pidfd_getfd(pidfd, targetfd int) (int, error) { r1, _, err := syscall.Syscall(sys_pidfd_getfd, uintptr(pidfd), uintptr(targetfd), 0) if err != 0 { return -1, err } return int(r1), nil } func main() { var ( pid, fd int err error ) pid, err = strconv.Atoi(os.Args[1]) fd, err = strconv.Atoi(os.Args[2]) if err != nil { panic(err) } fmt.Println(\u0026#34;pid:\u0026#34;, os.Getpid()) pidfd, err := pidfd_open(pid) if err != nil { panic(err) } newFd, err := pidfd_getfd(pidfd, fd) if err != nil { panic(err) } fmt.Println(newFd) time.Sleep(time.Hour) } If we run it:\n$ go run ./getfd.go 3155 3 pid: 4009 4 And check procfs:\n$ ls -la /proc/4009/fd/ lrwx------ 1 vagrant vagrant 64 Jul 10 13:24 0 -\u0026gt; /dev/pts/2 lrwx------ 1 vagrant vagrant 64 Jul 10 13:24 1 -\u0026gt; /dev/pts/2 lrwx------ 1 vagrant vagrant 64 Jul 10 13:24 2 -\u0026gt; /dev/pts/2 lrwx------ 1 vagrant vagrant 64 Jul 10 13:24 3 -\u0026gt; \u0026#39;anon_inode:[pidfd]\u0026#39; lr-x------ 1 vagrant vagrant 64 Jul 10 13:24 4 -\u0026gt; /var/tmp/file1.db \u0026lt;-------------- lrwx------ 1 vagrant vagrant 64 Jul 10 13:24 5 -\u0026gt; \u0026#39;anon_inode:[eventpoll]\u0026#39; lr-x------ 1 vagrant vagrant 64 Jul 10 13:24 6 -\u0026gt; \u0026#39;pipe:[43607]\u0026#39; l-wx------ 1 vagrant vagrant 64 Jul 10 13:24 7 -\u0026gt; \u0026#39;pipe:[43607]\u0026#39; File is with the same position:\n$ cat /proc/4009/fdinfo/4 pos:\t123 \u0026lt;-------------- flags: 02100000 mnt_id: 26 By the way, if we check the file descriptor of the pidfd object, we can observe some additional info about the opened pid:\n$ cat /proc/4009/fdinfo/3 pos:\t0 flags: 02000002 mnt_id: 15 Pid:\t3155 \u0026lt;------------------- NSpid: 3155 Shell redirections and file descriptors # Now it’s time to talk about file descriptors and shells. We start with some basics, but later in this chapter you’ll find several really nit examples which could significantly improve your shell experience and performance.\nFor all examples, I’ll use GNU Bash 5.1. But I’m sure, the same concerts and techniques are available in your favorite shell.\nLet’s start with simple and well-known redirections.\nInstead of stdin read, we can use a file:\n$ cat \u0026lt; /tmp/foo Some text The same we can do for the stdout:\n$ echo \u0026#34;123\u0026#34; \u0026gt; /tmp/foo # redirected stdout $ cat /tmp/foo 123 \u0026gt;\u0026gt; appends to a file instead of overwriting it:\n$ echo \u0026#34;123\u0026#34; \u0026gt;\u0026gt; /tmp/foo # append to a file $ cat /tmp/foo 123 123 In order to write stderr to file, we need to specify the file descriptor number:\n$ cat \u0026#34;123\u0026#34; 2\u0026gt; /tmp/foo # write stderr to a file $ cat /tmp/foo cat: 123: No such file or directory We can use the same file for both stdout and stderr:\ncat \u0026#34;123\u0026#34; \u0026gt; /tmp/foo 2\u0026gt;\u0026amp;1 All of the above internally opens a target file with the open() syscall and uses dup2() calls to overwrite the standard file descriptors with the fd of the file. For the latest one, the shell runs dup2() twice for the stdout() and stderr()\nThe general syntax for the redirection:\n\u0026gt; fd \u0026gt; file_name \u0026gt;\u0026amp; fd \u0026gt;\u0026amp; fd With bash we aren\u0026rsquo;t restricted by the standard fds and can open new ones. For instance to open an fd 10:\n$ exec 10\u0026lt;\u0026gt; /tmp/foo Check the procfs:\n$ ls -la /proc/$$/fd lrwx------ 1 vagrant vagrant 64 Jul 9 21:17 0 -\u0026gt; /dev/pts/2 lrwx------ 1 vagrant vagrant 64 Jul 9 21:17 1 -\u0026gt; /dev/pts/2 lrwx------ 1 vagrant vagrant 64 Jul 10 14:56 10 -\u0026gt; /tmp/foo \u0026lt;--------- lrwx------ 1 vagrant vagrant 64 Jul 9 21:17 2 -\u0026gt; /dev/pts/2 lrwx------ 1 vagrant vagrant 64 Jul 10 14:56 255 -\u0026gt; /dev/pts/2 If we run strace we can see how it works:\n... openat(T_FDCWD, \u0026#34;/tmp/foo\u0026#34;, O_RDWR|O_CREAT, 0666) = 3 # open dup2(3, 10) = 10 # duplicate close(3) = 0 # close unneded initial fd ... Now we can write there:\necho \u0026#34;123\u0026#34; \u0026gt;\u0026amp;10 And read from it:\n$ cat \u0026lt;\u0026amp;10 123 And when we finish, we can close it:\n$ exec 10\u0026lt;\u0026amp;- Fun fact: if you close the stdin, you’ll lose your ssh connection:\n$ exec 0\u0026lt;\u0026amp;- This happens because your bash is a session leader and a controlling terminal process. When the controlling terminal closes its terminal, the kernel sends a SIGHUP signal to it, and the shell exits. We will talk about sessions, leaders and terminals later in next series of posts.\nWe also can use “-” (dash, minus) char instead of a file name for some tools. It means to read a file content from the stdin. For example, it may be really useful with diff:\n$ echo \u0026#34;123\u0026#34; | diff -u /tmp/file1.txt - --- /tmp/file1.txt 2022-07-10 21:42:02.256998049 +0000 +++ - 2022-07-10 21:42:15.733486844 +0000 @@ -1 +1 @@ -124 +123 Another advanced feature of the bash is a process substitution, which involves the duplication of file descriptors. Long story short, you can create tmp files with on demand and use them in other tools awaiting file parameters.\nProcess substitution uses /dev/fd/\u0026lt;n\u0026gt; files to send the results of the process(es) within parentheses to another process.\nI like the following two examples. This approach helps improve my shell experience and saves me from creating temporary files. The first one is a diff example:\n$ diff -u \u0026lt;(cat /tmp/file.1 | sort | grep \u0026#34;string\u0026#34;) \u0026lt;(echo \u0026#34;string2\u0026#34;) --- /dev/fd/63 2022-07-10 21:53:39.960846984 +0000 +++ /dev/fd/62 2022-07-10 21:53:39.960846984 +0000 @@ -1 +1 @@ -string1 +string2 And the following one helps with strace and grep:\n$ strace -s0 -e openat -o \u0026gt;(grep file1.db) python3 ./dup.py 2243 openat(AT_FDCWD, \u0026#34;/var/tmp/file1.db\u0026#34;, O_RDONLY|O_CLOEXEC) = 3 Read next chapter → "},{"id":3,"href":"/docs/page-cache/1-prepare-environment-for-experiments/","title":"Prepare environment for experiments","section":"Linux Page Cache series","content":" Prepare environment for experiments # Before starting, I want to be on the same page with the reader so that any example or code snippet can be executed, compiled, and checked. Therefore we need a modern GNU/Linux installation to play with code and kernel.\nIf you are using Windows or Mac OS, I would suggest installing Vagrant with Virtual Box. For the GNU/Linux distributive, I\u0026rsquo;d like to use Arch Linux. Arch is a good example of an actual modern version of the GNU/Linux system (BTW, I use Arch Linux). It supports the latest kernels, systemd and cgroup v2.\nIf you\u0026rsquo;re already on Linux, you know what to do 😉.\nCan I use docker?\nUnfortunately, no. We need a system where we can go nuts and play around with cgroup limits, debug programs with low-level tools and run code as a root user without any limitations.\nSo below, I\u0026rsquo;m showing all you need to install on Arch.\nArch Linux provisioning # When you get your Arch running, please update it and install the following packages:\n$ pacman -Sy git, base-devel, go We need to install yay (https://github.com/Jguer/yay) in order to be able to setup software from community-driven repositories:\n$ cd ~ $ git clone https://aur.archlinux.org/yay.git $ cd yay $ makepkg -si Install vmtouch tool from aur:\n$ yay -Sy vmtouch We will need page-type tool from the kernel repo, so the easiest way to install it is to download the linux kernel release and make it manually:\n$ mkdir kernel $ cd kernel $ wget https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/snapshot/linux-5.14.tar.gz $ tar -xzf linux-5.14.tar.gz $ cd linux-5.14/tools/vm $ make $ sudo make install Now we are almost ready. We need to generate a test data file, which will be used in our experiments with Page Cache:\n$ dd if=/dev/random of=/var/tmp/file1.db count=128 bs=1M And the final step is dropping all linux caches in order to get a clean box:\n$ sync; echo 3 | sudo tee /proc/sys/vm/drop_caches Read next chapter → "},{"id":4,"href":"/docs/fd-pipe-session-terminal/2-pipes/","title":"Pipes","section":"GNU/Linux shell related internals","content":" Pipes # The pipe is a neat feature of the Linux kernel that allows us to build one-directional communication channels between related processes (often a parent and a child).\nPipes are usually well known from shells, where we use “|” symbol to build command pipelines. But first of all, the pipe is a system call, or actually, there are 2 of them: pipe() and pipe2() (man 2 pipe).\nYou can think of a pipe as a memory buffer with a byte stream API. Thus, by default, there are no messages or strict boundaries. The situation has changed since the Linux kernel 3.4 where the O_DIRECT flag and the packet mode were introduced. We will touch all variant of working with pipes in this chapter.\nAnother important feature of pipes is the max size of an atomic write. The PIPE_BUF constant (man 7 pipe) determines it and sets it to 4096 bytes. Please, read the man carefully if you want to rely on this guarantee.\nAs a result of a streaming nature, a reader and a writer can use completely different user-space buffer sizes if they want. All written bytes are read sequentially, so making the lseek() syscall for a pipe is impossible.\nThe pipes also provide a convenient notification API for both ends. The write calls to a pipe block if the internal kernel buffer is full. The writer will block or return EAGAIN (if it’s in nonblocking mode) until sufficient data has been read from the pipe to allow the writer to complete. On the other hand, if all pipe readers close their read file descriptors, the writer will get the SIGPIPE signal from the kernel, and all subsequent write() calls will return the EPIPE error.\nFrom a reader’s perspective, a pipe can return a zero size read (end-of-file, EOF) if all writers close all their write pipe file descriptors. A reader blocks if there is nothing to read until data is available (you can change this by opening a pipe in the nonblocking mode).\nImage 2. – Using a pipe to connect 2 processes Pipes are widely used in shells. The elegance of such an approach is that processes don’t have to know that they use pipes. They continue working with their standard file descriptors (stdin, stdout and stderr) as usual. Developers also don’t need to make any changes in their program\u0026rsquo;s source code in order to support this concept. It makes the process of connecting 2 programs composite, flexible, fast and reliable. Of course, in order to support such communication, shells have to do some additional work before spawning new commands (more details and examples below).\nInternally, a pipe buffer is a ring buffer with slots. Each slot has a size of a PIPE_BUF constant. The number of slots is variable, and the default number is 16. So, if we multiply 16 by 4KiB, we can get a default size of 64KiB for a pipe buffer.\nWe can control the capacity of a pipe by calling the fcntl() with the F_SETPIPE_SZ flag. A pipe\u0026rsquo;s system max size limit can be found in the /proc/sys/fs/pipe-max-size (man 7 pipe).\nWe can get the size of unread bytes in a pipe by calling ioctl() with FIONREAD operation. We’ll write an example later.\nThe usual question about pipes is, do we really need them? Can we use regular files instead? There are several issues and lacking of API with using files instead of pipes:\nThere is no easy way to notify a writer that a reader has stopped reading. For a reader, we can set up inotify (man 7 inotify) to efficiently track whether new changes appear. Also, regular files don’t have nonblocking API (this is changing with io_uring, but still, it’s much harder to use it in comparison with the pipe() syscall). One final introduction remark is that a pipe can be used by more than 2 processes. It’s possible to have multiple writers and readers for a single pipe. It’s not usuall because of the streaming nature of pipes and no clear boundaries by default, but with the new packet mode, it\u0026rsquo;s become more useful in some situations.\nHow shells internally create pipes # With shells we usually use pipes to connect the stdout and/or the stderr of a process and stdin of another process. For example:\nstdout to stdin:\n$ command1 | command2 stdout and stderr to stdin:\n$ command1 |\u0026amp; command2 or\ncommand1 2\u0026gt;\u0026amp;1 | command2 So let’s understand how shells connect the following 2 commands internally.\n$ ls -la | wc -l As we already know, a shell process has three special standard open file descriptors. Thus, all its children inherit them by default because of the fork() syscalls. The following simple program shows how a shell can create a pipe and connect 2 programs. It creates a pipe in the parent process, then makes a fork() call twice in order to run execve() for the ls and wc binaries. Before the execve() calls, the children duplicate the needed standart fd with one of the ends of the pipe.\nimport sys import os r, w = os.pipe() ls_pid = os.fork() if not ls_pid: # child os.close(r) os.dup2(w, sys.stdout.fileno()) os.close(w) os.execve(\u0026#34;/bin/ls\u0026#34;, [\u0026#34;/bin/ls\u0026#34;, \u0026#34;-la\u0026#34;, ], os.environ) wc_pid = os.fork() if not wc_pid: # child os.close(w) os.dup2(r, sys.stdin.fileno()) os.close(r) os.execve(\u0026#34;/usr/bin/wc\u0026#34;, [\u0026#34;/usr/bin/wc\u0026#34;, \u0026#34;-l\u0026#34;], os.environ) os.close(r) os.close(w) for i in range(2) : pid, status = os.waitpid(-1, 0) And if we run it:\n$ python3 ./simple_pipe.py 12 The one important note about the above code is how I close all not needed file descriptors of the pipe. We have to close them to allow the kernel to send us correct signals, block operations, and return EOF when there are no more writers.\nPipe and write buffer # Modern programming languages (for example, python) often buffer all their writes in memory before the actual write syscall executes. The main idea of such buffering is to get better I/O performance. It’s cheaper to make one big write() call than several smaller ones. There are 2 types of buffers that are widely used:\nBlock buffer For example, if its size is 4KiB, the buffer will write its content (flush) to the underlying fd only when it fills up completely or the explicit flush() call is invoked. Line buffer This buffer type flushes its content when the new line character write occurs to the buffer. The python (and other programming languages) changes the buffer type depending on the type of the underlying file descriptor. If the fd is a terminal, the buffer will be a line buffer. That makes sense because when we are in the interactive shell, we want to get the output as soon as possible. However, a block buffer will be used for pipes and regular files because it’s usually OK to postpone the flush for better performance.\nThe libc function isatty() (man 3 isatty) tests whether a file descriptor refers to a terminal.\nLet’s demonstrate this behavior with 2 scripts connected by a pipe. The first one will print 10 lines to stdout, and the other one will consumethese lines from its stdin.\nThe printing script print.py:\nimport time for i in range(4): print(f\u0026#34;{i}\u0026#34;) time.sleep(0.1) And the consumer script: stdin.py\nimport fileinput for i, line in enumerate(fileinput.input()): print(f\u0026#34;{i+1}: {line.rstrip()}\u0026#34;) If you run the print.py, you should see how the output will be printed in a line-by-line manner:\n$ python3 ./print.py 0 1 2 3 Now, if we run these 2 scripts with a pipe, you should see that the output freezes for a second, and it prints all lines at once afterwards:\n$ python3 ./print.py | python stdin.py 1: 0 2: 1 3: 2 4: 3 Now let’s make it smoother. We need to add a flush() call after each print() in the print.py:\nimport time import sys for i in range(4): print(f\u0026#34;{i}\u0026#34;) sys.stdout.flush() time.sleep(0.1) And rerun it. Now, you should be able to see that the lines appear smoothly one-by-one: $ python3 ./print.py | python stdin.py 1: 0 2: 1 3: 2 4: 3 It’s worth knowing that some core utilities have an option to control their buffering. For example, the grep can be forced to use a per line buffer with the --line-buffered option. Of course, this will give you a more interactive experience with some performance penalties. You can play with it and compare the outputs:\n$ strings /var/tmp/file1.db | grep --line-buffered -E \u0026#34;^sek\u0026#34; | cat sek.^ \\ sekA sekt $ strings /var/tmp/file1.db | grep -E \u0026#34;^sek\u0026#34; | cat sek.^ \\ sekA sekt SIGPIPE signal # One of the exciting aspects of the pipes is their notification and synchronization features.\nWe intentionally closed all unused fd in the above code with fork() and execve() calls. The reason for doing that was not only our desire to save file descriptors and write a cleaner code but also to support the pipe notification features.\nIf all readers close their fd of the pipe and a writer tries to send data into it, the writer process will get the SIGPIPE signal from the kernel. This is a brilliant idea. Let’s assume we want to grep a huge nginx access log (for example, 500GiB) in order to find a target string and care only about the first three results:\n$ cat /var/log/nginx/access.log | grep some_string | head -3 So, if we assume that the log file has all three target lines somewhere at the beginning of the file, the head command will exit almost immediately. Thus we don’t need to continue reading the file. As so, when the head util exits, it closes all its fd, including the stdin (which is a pipe). The subsequent writes from the grep will cause the kernel to send the SIGPIPE signal to it. The default handler for the SIGPIPE signal is to terminate, so grep will exit and close all its fd, including its stdin. And in its turn, the cat command will exit after receiving its own SIGPIPE signal. So the exit of the head starts the cascading exit of the whole shell pipeline.\nA shell are usually waiting on the processes with the waitpid() syscall and collects all return codes. When it sees that all the process pipeline has finished, the shell sets the exit status variable $? to the returned code of the last command in the pipeline (head in our case) and populates the $PIPESTATUS (bash) or $pipestatus (zsh) array variable with all return codes of the piplene.\nLet me demonstrate it. As you can see, all the above works without any support in the cat, grep or head tools. It’s the beauty of the pipes and shells collaboration.\nNow we are ready to write our own prove of the above:\nimport signal import os import sys def signal_handler(signum, frame): print(f\u0026#34;[pipe] signal number: {signum}\u0026#34;, file=sys.stderr) os._exit(signum) signal.signal(signal.SIGPIPE, signal_handler) for i in range(9999): print(f\u0026#34;{i}\u0026#34;) And run it:\n$ python3 ./print.py | head -3 0 1 2 [pipe] signal number: 13 \u0026lt;-------------- $ echo ${PIPESTATUS[@]} 13 0 $pipestatus, $? and pipefail # We are ready to take a bit closer look at the exit statuses of a bash pipeline. By default, the last command in the pipe is used for the $? variable, which could sometimes lead to unexpected results. For instance:\n$ echo \u0026#39;some text\u0026#39; | grep no_such_text | cut -f 1 $ echo $? 0 $ echo \u0026#39;some text\u0026#39; | grep no_such_text | cut -f 1 $ echo ${PIPESTATUS[@]} 0 1 0 But fortunately, we can change this behavior with a pipefail bash option:\n$ set -o pipefail $ echo \u0026#39;some text\u0026#39; | grep no_such_text | cut -f 1 $ echo $? 1 FIFO or Named pipes # So far, we have been talking about pipes in the context of related processes (a parent and its children), but we also have the option to share a pipe easily among any number of unrelated processes. We can create a disk reference for a pipe which is called a named pipe or a FIFO file.\nThere is one high-level man 7 fifo and a tool to create a fifo file mkfifo (man 1 mkfifo)).\nThe permission control is based on regular file permissions. So, if a process has “write” permissions, it can write to this named pipe.\nAll other aspects are identical to a regular pipe. The kernel internally creates the same pipe object and doesn’t store any data on disk.\nThe FIFO file could be helpful when you need to build a connection between completely unrelated programs or daemons without changing their source code.\npv tool # pv or pipe viewer (man 1 pv) is a nifty tool to work with pipes and file descriptors. We can insert it in any pipeline place, and it will show additional info such as ETA, write rate and amount of transferred data with an incredible visual progress bar.\nHere is the basic usage with a file shows us how fast the reading strings command can consume a file:\n$ pv /var/tmp/file1.db | strings \u0026gt; /dev/null 100MiB 0:00:01 [67.9MiB/s] [===========================================\u0026gt;] 100% It also can rate limit a pipe, which is really useful for tests:\n$ cat /var/tmp/file1.db | pv --rate-limit=1K | strings Another neat feature is monitoring a process’s progress for every open file descriptor. Under the hood it uses procfs and fdinfo folders to get the positions for all opened files:\n$ pv -d 6864 3:/var/tmp/file1.db: 234 B 0:00:01 [0.00 B/s] [\u0026gt; ] 0% ETA 0:00:00 123:/var/tmp/file1.db: 234 B 0:00:01 [0.00 B/s] [\u0026gt; ] 0% ETA 0:00:00 Pipe usage # We can get the size of unread bytes in a pipe by calling ioctl() with FIONREAD and a pipe fd. But how to get pipe usage from an unrelated process that doesn’t have the pipe file descriptor, for instance from a monitoring tool. Or, for example, we started a long running pipeline and not sure if the consumer of the pipe is reading data:\n$ dd if=/dev/urandom | strings \u0026gt; /dev/null We can, of course, use strace and check the read() syscalls in its output, but the reader could a read() syscall with a huge buffer that we can miss in the strace output.\nSo, in order to achieve the goal, we need to get the pipe file descriptor somehow. The most elegant solution (but not without drawbacks) is to steal the fd with sys_pidfd_getfd() system call and then use ioctl to get usage information.\nThe code can be something like the following:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;syscall\u0026#34; \u0026#34;golang.org/x/sys/unix\u0026#34; ) const ( sys_pidfd_open = 434 sys_pidfd_getfd = 438 FIONREAD = 0x541B ) func pidfd_open(pid int) (int, error) { r1, _, err := syscall.Syscall(sys_pidfd_open, uintptr(pid), 0, 0) if err != 0 { return -1, err } return int(r1), nil } func pidfd_getfd(pidfd, targetfd int) (int, error) { r1, _, err := syscall.Syscall(sys_pidfd_getfd, uintptr(pidfd), uintptr(targetfd), 0) if err != 0 { return -1, err } return int(r1), nil } func main() { var ( pid, fd int err error ) pid, err = strconv.Atoi(os.Args[1]) fd, err = strconv.Atoi(os.Args[2]) if err != nil { panic(err) } pidfd, err := pidfd_open(pid) if err != nil { panic(err) } newFd, err := pidfd_getfd(pidfd, fd) if err != nil { panic(err) } for { size, err := unix.IoctlGetInt(newFd, FIONREAD) if err != nil { panic(err) } fmt.Printf(\u0026#34;size:\\t%d\\n\u0026#34;, size) } } Run our target pipeline:\n$ dd if=/dev/urandom | pv --rate-limit 30K | strings \u0026gt; /dev/null ^ KiB 0:00:16 [27.6KiB/s] And run our tool:\n$ sudo go run ./pipe_capacity.go 19990 1 size: 62464 size: 62464 size: 63488 size: 63488 size: 63488 size: 63488 size: 63488 The main drawback of such a technique is that we are holding the write end of the pipe open. This can lead to an extended life of the reader because it will block on an empty pipe instead of getting EOF (see the pipe notification feature explained above).\nPackets pipe mode (O_DIRECT) # Since the Linux kernel 3.4, a pipe can be created with the O_DIRECT flag. It puts it into packet mode. From my point of view, this mode can be successfully used only with writes and reads that are less or equal to the PIPE_BUF size (4KiB) because atomicity is guaranteed only in this case.\nThe packet mode is different from the default stream mode in the following ways:\nThe kernel doesn’t try to merge writes into one ring buffer slot here and here. It, of course, leads to the underutilization of the pipe buffer, but provides guarantee of boundaries instead. Readers with read() of PIPE_BUF size get the same messages as the writers wrote; If the reader’s buffer is less than the data in the slot (some misconfigured reader infiltrated), then the remaining data is discarded to protect the boundaries of messages. Now let’s write an example with 2 writers and 2 readers. Every writer writes less than 4KiB, so readers will get one full message on every read:\nimport sys import os import time PIPE_BUF = 4096 print(f\u0026#34;supervisor: {os.getpid()}\u0026#34;, file=sys.stderr) r, w = os.pipe2(os.O_DIRECT) # fork 2 writers for instance in range(2): writer_pid = os.fork() if not writer_pid: print(f\u0026#34;writer{instance}: {os.getpid()}\u0026#34;, file=sys.stderr) os.close(r) pid = os.getpid() for i in range(100): os.write(w, f\u0026#34;writer{instance}: {i}\u0026#34;.encode()) time.sleep(1) # fork 2 readers for instance in range(2): reader_pid = os.fork() if not reader_pid: print(f\u0026#34;reader{instance}: {os.getpid()}\u0026#34;, file=sys.stderr) os.close(w) pid = os.getpid() for i in range(100): data = os.read(r, PIPE_BUF) if not len(data): break print(f\u0026#34;reader{instance}: {data}\u0026#34;) os.close(r) os.close(w) for i in range(4): os.waitpid(-1,0) Run it:\n$ python3 ./packets.py supervisor: 1200 writer0: 1201 reader0: 1203 reader0: b\u0026#39;writer0: 0\u0026#39; writer1: 1202 reader0: b\u0026#39;writer1: 0\u0026#39; reader1: 1204 reader0: b\u0026#39;writer1: 1\u0026#39; reader0: b\u0026#39;writer0: 1\u0026#39; reader0: b\u0026#39;writer0: 2\u0026#39; reader0: b\u0026#39;writer1: 2\u0026#39; reader0: b\u0026#39;writer0: 3\u0026#39; reader1: b\u0026#39;writer1: 3\u0026#39; reader0: b\u0026#39;writer0: 4\u0026#39; reader1: b\u0026#39;writer1: 4\u0026#39; If we remove the O_DIRECT flag and rerun it, we can see how readers start to break the boundaries of messages and, from time to time, get 2 messages instead of 1. The situation could be even worse, and the boundaries could be violated if a reader reads a buffer less than a writer’s written.\n… reader0: b\u0026#39;writer0: 2writer1: 2\u0026#39; reader1: b\u0026#39;writer0: 3writer1: 3\u0026#39; reader1: b\u0026#39;writer1: 4writer0: 4\u0026#39; reader0: b\u0026#39;writer0: 5\u0026#39; reader1: b\u0026#39;writer1: 5\u0026#39; reader0: b\u0026#39;writer0: 6writer1: 6\u0026#39; reader1: b\u0026#39;writer1: 7\u0026#39; reader0: b\u0026#39;writer0: 7\u0026#39; PIPE Nonblocking I/O # Unlike regular files, pipes natively support nonblocking I/O. You can create a new pipe or switch an existing pipe to the nonblocking I/O mode. The most important outcome of doing this is the ability to poll a pipe using poll(), select() and epoll() event notification facilities. Nonblocking mode saves the CPU (if correctly written) and provides a unified API for programs and developers.\nNonblocking mode might be also useful to write user space busy loops in order to get better throughput by trading more CPU usage. The idea is to skip some kernel wake up logic and return from kernel mode as soon as possible.\nThe following example shows that even with python, where exceptions are slow, we can get a better throughput with a busy loop:\nNo busy loop code:\nimport os rand = os.getrandom(1\u0026lt;\u0026lt;16-1) while True: os.write(1, rand) Max throughput: in my virtual machine:\n$ python3 ./no_busy_loop.py | pv | strings \u0026gt; /dev/null 631MiB 0:00:10 [74.5MiB/s] With busy loop:\nimport os import fcntl flags = fcntl.fcntl(1, fcntl.F_GETFL, 0) fcntl.fcntl(1, fcntl.F_SETFL, flags | os.O_NONBLOCK) rand = os.getrandom(1\u0026lt;\u0026lt;16-1) while True: try: n = os.write(1, rand) except BlockingIOError: continue I was able to get 10% better throughput in my test vm:\n$ python3 ./busy_loop.py | pv | strings \u0026gt; /dev/null 799MiB 0:00:11 [82.7MiB/s] Partial writes and syscall restarts # Now we are ready to delve into the kernel internals a bit deeper. Let’s assume we want to write 512 MiB of some data to a pipe. We already have it all in memory and call the write() syscall:\ndata = os.getrandom(1\u0026lt;\u0026lt;29) # 512 MiB os.write(1, data) # where stdout is a pipe in a shell pipeline We know from the above that the size of a pipe by default is 64KiB, but the man 7 pipe says:\nApplications should not rely on a particular capacity: an application should be designed so that a reading process consumes data as soon as it is available, so that a writing process does not remain blocked. It means that, in default blocking I/O mode, our write() call should block until all bytes have not been transferred through a pipe. It makes sense, the userspace application should not, in theory, care about the underlying kernel machinery. We have a userspace buffer with data, and we should be able to write it in one blocking call. But fortunately or unfortunately, things are a bit more complicated.\nOne theory we also need to recall here is that the kernel puts a process into a sleep state if a syscall blocks. There are 2 sleep types: interruptible (S) sleep and uninterruptible (D) sleep.\nFor example, the above code snippet with the write() syscall puts a process into the interruptible state (because it writes to a pipe):\n$ ps axu | grep write2.py vagrant S+ 20:36 0:15 python3 ./write2.py where S informs us that the process is in the interruptible sleep (waiting for an event to complete).\nSuch processes are removed from the kernel scheduler list and are put in a dedicated queue waiting for a particular event.\nThe interruptible sleep state differs from the uninterruptible in that the kernel can deliver signals to the process. Rephrasing, it’s possible to receive and handle signals during the blocking syscall in the interruptible sleep state. But the reasonable question is, what happens after the signal is handled in the middle of such a syscall? Let’s figure it out.\nWe start with the pipe_write() kernel function, where we can find the following code:\nif (signal_pending(current)) { if (!ret) ret = -ERESTARTSYS; break; } The above confirms that the signal could interrupt the process during the blocking pipe write() syscall. The interesting part here is the ret variable. If it doesn’t have anything, the kernel sets it to the -ERESTARTSYS error. Otherwise, the kernel leaves it as is. In both cases, the kernel exits from the infinitive for loop. This infinitive for loop is what keeps the write() syscall in the blocking state, and it’s in charge of data transferring between usersapace buffer (512 MiB in our case) and kernel space pipe ring buffer.\nIn turn, the ret variable stores the number of transferred through the pipe bytes. It can be much bigger than the 64KiB default pipe size because there is always at least one consumer that reads from this pipe.\nOne more piece of information that will help us understand the following examples and behavior is the ERESTARTSYS error. It signals that the kernel can safely restart a syscall because it hasn\u0026rsquo;t done any meaningful work and has no side effects.\nWith all the above said, we are ready to do some coding and debugging in order to answer the question of whether it is sufficient to do one write() to a pipe.\nIn our tests we’ll use bpftrace. It’s a handy tool that allows us to trace kernel functions via eBPF trampolines, which allows kernel code to call into BPF programs with practically zero overhead. We’ll be tracing a pipe_write() kernel function to get insides about the actual pipe writes.\nLet’s start with a producer of data. Here we have a signal handler for the SIGUSR1 signal, which prints the signal code, a buffer with random 512 MiB, and one write() syscall to stdout fd.\nimport signal import sys import os def signal_handler(signum, frame): print(f\u0026#34;signal {signum} {frame}\u0026#34; ,file=sys.stderr) signal.signal(signal.SIGUSR1, signal_handler) print(os.getpid(), file=sys.stderr) rand = os.getrandom(1\u0026lt;\u0026lt;29) print(\u0026#34;generated\u0026#34;, file=sys.stderr) n = os.write(1, rand) print(f\u0026#34;written: {n}\u0026#34;, file=sys.stderr) Now we need to write a consumer for a pipe. It will sleep for 30 seconds and afterward reads all data from the stdin in 4KiB chunks.\nimport time import sys print(\u0026#34;start sleeping\u0026#34;, file=sys.stderr) time.sleep(30) print(\u0026#34;stop sleeping\u0026#34;, file=sys.stderr) r = sys.stdin.buffer.read(4096) while len(r) \u0026gt; 0: r = sys.stdin.buffer.read(4096) print(\u0026#34;finished reading\u0026#34;, file=sys.stderr) We are ready for experiments. Let’s launch bpftrace first. We’re looking for python3 commands and want to print the return value of the pipe_write() kernel function. Please, run the following in a terminal window.\n$ sudo bpftrace -e \u0026#39;kretfunc:pipe_write /comm == \u0026#34;python3\u0026#34;/ { printf(\u0026#34;%d\\n\u0026#34;, retval);}\u0026#39; Attaching 1 probe... In another terminal window, we need to start our shell pipeline under strace tool for the writer. strace logs all write() syscalls into log.txt.\n$ strace --output log.txt -s0 -e write -- python3 ./write2.py | python3 ./slow_reader.py start sleeping 1791 generated We are in a situation where the buffer is full, the writer is in the interruptible sleep state (S), and the reader is still sleeping. It’s time to open one more console and send the SIGUSR1 signal to the blocked writer:\n$ kill -USR1 1791 In the console with the pipeline, you should eventually see something like the following:\n$ strace --output log.txt -s0 -e write -- python3 ./write2.py | python3 ./slow_reader.py start sleeping 1791 generated signal 10 \u0026lt;frame at 0x7f59b135da40, file \u0026#39;/home/vagrant/data/blog/post2/./write2.py\u0026#39;, line 15, code \u0026lt;module\u0026gt;\u0026gt; written: 65536 stop sleeping finished reading The writer received the signal and exited. It also printed that it had successfully transferred only 65536 bytes (doesn\u0026rsquo;t look familiar?).\nThe console with the bpftrace confirms the above. The pipe_write() syscall managed to write only 64KiB of data.\n$ sudo bpftrace -e \u0026#39;kretfunc:pipe_write /comm == \u0026#34;python3\u0026#34;/ { printf(\u0026#34;%d\\n\u0026#34;, retval);}\u0026#39; Attaching 1 probe... 65536 The strace log shows the same:\n$ cat log.txt write(1, \u0026#34;\u0026#34;..., 536870912) = 65536 --- SIGUSR1 {si_signo=SIGUSR1, si_code=SI_USER, si_pid=806, si_uid=1000} --- It looks like it is not sufficient to have only one syscall. If we now open the write() syscall documentation (man 2 write):\nNote that a successful write() may transfer fewer than count bytes. Such partial writes can occur for various reasons; for example, because there was insufficient space on the disk device to write all of the requested bytes, or because a blocked write() to a socket, pipe, or similar was interrupted by a signal handler after it had transferred some, but before it had transferred all of the requested bytes. In the event of a partial write, the caller can make another write() call to transfer the remaining bytes. The subsequent call will either transfer further bytes or may result in an error (e.g., if the disk is now full). The documentation answers our initial question, but there is something, even more, to show here.\nAs we saw, the pipe_write() function can also return the ERESTARTSYS error if no bytes are written. It is an interesting case, and the kernel can be asked to restart such syscalls automatically without any userspace retries. It makes total sense; the syscall didn’t have any chances to do its work, so the state is the same. The configuration of the kernel restart is done by setting the SA_RESTART flag. By default, it is already enabled in python. You can check it with the strace:\nrt_sigaction(SIGUSR1, {sa_handler=0x45c680, sa_mask=~[], sa_flags=SA_RESTORER|SA_ONSTACK|SA_RESTART|SA_SIGINFO, sa_restorer=0x45c7c0}, NULL, 8) = 0 Now we are finished with all the theory and experiments. But still have the task unresolved. What is the recommended way to write such big buffers into a pipe? We can find the answer in the python source code and its buffer writer implementation:\ndef _flush_unlocked(self): … while self._write_buf: … n = self.raw.write(self._write_buf) … del self._write_buf[:n] The above snippet shows how python restarts the write() syscall in case of a partial write.\nNow let’s rewrite our producer to use a buffered writer and demonstrate two restart concepts:\nthe automatic syscall restart; the restart after a partial write. import signal import sys import os def signal_handler(signum, frame): print(f\u0026#34;signal {signum} {frame}\u0026#34; ,file=sys.stderr) signal.signal(signal.SIGUSR1, signal_handler) print(os.getpid(), file=sys.stderr) rand = os.getrandom(1\u0026lt;\u0026lt;29) print(\u0026#34;generated\u0026#34;, file=sys.stderr) n = sys.stdout.buffer.write(rand) # \u0026lt;------------------- changed print(f\u0026#34;written: {n}\u0026#34;, file=sys.stderr) Start a pipeline:\n$ strace --output log.txt -s0 -e write -- python3 ./write2.py | python3 ./slow_reader.py start sleeping 19058 generated This time let’s send 4 signals:\n$ kill -USR1 19058 $ kill -USR1 19058 $ kill -USR1 19058 $ kill -USR1 19058 The output has changed. Now the writer was able to write all the data.\n$ strace --output log.txt -s0 -e write -- python3 ./write2.py | python3 ./slow_reader.py start sleeping 19058 generated signal 10 \u0026lt;frame at 0x7f21a4705a40, file \u0026#39;./write2.py\u0026#39;, line 15, code \u0026lt;module\u0026gt;\u0026gt; signal 10 \u0026lt;frame at 0x7f21a4705a40, file \u0026#39;./write2.py\u0026#39;, line 15, code \u0026lt;module\u0026gt;\u0026gt; signal 10 \u0026lt;frame at 0x7f21a4705a40, file \u0026#39;./write2.py\u0026#39;, line 15, code \u0026lt;module\u0026gt;\u0026gt; signal 10 \u0026lt;frame at 0x7f21a4705a40, file \u0026#39;./write2.py\u0026#39;, line 15, code \u0026lt;module\u0026gt;\u0026gt; stop sleeping written: 536870912 finished reading The bpftrace logs show what we expected. For the first write we see the default pipe buffer size, next we see three ERESTARTSYS errors which reflect our 4 signals, and a final big write with all remaining data.\n$ sudo bpftrace -e \u0026#39;kretfunc:pipe_write /comm == \u0026#34;python3\u0026#34;/ { printf(\u0026#34;%d\\n\u0026#34;, retval);}\u0026#39; 65536 -512 -512 -512 536805376 In strace log we can also see the information on syscall restarts, and it confirms what we saw in bpftrace log.\n$ cat log.txt write(1, \u0026#34;\u0026#34;..., 536870912) = 65536 --- SIGUSR1 {si_signo=SIGUSR1, si_code=SI_USER, si_pid=14994, si_uid=1000} --- write(1, \u0026#34;\u0026#34;..., 536805376) = ? ERESTARTSYS (To be restarted if SA_RESTART is set) --- SIGUSR1 {si_signo=SIGUSR1, si_code=SI_USER, si_pid=14994, si_uid=1000} --- write(1, \u0026#34;\u0026#34;..., 536805376) = ? ERESTARTSYS (To be restarted if SA_RESTART is set) --- SIGUSR1 {si_signo=SIGUSR1, si_code=SI_USER, si_pid=14994, si_uid=1000} --- write(1, \u0026#34;\u0026#34;..., 536805376) = ? ERESTARTSYS (To be restarted if SA_RESTART is set) --- SIGUSR1 {si_signo=SIGUSR1, si_code=SI_USER, si_pid=14994, si_uid=1000} --- write(1, \u0026#34;\u0026#34;..., 536805376) = 536805376 Also, if we sum the returns of the write() syscalls, we’ll get the initial random bytes buffer:\n536805376 + 65536 = 536870912 The first write() restart was done by the python buffered writer due to a partial write of 64KiB, and all other 3 were restarted by the kernel due to the ERESTARTSYS error and the SA_RESTART flag.\nPipe performance: splice(), vmsplice() and tee() # Generally, it uses double buffering when a program makes reads()/writes() calls to a regular file, a socket or a pipe. One buffer is allocated in the user space and then copied to the kernel. Such a situation leads to the loss of performance and undesirable memory allocations. Another potential performance penalty for high-performance tools is the number and duration of system calls for one unit of program iteration. For instance, if we want to replace every 2nd line of a file, we need to read the file in some chunks (1 read() syscall), make changes, and write the changed buffer back (1 write() syscall). These sequences of operations should be while we don\u0026rsquo;t reach the EOF.\nBut luckily, there are 3 syscalls that can significantly improve your code, especially if you are going to use stdin or stdout:\nsplice() – moves data from the buffer to an arbitrary file descriptor, or vice versa, or from one buffer to another (man 2 splice) vmsplice() – \u0026ldquo;copies\u0026rdquo; data from user space into the buffer (man 2 vmsplice) tee() - allocates internal kernel buffer (man 2 tee) The main idea here is what if we use a pipe not as a channel between 2 or more processes but just as an in-kernel ring buffer? Yes, of course, if you need to work with stdin and stdout, you can get a win-win situation because you don’t have to create an artificial pipe and use your real one.\nSo, for example, golang uses pipes (or pool of them) in some zero-copy operations between sockets when the io.Copy()/io.CopyN()/io.CopyBuffer()/io.ReaderFrom() are called.\nSo, the usage:\nr,w = pipe() # allocate a kernel ring buffer of 64KB size. for ;; { n = splice(socket1, w) if n \u0026lt; 0 { break // error, need to check it more carefully here } m = splice(r, socket2) if m \u0026lt; 0 { break // error, need to check it more carefully here } if m \u0026lt; n { // partial write // need to resend the rest of the buffer } } The above code is, of course, a pseudo code and doesn\u0026rsquo;t cover interruption errors and partial writes. But the main idea should be clear.\nIn theory, we can also increase the pipe buffer size, but it depends on the system and the CPU cache size. But in some cases, the bigger buffer might lead to performance degradations. So do your performance tests carefully.\nYou can also use this approach to copy a file to another place. But there is even a better solution – the copy_file_range syscall (man 2 copy_file_range). This one syscall does all the work for copying a file. As I mentioned earlier, fewer syscalls lead to better performance.\nvmsplice() is another beast that could be useful when you want to make changes in the user space and then move this memory to the kernel pipe buffer without copying it. The example:\nr,w = pipe() // allocate a kernel ring buffer of 64KB size. n = read(file, buf) // read data from the file into the user space buffer modify_data(buf) // apply some business logic to the data chunk m = vmsplice(w, buf) // transfer buffer to the kernel buffer The above code is a simplified version of what can be done. But unfortunately, in reality, dealing with vmsplice is complex, and bad documentation doesn\u0026rsquo;t help at all. If you want to go this way, please read the kernel source code first in order to understand all possible problems with vmsplice and zero data coping.\nThe last syscall that we have is tee(). It’s usually used with splice(). You probably know about the tee cli tool. (man 1 tee). The purpose of the util is to copy data from one pipe to another one while duplicating the data to a file. The coreutils implementation of tee uses read() and write() system calls to work with the pipes. But we are going to write our own version with 2 pretty new syscalls: tee() and splice() instead.\nThe tee()system call \u0026ldquo;copies\u0026rdquo; data from one buffer (pipe) to another. In reality, no real coping happens. Under the hood, the kernel just changes buffer references for pipe memory. Thus, tee() syscall does not consume any data, so the subsequent splice() call can get this data from a pipe.\nSo our homebrew implementation could be the following:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;syscall\u0026#34; ) const ( SPLICE_F_MOVE = 1 SPLICE_F_NONBLOCK = 2 ) func main() { file_path := os.Args[1] fmt.Println(\u0026#34;pid:\u0026#34;, os.Getpid()) file, err := os.OpenFile(file_path, os.O_RDWR|os.O_CREATE|os.O_TRUNC, 0755) if err != nil { panic(err) } for { n, err := syscall.Tee(0, 1, 1\u0026lt;\u0026lt;32-1, SPLICE_F_NONBLOCK) if err == syscall.EAGAIN { continue } if err != nil { panic(err) } if n == 0 { break } for n \u0026gt; 0 { slen, err := syscall.Splice(0, nil, int(file.Fd()), nil, int(n), SPLICE_F_MOVE) if err != nil { panic(err) } n -= slen } } } Let’s test it:\n$ cat /var/tmp/file1.db |./tee /tmp/tee.log | strings | grep -E \u0026#34;^ss1\u0026#34; ss1T ss1vg ss1j1; ss1* And verify that the file are identical with md5sum:\n$ md5sum /var/tmp/file1.db 737f4f46feed57b4c6bdde840945948e /var/tmp/file1.db $ md5sum /tmp/tee.log 737f4f46feed57b4c6bdde840945948e /tmp/tee.log And a final note, I would suggest reading the archive of Linus’s emails about splice(), tee() and vmsplice() here https://yarchive.net/comp/linux/splice.html. You can find there a lot of design questions and solution for all these syscalls.\nRead next chapter → "},{"id":5,"href":"/docs/page-cache/2-essential-page-cache-theory/","title":"Essential Linux Page Cache theory","section":"Linux Page Cache series","content":" Essential Page Cache theory # First of all, let’s start with a bunch of reasonable questions about Page Cache:\nWhat is the Linux Page Cache? What problems does it solve? Why do we call it «Page» Cache ? In essence, the Page Cache is a part of the Virtual File System (VFS) whose primary purpose, as you can guess, is improving the IO latency of read and write operations. A write-back cache algorithm is a core building block of the Page Cache.\nNOTE\nIf you\u0026rsquo;re curious about the write-back algorithm (and you should be), it\u0026rsquo;s well described on Wikipedia, and I encourage you to read it or at least look at the figure with a flow chart and its main operations.\n\u0026ldquo;Page\u0026rdquo; in the Page Cache means that linux kernel works with memory units called pages. It would be cumbersome and hard to track and manage bites or even bits of information. So instead, Linux\u0026rsquo;s approach (and not only Linux\u0026rsquo;s, by the way) is to use pages (usually 4K in length) in almost all structures and operations. Hence the minimal unit of storage in Page Cache is a page, and it doesn\u0026rsquo;t matter how much data you want to read or write. All file IO requests are aligned to some number of pages.\nThe above leads to the important fact that if your write is smaller than the page size, the kernel will read the entire page before your write can be finished.\nThe following figure shows a bird\u0026rsquo;s-eye view of the essential Page Cache operations. I broke them down into reads and writes.\nAs you can see, all data reads and writes go through Page Cache. However, there are some exceptions for Direct IO (DIO), and I\u0026rsquo;m talking about it at the end of the series. For now, we should ignore them.\nNOTE\nIn the following chapters, I\u0026rsquo;m talking about read(), write(), mmap() and other syscalls. And I need to say, that some programming languages (for example, Python) have file functions with the same names. However, these functions don\u0026rsquo;t map exactly to the corresponding system calls. Such functions usually perform buffered IO. Please, keep this in mind.\nRead requests # Generally speaking, reads are handled by the kernel in the following way:\n① – When a user-space application wants to read data from disks, it asks the kernel for data using special system calls such as read(), pread(), vread(), mmap(), sendfile(), etc.\n② – Linux kernel, in turn, checks whether the pages are present in Page Cache and immediately returns them to the caller if so. As you can see kernel has made 0 disk operations in this case.\n③ – If there are no such pages in Page Cache, the kernel must load them from disks. In order to do that, it has to find a place in Page Cache for the requested pages. A memory reclaim process must be performed if there is no free memory (in the caller\u0026rsquo;s cgroup or system). Afterward, kernel schedules a read disk IO operation, stores the target pages in the memory, and finally returns the requested data from Page Cache to the target process. Starting from this moment, any future requests to read this part of the file (no matter from which process or cgroup) will be handled by Page Cache without any disk IOP until these pages have not been evicted.\nWrite requests # Let\u0026rsquo;s repeat a step-by-step process for writes:\n(Ⅰ) – When a user-space program wants to write some data to disks, it also uses a bunch of syscalls, for instance: write(), pwrite(), writev(), mmap(), etc. The one big difference from the reads is that writes are usually faster because real disk IO operations are not performed immediately. However, this is correct only if the system or a cgroup doesn\u0026rsquo;t have memory pressure issues and there are enough free pages (we will talk about the eviction process later). So usually, the kernel just updates pages in Page Cache. it makes the write pipeline asynchronous in nature. The caller doesn’t know when the actual page flush occurs, but it does know that the subsequent reads will return the latest data. Page Cache protects data consistency across all processes and cgroups. Such pages, that contain un-flushed data have a special name: dirty pages.\n(II) – If a process\u0026rsquo; data is not critical, it can lean on the kernel and its flush process, which eventually persists data to a physical disk. But if you develop a database management system (for instance, for money transactions), you need write guarantees in order to protect your records from a sudden blackout. For such situations, Linux provides fsync(), fdatasync() and msync() syscalls which block until all dirty pages of the file get committed to disks. There are also open() flags: O_SYNC and O_DSYNC, which you also can use in order to make all file write operations durable by default. I\u0026rsquo;m showing some examples of this logic later.\nRead next chapter → "},{"id":6,"href":"/docs/fd-pipe-session-terminal/3-process-groups-jobs-and-sessions/","title":"Process groups, jobs and sessions","section":"GNU/Linux shell related internals","content":" Process groups, jobs and sessions # A new process group is created every time we execute a command or a pipeline of commands in a shell. Inside a shell, a process group is usually called a job. In its turn, each process group belongs to a session. Linux kernel provides a two-level hierarchy for all running processes (look at Image 3 below). As such, a process group is a set of processes, and a session is a set of related process groups. Another important limitation is that a process group and its members can be members of a single session.\n$ sleep 100 # a process group with 1 process $ cat /var/log/nginx.log | grep string | head # a process group with 3 processes Process groups # A process group has its process group identificator PGID and a leader who created this group. The PID of the group leader is equal to the corresponding PGID. As so, the type of PID and PGID are the same, and is (pid_t)[https://ftp.gnu.org/old-gnu/Manuals/glibc-2.2.3/html_node/libc_554.html]. All new processes created by the group members inherit the PGID and become the process group members. In order to create a group, we have setpgid() and setpgrp() syscalls (man 2 getpgrp()).\nA process group lives as long as it has at least one member. It means that even if the group leader terminates, the process group is valid and continues carrying out its duties. A process can leave its process group by:\njoining another group; creating its own new group; terminating. Linux kernel can reuse PIDs for new processes if only the process group with that PGID doesn’t have members. It secures a valid hierarchy of processes.\nA leader of a process group can’t join another process group because the constraints between PID of the process and PGID of the members of the group will be violated.\nTwo interesting features of process groups are:\na parent process can wait() for its children using the process group id; a signal can be sent to all members of a process group by using killpg() or kill() with a negative PGID parameter. The below command sends a SIGTERM(15) to all members of the process group 123:\n$ kill -15 -123 The following 2 scripts demonstrate this feature. We have 2 long-running scripts in a process group (it was created for us automatically by shell) connected by a pipe.\nprint.py import signal import os import sys import time def signal_handler(signum, frame): print(f\u0026#34;[print] signal number: {signum}\u0026#34;, file=sys.stderr) os._exit(signum) signal.signal(signal.SIGTERM, signal_handler) print(f\u0026#34;PGID: {os.getpgrp()}\u0026#34;, file=sys.stderr) for i in range(9999): print(f\u0026#34;{i}\u0026#34;) sys.stdout.flush() time.sleep(1) and\nstdin.py import fileinput import signal import os import sys def signal_handler(signum, frame): print(f\u0026#34;[stdin] signal number: {signum}\u0026#34;, file=sys.stderr) os._exit(signum) signal.signal(signal.SIGTERM, signal_handler) for i, line in enumerate(fileinput.input()): print(f\u0026#34;{i+1}: {line.rstrip()}\u0026#34;) Start the pipeline, and in the middle of the execution, run a kill command in a new terminal window.\n$ python3 ./print.py | python3 ./stdin.py PGID: 9743 1: 0 2: 1 3: 2 4: 3 [stdin] signal number: 15 [print] signal number: 15 And kill it by specifying the PGID:\n$ kill -15 -9743 Sessions # For its part, a session is a collection of process groups. All members of a session identify themselves by the identical SID. It’s also the pid_t type, and as a process group, also inherited from the session leader, which created the session. All processes in the session share a single controlling terminal (we’ll talk about this later).\nA new process inherits its parent’s session ID. In order to start a new session a process should call setsid() (man 2 setsid). The process running this syscall begins a new session, becomes its leader, starts a new process group, and becomes its leader too. SID and PGID are set to the process’ PID. That’s why the process group leader can’t start a new session: the process group could have members, and all these members must be in the same session.\nBasically, a new session is created in two cases:\nWhen we need to log in a user with an interactive shell. A shell process becomes a session leader with a controlling terminal (about this later). A daemon starts and wants to run in its own session in order to secure itself (we will touch daemons in more detail later). The following image shows a relationship between a session, its process groups and processes.\nImage 3. – 2 level hierarchy of processes ❶ – Session id (SID) is the same as the session leader process (bash) PID. ❷ – The session leader process (bash) has its own process group, where it’s a leader, so PGID is the same as its PID. ❸, ❹ – The session has 2 more process groups with PGIDs 200 and 300. ❺, ❻ – Only one group can be a foreground for a terminal. All other process groups are background. We will touch on these terms in a minute. ❼, ❽, ❾ – All members of a session share a pseudoterminal /dev/pts/0. In order to get all the above information for a running process, we can read the /proc/$PID/stat file. For example, for my running bash shell $$ porcess:\n$ cat /proc/$$/stat | cut -d \u0026#34; \u0026#34; -f 1,4,5,6,7,8 | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | paste \u0026lt;(echo -ne \u0026#34;pid\\nppid\\npgid\\nsid\\ntty\\ntgid\\n\u0026#34;) - pid 8415 # PID ppid\t8414 # parent PID pgid\t8415 # process group ID sid 8415 # sessions ID tty 34816 # tty number tgid\t9348 # foreground process group ID where (man 5 procfs https://man7.org/linux/man-pages/man5/proc.5.html):\npid – the process id. ppid – the PID of the parent of this process. pgrp – the process group id of the process. sid – the session id of the process. tty – the controlling terminal of the process. (The minor device number is contained in the combination of bits 31 to 20 and 7 to 0; the major device number is in bits 15 to 8.) tgid – the id of the foreground process group of the controlling terminal of the process. Controlling terminal, controlling process, foreground and background process groups # A controlling terminal is a terminal (tty, pty, console, etc) that controls a session. There may not be a controlling terminal for a session. It is usual for daemons.\nIn order to create a controlling terminal, at first, the session leader (usually a shell process) starts a new session with setsid(). This action drops a previously available terminal if it exists. Then the process needs to open a terminal device. On this first open() call, the target terminal becomes the controlling terminal for the session. From this point in time, all existing processes in the session are able to use the terminal too. The controlling terminal is inherited by fork() call and preserved by execve(). A particular terminal can be the controlling terminal only for one session.\nA controlling terminal sets 2 important definitions: a foreground process group and a background process group. At any moment, there can be only one foreground process group for the session and any number of background ones. Only processes in the foreground process group can read from the controlling terminal. On the other hand, writes are allowed from any process by default. There are some tricks with terminals, we touch them later, when we will talk solely about terminals.\nA terminal user can type special signal-generating terminal characters on the controlling terminal. The most famous ones are CTRL+C and CTRL+Z. As its name suggests, a corresponding signal is sent to the foreground process group. By default, the CTRL+C triggers a SIGINT signal, and CTRL+Z a SIGTSTP signal.\nAlso, opening the controlling terminal makes the session leader the controlling process of the terminal. Starting from this moment, if a terminal disconnection occurs, the kernel will send a SIGHUP signal to the session leader (usually a shell process).\nThe tcsetpgrp() (man 3 tcsetpgrp) is a libc function to promote a process group to the foreground group of the controlling terminal. There is also the tcgetpgrp() function to get the current foreground group. These functions are used primarily by shells in order to control jobs. On linux, we can also use ioctl() with TIOCGPGRP and TIOCSPGRP operations to get and set the foreground group.\nLet\u0026rsquo;s write a script that emulates the shell logic of creating a process group for a pipeline.\npg.py import os print(f\u0026#34;parent: {os.getpid()}\u0026#34;) pgpid = os.fork() # ⓵ if not pgpid: # child os.setpgid(os.getpid(), os.getpid()) # ⓶ os.execve(\u0026#34;./sleep.py\u0026#34;, [\u0026#34;./sleep.py\u0026#34;, ], os.environ) print(f\u0026#34;pgid: {pgpid}\u0026#34;) pid = os.fork() if not pid: # child os.setpgid(os.getpid(), pgpid) # ⓷ os.execve(\u0026#34;./sleep.py\u0026#34;, [\u0026#34;./sleep.py\u0026#34;, ], os.environ) pid = os.fork() if not pid: # child os.setpgid(os.getpid(), pgpid) # ⓷ os.execve(\u0026#34;./sleep.py\u0026#34;, [\u0026#34;./sleep.py\u0026#34;, ], os.environ) for i in range(3) : pid, status = os.waitpid(-1, 0) ⓵ – Create the first process in the shell pipeline.\n⓶ – Start a new process group and store its PID as a new PGID for all future processes.\n⓷ – Start new processes and move them into the process group with PGID.\nSo when we run it we can see the following output:\npython3 ./pg.py parent: 8429 pgid: 8430 8431 sleep 8432 sleep 8430 sleep The full state of processes:\n$ cat /proc/8429/stat | cut -d \u0026#34; \u0026#34; -f 1,4,5,6,7,8 | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | paste \u0026lt;(echo -ne \u0026#34;pid\\nppid\\npgid\\nsid\\ntty\\ntgid\\n\u0026#34;) - pid 8429 ppid 8415 pgid 8429 sid 8415 tty 34816 tgid 8429 $ cat /proc/8430/stat | cut -d \u0026#34; \u0026#34; -f 1,4,5,6,7,8 | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | paste \u0026lt;(echo -ne \u0026#34;pid\\nppid\\npgid\\nsid\\ntty\\ntgid\\n\u0026#34;) - pid 8430 ppid 8429 pgid 8430 sid 8415 tty 34816 tgid 8429 $ cat /proc/8431/stat | cut -d \u0026#34; \u0026#34; -f 1,4,5,6,7,8 | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | paste \u0026lt;(echo -ne \u0026#34;pid\\nppid\\npgid\\nsid\\ntty\\ntgid\\n\u0026#34;) - pid 8431 ppid 8429 pgid 843 sid 8415 tty 34816 tgid 8429 $ cat /proc/8432/stat | cut -d \u0026#34; \u0026#34; -f 1,4,5,6,7,8 | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | paste \u0026lt;(echo -ne \u0026#34;pid\\nppid\\npgid\\nsid\\ntty\\ntgid\\n\u0026#34;) - pid 8432 ppid 8429 pgid 8430 sid 8415 tty 34816 tgid 8429 The only problem with the above code is we didn\u0026rsquo;t transfer the foreground group to our newly created process group. The tgid in the above output shows that. The 8429 PID is a PGID of the parent pg.py script, not the newly created process group 8430.\nNow, if we press CTRL+C to terminate the processes, we’ll stop only the parent with PID 8429. It happens because it’s in the foreground group from the perspective of the controlling terminal. All processes in the 8430 group will continue running in the background. If they try to read from the terminal (stdin), they will be stopped by the controlling terminal by sending them a SIGTTIN signal. It is a result of trying to read from the controlling terminal without acquiring the foreground group. If we log out or close the controlling terminal, this group will not get a SIGHUP signal, because the bash process (the controlling process) doesn’t know that we started something in the background.\nIn order to fix this situation, we need to notify the controlling terminal that we want to run another process group in the foreground. Let’s modify the code and add the tcsetpgrp() call.\nimport os import time import signal print(f\u0026#34;parent: {os.getpid()}\u0026#34;) pgpid = os.fork() if not pgpid: # child os.setpgid(os.getpid(), os.getpid()) os.execve(\u0026#34;./sleep.py\u0026#34;, [\u0026#34;./sleep.py\u0026#34;, ], os.environ) print(f\u0026#34;pgid: {pgpid}\u0026#34;) pid = os.fork() if not pid: # child os.setpgid(os.getpid(), pgpid) os.execve(\u0026#34;./sleep.py\u0026#34;, [\u0026#34;./sleep.py\u0026#34;, ], os.environ) pid = os.fork() if not pid: # child os.setpgid(os.getpid(), pgpid) os.execve(\u0026#34;./sleep.py\u0026#34;, [\u0026#34;./sleep.py\u0026#34;, ], os.environ) tty_fd = os.open(\u0026#34;/dev/tty\u0026#34;, os.O_RDONLY) # ⓵ os.tcsetpgrp(tty_fd, pgpid) # ⓶ for i in range(3): # ⓷ os.waitpid(-1, 0) h = signal.signal(signal.SIGTTOU, signal.SIG_IGN) # ⓸ os.tcsetpgrp(tty_fd, os.getpgrp()) # ⓹ signal.signal(signal.SIGTTOU, h) # ⓺ print(\u0026#34;got foreground back\u0026#34;) time.sleep(99999) ⓵ – In order to run the tcsetpgrp(), we need to know the current controlling terminal path. The safest way to do that is to open a special virtual file /dev/tty. If a process has a controlling terminal, it returns a file descriptor for that terminal. We, in theory, can use one of the standard file descriptors too. But it\u0026rsquo;s not sustanable because the caller can redirects all of them.\n⓶ – Put the new process group into the foreground group of the controlling terminal.\n⓷ – Here, we wait for the processes to exit. It is where we should call CTRL+C.\n⓸ – Before we command the controlling terminal to return into the foreground session we need to silence the SIGTTOU signal. The man page says: If tcsetpgrp() is called by a member of a background process group in its session, and the calling process is not blocking or ignoring SIGTTOU, a SIGTTOU signal is sent to all members of this background process group. We don’t need this signal, so it’s OK to block it.\n⓹ – Returning to the foreground.\n⓺ – Restoring the SIGTTOU signal handler.\nAnd if we now run the script and press CTRL+C, everything should work as expected.\n$ python3 ./pg.py parent: 8621 pgid: 8622 8622 sleep 8624 sleep 8623 sleep ^C \u0026lt;------------------- CTRL+C was pressed Traceback (most recent call last): File \u0026#34;/home/vagrant/data/blog/post2/./sleep.py\u0026#34;, line 7, in \u0026lt;module\u0026gt; Traceback (most recent call last): File \u0026#34;/home/vagrant/data/blog/post2/./sleep.py\u0026#34;, line 7, in \u0026lt;module\u0026gt; Traceback (most recent call last): File \u0026#34;/home/vagrant/data/blog/post2/./sleep.py\u0026#34;, line 7, in \u0026lt;module\u0026gt; time.sleep(99999) KeyboardInterrupt time.sleep(99999) KeyboardInterrupt time.sleep(99999) KeyboardInterrupt got foreground back \u0026lt;----------------- back to foreground Shell job control # Now it’s time to understand how shells allow us to run multiple commands simultaneously and how we can control them.\nFor instance, when we run the following pipeline:\n$ sleep 999 | grep 123 The shell here:\ncreates a new process group with the PGID of the first process in the group; puts this group in the foreground group by notifying the terminal with tcsetpgrp(); stores the PIDs and sets up a waitpid() syscall. The process group is also known as a shell job. The PIDs:\n$ ps a | grep sleep 9367 pts/1\tS+ 0:00 sleep 999 $ ps a | grep grep 9368 pts/1\tS+ 0:00 grep 123 And if we get the details for sleep:\n$ cat /proc/9367/stat | cut -d \u0026#34; \u0026#34; -f 1,4,5,6,7,8 | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | paste \u0026lt;(echo -ne \u0026#34;pid\\nppid\\npgid\\nsid\\ntty\\ntgid\\n\u0026#34;) - pid 9367 ppid\t6821 pgid\t9367 sid 6821 tty 34817 tgid\t9367 and for grep:\n$ cat /proc/9368/stat | cut -d \u0026#34; \u0026#34; -f 1,4,5,6,7,8 | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | paste \u0026lt;(echo -ne \u0026#34;pid\\nppid\\npgid\\nsid\\ntty\\ntgid\\n\u0026#34;) - pid 9368 ppid\t6821 pgid\t9367 sid 6821 tty 34817 tgid\t9367 While waiting for the foreground job to finish, we can move this job to the background by pressing Ctrl+Z. It is a control action for the terminal, which sends a SIGTSTP signal to the foreground process group. The default signal handler for a process is to stop. In its turn, bash gets a notification from the waitpid(), that the statuses of the monitoring processes have changed. When bash sees that the foreground group has become stopped, it returns the foreground back to shell by running tcsetpgrp():\n^Z [1]+ Stopped sleep 999 | grep 123 $ We can get the current statuses of all known jobs by using the built-in jobs command:\n$ jobs -l [1]+ 7962 Stopped sleep 999 7963 | grep 123 We may resume a job in the background by calling bg shell built-in with the ID of the job.When we use bg with a background stopped job, the shell uses killpg and SIGCONT signal.\n$ bg %1 [1]+ sleep 999 | grep 123 \u0026amp; If we check the status now, we can see that it’s running in the background.\n$ jobs -l [1]+ 7962 Running sleep 999 7963 | grep 123 \u0026amp; If we want, we can move the job back in the foreground by calling fg built-in shell command:\n$ fg %1 sleep 999 | grep 123 We also can start a job in the background by adding an ampersand (\u0026amp;) char in the end of the pipeline:\n$ sleep 999 | grep 123 \u0026amp; [1] 9408 $ kill command # kill is usually a shell built-in for at least two reasons:\nShell usually allows to kill jobs by their job ids. So we need to be able to resolve internal job IDs into process group IDs (the %job_id syntaxis). Allow users to send signals to processes if the system hits the max running process limit. Usually, during emergencies and system misbehaviour. For example, we can check how bash does it – int kill_builtin() and zsh – int bin_kill().\nAnother helpful piece of knowledge about the kill command and system calls is a \u0026ldquo;-1\u0026rdquo; process group. It\u0026rsquo;s a special group, and the signal to it will fan out the signal to all processes on the system except the PID 1 process (it\u0026rsquo;s almost always a systemd process on all modern GNU/Linux distributions):\n[remote ~] $ sudo kill -15 -1 Connection to 192.168.0.1 closed by remote host. Connection to 192.168.0.1 closed. [local ~] $ Terminating shell # When a controlling process loses its terminal connection, the kernel sends a SIGHUP signal to inform it of this fact. If either the controlling process or other members of the session ignores this signal, or handle it and do nothing, then the further read from and write to the closed terminal (ususally /dev/pts/*) calls will return the end-of-file (EOF) zero bytes.\nShell processes (which are usually control terminals) have a handler to catch SIGHUP signals. Receiving a signal starts a fan-out process of sending SIGHUP signals to all jobs it has created and know about (remember the fg, bg and waitpid()). The default action for the SIGHUP is terminate.\nnohup and disown # But suppose we want to protect our long-running program from being suddenly killed by a broken internet connection or low laptop battery. In that case, we can start a program under nohup tool or use bash job control disownbuilt-in command. Let’s understand how they work and where they are different.\nThe nohup performs the following tricks:\nChanges the stdin fd to /dev/null. Redirects the stdout and stderr to a file on disk. Set an ignore SIG_IGN flag for SIGHUP signal. The interesting moment here is that the SIG_IGN is preserved after the execve() syscall. Run the execve(). All the above make the program immune to the SIGHUP signal and can’t fail due to writing to or reading from the closed terminal.\n$ nohup ./long_running_script.py \u0026amp; [1] 9946 $ nohup: ignoring input and appending output to \u0026#39;nohup.out\u0026#39; $ jobs -l [1]+ 9946 Running nohup ./long_running_script.py \u0026amp; As you can see from the output, the bash knows about this process and can show it in jobs.\nAnother way we have to achieve long-running programs to survive the controlling terminal closure is a built-in disown of the bash shell. Instead of ignoring the SIGHUP signal, it just removes the job\u0026rsquo;s PID from the list of known jobs. Thus no SIGHUP signal will be sent to the group.\n$ ./long_running_script.py \u0026amp; [1] 9949 $ jobs -l [1]+ 9949 Running ./long_running_script.py \u0026amp; $ disown 9949 $ jobs -l $ ps a | grep 9949 9949 pts/0\tS 0:00 /usr/bin/python3 ./long_running_script.py 9954 pts/0\tS+ 0:00 grep 9949 The drawback of the above solution is we don’t overwrite and close the terminal standard fd. So if the tool decides to write to or read from the closed terminal, it could fail.\nThe other conclusion we can make is that the shell doesn’t send SIGHUP to processes or groups it did not create, even if the process is in the same session where the shell is a session leader. Daemons # A daemon is a long living process. It is often started at the system’s launch and service until the OS shutdown. Daemon runs in the background without a controlling terminal. The latest guarantees that the process never gets terminal-related signals from the kernel: SIGINT, SIGTSTP, and SIGHUP.\nThe classic “unix” way of spawning daemons is performed by a double-fork technique. After both fork() calls the parents exit immediately.\nThe first fork() is needed: to become a child of the systemd process with PID 1; if a daemon starts manually from a terminal, it puts itself into the background and a shell doesn’t know about it, so it can’t terminate the daemon easily; the child is guaranteed not to be a process group leader, so the following setsid() call starts a new session and breaks a possible connection to the existing controlling terminal. The second fork() is done in order to stop being the session leader. This step protects a daemon from opening a new controlling terminal, as only a session leader can do that. The gnu provides a convininet libc function to demonize our program: daemon() man 3 daemon.\nBut nowadays, systems with systemd tend not to follow the double-fork trick. Instead developers highly rely on systemd features:\nsystemd can starts a new process session for daemons; it can swap the standard file descriptors for stdin, stdout and stderr with regular files or sockets instead of manually close or redirect them to syslog. For example nginx code: ... fd = open(\u0026#34;/dev/null\u0026#34;, O_RDWR); ... if (dup2(fd, STDIN_FILENO) == -1) { ngx_log_error(NGX_LOG_EMERG, log, ngx_errno, \u0026#34;dup2(STDIN) failed\u0026#34;); return NGX_ERROR; } ... So, a daemon can continue safely write to the stderr and stdout and don’t be afraid of getting the EOF because of a closed terminal. The following setting controls that:\nStandardOutput= StandardError= For instance, etcd service doesn\u0026rsquo;t do a double-fork and fully rely on the systemd. That’s why its PID is a PGID and SID, so it’s a session leader.\n$ cat /proc/10350/stat | cut -d \u0026#34; \u0026#34; -f 1,4,5,6,7,8 | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | paste \u0026lt;(echo -ne \u0026#34;pid\\nppid\\npgid\\nsid\\ntty\\ntgid\\n\u0026#34;) - pid 10350 ppid\t1 pgid\t10350 sid 10350 tty 0 tgid\t-1 Also systemd has a lot of other features for modern service developers such as helpers for live upgrades, socket activation, sharing sockets, cgroup limits, etc\u0026hellip;\nRead next chapter → "},{"id":7,"href":"/docs/fd-pipe-session-terminal/4-terminals-and-pseudoterminals/","title":"Terminals and pseudoterminals","section":"GNU/Linux shell related internals","content":" Terminals and pseudoterminals # Terminals come to us from the history of UNIX systems. Basically, terminals provided an API for the console utils (physical ones!) to generalize interaction with users. It includes ways of reading input and writing to it in two modes:\nthe canonical mode (default) – input is buffered line by line and read into after a new line char \\n occurs; the noncanonical mode – an application can read terminal input a character at a time. For example vi, emacs and less use this mode. Nowadays, with the widespread use of rich graphical UIs, the significance of the terminals are lesser than it was, but still, we use this protocol implicitly every time we start an ssh connection.\nThe are a bunch of files under /dev/ directory that represents different types of terminals:\n/dev/tty* – physical consoles; /dev/ttyS* – serial connections; /dev/pts/* – pseudoterminals. Also, the /proc/tty/drivers file contains other supported drivers.\nSo, in order to determine what terminal file the current shell session is using, we have a tty cli tool (man 1 tty).\nOn my remote ssh connection:\n$ tty /dev/pts/0 For a physical console:\n$ tty /dev/tty1 We can also open a virtual device /dev/tty to get a fd of the controlling terminal if it exists for the current process.\nPseudoterminal (devpts) # In order to make it possible to use a terminal remotely, the Linux kernel provides a feature called pseudoterminal or devpts (https://www.kernel.org/doc/html/latest/filesystems/devpts.html).\nIt allows us to build terminal emulators and use them instead of a real terminal, where an application expects a terminal device. Using pseudoterminals we can build terminal proxies, record screen sessions and mock user input. You can think about pseudoterminal like as a special type of Inter-process communication (IPC). It\u0026rsquo;s a bidirectional communication channel. All operations that can be applied to a terminal device can also be applied to a pts device end, including something that doesn\u0026rsquo;t make sense. For example: changing the speed of connection transforms into a no-op internally.\nPseudoterminal consists of 2 parts:\nA ptmx part which is a leader for the pseudoterminal. This end is used to emulate the user input and read back the program output. A pts is a secondary end. This part is given to an application that needs a terminal. The following image shows how an ssh client uses pseudoterminals to establish remote access.\nImage 4. – ssh client, sshd server and two pairs of pseudoterminals\t❶ – We usually have a graphical UI on our local host and use some kind of xterm to run a local console. The UI subsystem receives all keyboard inputs, so it opens a pseudoterminal and redirects it into its ptmx device. The other side of the terminal is what an xterm emulator sees. ❷ – The local bash process creates a new foreground process group (job). It\u0026rsquo;s running in the foreground, so a ssh gets full control of the terminal. ssh client is a special terminal program that leverages the full power of terminals. It sets the terminal into the raw mode. Thus, the future CTRL-C and CTRL-Z combinations do not affect the local ssh process. Instead, all such commands will be sent to the remote side of the ssh connection and interpreted there. When the ssh client exits, it returns all settings back. ❸, ❼ – The communication between the ptmx and pts happens in the kernel and is hidden from our eyes. ❹ – sshd server is listening to new connections. It makes a fork for each connected user and checks their credentials. ❺ – The sshd process creates a new pseudoterminal pair. It basically connects the ptmx side and the client tcp socket. ❻ – Then the sshd process makes a fork(), opens a corresponding new pts, starts a new session (setsid()), opens our pts making it the controlling terminal of the session and duplicates standard file descriptors 0,1 and 2 with the pts descriptor. Now it’s ready to call execve() to start bash shell. Let’s emulate the above with a small example. We are creating a new session with a new pseudoterminal pair and write the stdin into a file on disk.\nimport os import time import sys print(f\u0026#34;parent: {os.getpid()}\u0026#34;) ptmx, secondary = os.openpty() pid = os.fork() if not pid: print(f\u0026#34;child: {os.getpid()}\u0026#34;) os.close(ptmx) os.setsid() name = os.ttyname(secondary) print(name) s = os.open(name, os.O_RDWR) os.dup2(s, 0) os.dup2(s, 1) os.dup2(s, 2) os.close(secondary) os.close(s) with open(\u0026#39;/tmp/file.txt\u0026#39;, \u0026#34;w\u0026#34;) as f: for l in sys.stdin: f.write(l) f.flush() time.sleep(999999) else: os.close(secondary) os.write(ptmx, b\u0026#34;text\\n\u0026#34;) os.waitpid(-1,0) Run it:\n$ python3 ./terminal.py parent: 8776 child: 8777 /dev/pts/3 Check the file:\n$ cat /tmp/file.txt text File descriptors show that all is good:\n$ ls -l /proc/8776/fd lrwx------ 1 vagrant vagrant 64 Jul 12 21:45 0 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 12 21:45 1 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 12 21:45 2 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 12 21:45 3 -\u0026gt; /dev/ptmx $ ls -l /proc/8777/fd lrwx------ 1 vagrant vagrant 64 Jul 12 21:45 0 -\u0026gt; /dev/pts/3 lrwx------ 1 vagrant vagrant 64 Jul 12 21:45 1 -\u0026gt; /dev/pts/3 lrwx------ 1 vagrant vagrant 64 Jul 12 21:45 2 -\u0026gt; /dev/pts/3 l-wx------ 1 vagrant vagrant 64 Jul 12 21:45 3 -\u0026gt; /tmp/file.txt Terminal settings # The ptmx and pts devices share terminal attributes (termios) and window size (winsize) structures.\nThe current setting of a terminal can be obtained and updated by the stty command:\n$ stty -a As we discussed earlier, the background jobs can print to the stdout by default. However, we can change it by setting TOSTOP flag for the terminal. If we do that, the background process group will receive a SIGTTOU signal from the kernel. The default handler for this signal is stop.\n$ stty tostop And run some background job:\n$ yes | grep y \u0026amp; [1] 10694 [vagrant@archlinux post2]$ jobs -l [1]+ 10693 Stopped (tty output)\tyes 10694 | grep y And return it back:\n$ stty -tostop We also can return back to the default setting by using the “sane” parameter:\n$ stty sane Handling Terminal Signals # As we discussed, the kernel can send some terminal signals to foreground and background processes. Some of them we already touched:\nSIGTTIN – a background process tried to read from a terminal. SIGTTOU – a background process tries to write to a terminal when the tostop flag is set or a background process asks to send it to the foreground. SIGTSTP – a default response to a CTRL-Z pressed combination. The noncanonical programs such as vi, emacs and less, need to handle all the above signals in order to reset terminal settings back and forth, redraw a terminal content and place the cursor in the right place.\nAnother interesting terminal signal we haven’t seen is the SIGWINCH signal. A foreground process receives it when size of a terminal window has changed. Usually, a program uses ioctl() with the TIOCGWINSZ operation to get the current size in its signal handler. For example:\nimport time import signal import termios import fcntl import struct def signal_handler(signum, frame): packed = fcntl.ioctl(0, termios.TIOCGWINSZ, struct.pack(\u0026#39;HHHH\u0026#39;, 0, 0, 0, 0)) rows, cols, h_pixels, v_pixels = struct.unpack(\u0026#39;HHHH\u0026#39;, packed) print(rows, cols, h_pixels, v_pixels) signal.signal(signal.SIGWINCH, signal_handler) time.sleep(9999) And if you start it and play with size of terminal window:\n$ python3 ./size.py 32 85 1360 1280 32 72 1152 1280 32 74 1184 1280 32 86 1376 1280 32 83 1328 1280 33 73 1168 1320 screen and tmux # screen (man 1 screen) and tmux (man 1 tmux) are usually used for protecting shell sessions between connections. It is also widely used for long-running jobs and better ssh user client experience. Both use pseudoterminals to multiplex a single physical terminal (or terminal window) between multiple processes (multiple shell sessions). In this section, we will talk about tmux, but the screen is almost the same in all discussed topics here.\nOn the first start, tmux starts a server with a set of ptmx corresponding to its panes. Clients of tmux (tmux attach) use a default unix socket to find and connect to the server:\n$ ls -lad /tmp/tmux-1000/default srwxrwx--- 1 vagrant vagrant 0 Jul 14 12:57 /tmp/tmux-1000/default where 1000 is a user id UID.\ntmux doesn\u0026rsquo;t do any tcsetpgrp() calls, because any panel or window creates a new pair of terminals.\nSo let’s demonstrate it. After we ssh to a box, we have our bash with PID 11761 :\n$ echo $$ 11761 $ ls -la /proc/$$/fd lrwx------ 1 vagrant vagrant 64 Jul 14 12:08 0 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 14 12:08 1 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 14 12:08 2 -\u0026gt; /dev/pts/0 Let assume, that we already have a working tmux session on the server. So if we check the ps command, we can see it with PID 11781.\n... ├─sshd(11619)───sshd(11749)───sshd(11760)───bash(11761) ... └─tmux: server(11780)───bash(11781) Now let’s attach to the tmux session:\n$ tmux attach We get a new bash PID from the above ps output and a new pseudoterminal:\n$ echo $$ 11781 $ ls -la /proc/$$/fd lrwx------ 1 vagrant vagrant 64 Jul 14 12:10 0 -\u0026gt; /dev/pts/1 lrwx------ 1 vagrant vagrant 64 Jul 14 12:10 1 -\u0026gt; /dev/pts/1 lrwx------ 1 vagrant vagrant 64 Jul 14 12:10 2 -\u0026gt; /dev/pts/1 If we check the pseudoterminal devpts folder /dev/pts/:\n$ ls -la /dev/pts/ crw--w---- 1 vagrant tty 136, 0 Jul 14 12:11 0 crw--w---- 1 vagrant tty 136, 1 Jul 14 12:11 1 c--------- 1 root\troot 5, 2 Jul 9 21:14 ptmx we can see that there are 2 pseudoterminals, one is our ssh client, and the other is our tmux.\nLet’s observe the tmux server’s file descriptors:\n$ ls -la /proc/11780/fd lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 0 -\u0026gt; /dev/null lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 1 -\u0026gt; /dev/null lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 2 -\u0026gt; /dev/null … lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 5 -\u0026gt; /dev/pts/0 … lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 8 -\u0026gt; /dev/ptmx We see it has an open /dev/ptmx to control the terminal on /dev/pts/1, and a /dev/pts/0 to read our input and write output back to our ssh connection.\nNow, if we detach, we can still see the bash process in the ps output:\n$ pstree -p tmux: server(11780)───bash(11781) It’s left there and is waiting for us. Talkinh about the tmux server, it closed /dev/pts/0 because we returned back control of the ssh terminal and it doesn\u0026rsquo;t need to read and write to it anymore:\n$ ls -la /proc/11780/fd lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 0 -\u0026gt; /dev/null lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 1 -\u0026gt; /dev/null lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 2 -\u0026gt; /dev/null … lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 8 -\u0026gt; /dev/ptmx Also, if we take a look the procfs, we will find out that tmux server has its own session and process group. It makes sense, it should not depend on any of the active terminal connections.\n$ cat /proc/11780/stat | cut -d \u0026#34; \u0026#34; -f 1,5,6,7,8,9 | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | paste \u0026lt;(echo -ne \u0026#34;pid\\nppid\\npgid\\nsid\\ntty\\ntgid\\n\u0026#34;) - pid 11780 ppid 1 pgid 11780 sid 11780 tty 0 tgid -1 If we open one more session, we will see one more shell process:\n└─tmux: server(11780)─┬─bash(11781) └─bash(11846) And open file descriptors of the server:\n$ ls -la /proc/11780/fd lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 0 -\u0026gt; /dev/null lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 1 -\u0026gt; /dev/null lrwx------ 1 vagrant vagrant 64 Jul 14 12:18 10 -\u0026gt; /dev/pts/2 lrwx------ 1 vagrant vagrant 64 Jul 14 12:20 11 -\u0026gt; /dev/ptmx ... lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 7 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 8 -\u0026gt; /dev/ptmx The overall schema described above could be presented in the below image 5:\nImage 5. – tmux client-server architecture tmux server opens as many pseudoterminals as needed, but none of them is a controlling terminal. It is possible to do it in several ways, and the most simple one is to open a /dev/pts/ptmx with the O_NOCTTY open flag (man 2 open).\nIn order to set a demonize, the tmux server made a single fork() and setsid():\nint daemon(int nochdir, int noclose) { int fd; switch (fork()) { case -1: return (-1); case 0: break; default: _exit(0); } if (setsid() == -1) return (-1); if (!nochdir) (void)chdir(\u0026#34;/\u0026#34;); if (!noclose \u0026amp;\u0026amp; (fd = open(_PATH_DEVNULL, O_RDWR, 0)) != -1) { (void)dup2(fd, STDIN_FILENO); (void)dup2(fd, STDOUT_FILENO); (void)dup2(fd, STDERR_FILENO); if (fd \u0026gt; 2) (void)close (fd); } #ifdef __APPLE__ daemon_darwin(); #endif return (0); } It makes the tmux server immune to terminal terminations and signals logic I described earlier.\nPseudoterminal proxy # As I mentioned earlier, we could think about pseudoterminals as a proxy. The reasonable question is can we leverage them in our day-to-day scripting routines? The answer, as you can guess, is yes. There are two incredible tools: expect (man 1 expect) and script (man 1 script) that uses pseudoterminals in the proxy mode and are super helpful in writing basic automation.\nexpect # The expect program uses a pseudoterminal to allow an interactive terminal-oriented program to be driven from a script file. Let’s assume we need to automate an ssh connection in a shell script. We want to insert username and password when the ssh client asks for them. We can easily achieve this with expect:\n#!/usr/bin/expect set timeout 20 set host [lindex $argv 0] set username [lindex $argv 1] set password [lindex $argv 2] spawn ssh \u0026#34;$username\\@$host\u0026#34; expect \u0026#34;password:\u0026#34; send \u0026#34;$password\\r\u0026#34;; interact And test it:\n[local ~] $ ./ssh.exp 192.168.0.1 vagrant vagrant spawn ssh vagrant@192.168.0.1 vagrant@192.168.0.1\u0026#39;s password: [remote ~]$ where ”vagrant“ is our username and password.\nscript # Another task is to record a terminal session. The pseudoterminals are used in the script program, which records all of the input and output that occurs during a shell session.\nRecord to file:\n$ script --timing=time.txt script.log Replay:\n$ scriptreplay --timing=time.txt script.log Changing a process\u0026rsquo;s controlling terminal # And lastly, I want to show you one more fascinating tool reptyr https://github.com/nelhage/reptyr. Imagine, you forgot to start a screen or tmux session and have run a long-running script. Using reptyr you can move it under a screen or tmux session without a restart!\nIt uses ptrace systemcall to change the session id of the running process.\nWe use ptrace to attach to a target process and force it to execute code of our own choosing in order to open the new terminal, and dup2 it over stdout and stderr.\nMore info about it could be found in the detailed author’s blog post:\nhttps://blog.nelhage.com/2011/02/changing-ctty/\nHow it works tl;dr:\nWhile we have mutt captured with ptrace, we can make it fork a dummy child, and start tracing that child, too. We’ll make the child setpgid to make it into its own process group, and then get mutt to setpgid itself into the child’s process group. mutt can then setsid, moving into a new session, and now, as a session leader, we can finally ioctl(TIOCSCTTY) on the new terminal, and we win.\n"},{"id":8,"href":"/docs/page-cache/3-page-cache-and-basic-file-operations/","title":"Page Cache and basic file operations","section":"Linux Page Cache series","content":" Page Cache and basic file operations # Now it\u0026rsquo;s time to roll up our sleeves and get started with some practical examples. By the end of this chapter, you will know how to interact with Page Cache and which tools you can use.\nUtils needed for this section:\nsync (man 1 sync) – a tool to flush all dirty pages to persistent storage; /proc/sys/vm/drop_caches (man 5 proc) – the kernel procfs file to trigger Page Cache clearance; vmtouch – a tool for getting Page Cache info about a particular file by its path. NOTE For now, we ignore how vmtouch works. I\u0026rsquo;m showing how to write an alternative with almost all its features later. File reads # Reading files with read() syscall # I start with a simple program that reads the first 2 bytes from our test file /var/tmp/file1.db.\nwith open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;br\u0026#34;) as f: print(f.read(2)) Usually, these kinds of read requests are translated into the read() syscall. Let\u0026rsquo;s run the script with strace (man 1 strace) to make sure that f.read() uses read() syscall:\n$ strace -s0 python3 ./read_2_bytes.py The output should look something like this:\n... openat(AT_FDCWD, \u0026#34;./file1.db\u0026#34;, O_RDONLY|O_CLOEXEC) = 3 ... read(3, \u0026#34;%B\\353\\276\\0053\\356\\346Nfy2\\354[\u0026amp;\\357\\300\\260%D6$b?\u0026#39;\\31\\237_fXD\\234\u0026#34;..., 4096) = 4096 ... NOTE\nThe read() syscall returned 4096 bytes (one page) even though the script asked only for 2 bytes. It\u0026rsquo;s an example of python optimizations and internal buffered IO. Although this is beyond the scope of this post, but in some cases it is important to keep this in mind.\nNow let’s check how much data the kernel\u0026rsquo;s cached. In order to get this info, we use vmtouch:\n$ vmtouch /var/tmp/file1.db Files: 1 LOOK HERE Directories: 0 ⬇ Resident Pages: 20/32768 80K/128M 0.061% Elapsed: 0.001188 seconds From the output, we can see that instead of 2B of data that Python\u0026rsquo;s asked for, the kernel has cached 80KiB or 20 pages.\nBy design, the kernel can\u0026rsquo;t load anything less than 4KiB or one page into Page Cache, but what about the other 19 pages? It is a excellent example of the kernel\u0026rsquo;s read ahead logic and preference to perform sequential IO operations over random ones. The basic idea is to predict the subsequent reads and minimize the number of disks seeks. Syscalls can control this behavior: posix_fadvise() (man 2 posix_fadvise) and readahead() (man 2 readahead).\nNOTE\nUsually, it doesn\u0026rsquo;t make a big difference for database management systems and storages to tune the default read-ahead parameters in a production environment. If DBMS doesn\u0026rsquo;t need data that were cached by the read-ahead, the kernel memory reclaim policy should eventually evict these pages from Page Cache. And usually, the sequential IO is not expensive for kernel and hardware. Disabling read-ahead at all might even lead to some performance degradations due to increased number of disk IO operations in the kernel queues, more context switches and more time for kernel memory management subsystem to recognize the working set. We will talk about memory reclaiming policy, memory pressure, and cache writeback later in this series.\nLet’s now use posix_fadvise() to notify the kernel that we are reading the file randomly, and thus we don\u0026rsquo;t want to have any read ahead features:\nimport os with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;br\u0026#34;) as f: fd = f.fileno() os.posix_fadvise(fd, 0, os.fstat(fd).st_size, os.POSIX_FADV_RANDOM) print(f.read(2)) Before running the script, we need to drop all caches:\n$ echo 3 | sudo tee /proc/sys/vm/drop_caches \u0026amp;\u0026amp; python3 ./read_2_random.py And now, if you check the vmtouch output, you can see that there is only one page as expected:\n$ vmtouch /var/tmp/file1.db Files: 1 LOOK HERE Directories: 0 ⬇ Resident Pages: 1/32768 4K/128M 0.00305% Elapsed: 0.001034 seconds Reading files with mmap() syscall # For reading data from files we can also use mmap() syscall (man 2 mmap). mmap() is a \u0026ldquo;magic\u0026rdquo; tool and can be used to solve a wide range of tasks. But for our tests, we need only one of its features – an ability to map a file into a process memory in order to access the file as a flat array. I\u0026rsquo;m talking about mmap() in more detail later. But at the moment, if you are not familiar with it, mmap() API should be clear from the following example:\nimport mmap with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;r\u0026#34;) as f: with mmap.mmap(f.fileno(), 0, prot=mmap.PROT_READ) as mm: print(mm[:2]) The above code does the same as we\u0026rsquo;ve just done with read() syscall. It reads the first 2 bytes of the file.\nAlso, for test purposes, we need to flush all caches before the script should be executed:\n$ echo 3 | sudo tee /proc/sys/vm/drop_caches \u0026amp;\u0026amp; python3 ./read_2_mmap.py And checking the Page Cache content:\n$ vmtouch /var/tmp/file1.db Files: 1. LOOK HERE Directories: 0 ⬇ Resident Pages: 1024/32768 4M/128M 3.12% Elapsed: 0.000627 seconds As you can see, mmap() has performed an even more aggressive readahead.\nLet\u0026rsquo;s change the readahead with madvise() syscall like we did with fadvise().\nimport mmap with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;r\u0026#34;) as f: with mmap.mmap(f.fileno(), 0, prot=mmap.PROT_READ) as mm: mm.madvise(mmap.MADV_RANDOM) print(mm[:2]) Run it:\n$ echo 3 | sudo tee /proc/sys/vm/drop_caches \u0026amp;\u0026amp; python3 ./read_2_mmap_random.py and Page Cache content:\n$ vmtouch /var/tmp/file1.db Files: 1 LOOK HERE Directories: 0 ⬇ Resident Pages: 1/32768 4K/128M 0.00305% Elapsed: 0.001077 seconds As you can see from the above output, with the MADV_RANDOM flag, we managed to achieve exactly one page read from disk and thus one page in Page Cache.\nFile writes # Now let\u0026rsquo;s play with writes.\nWriting to files with write() syscall # Let’s continue working with our experimental file and try to update the first 2 bytes instead:\nwith open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;br+\u0026#34;) as f: print(f.write(b\u0026#34;ab\u0026#34;)) NOTE\nBe careful, and don\u0026rsquo;t open a file with w mode. It will rewrite your file with 2 bytes. We need r+ mode.\nDrop all caches and run the above script:\nsync; echo 3 | sudo tee /proc/sys/vm/drop_caches \u0026amp;\u0026amp; python3 ./write_2_bytes.py Now let\u0026rsquo;s check the content of the Page Cache.\n$ vmtouch /var/tmp/file1.db Files: 1 LOOK HERE Directories: 0 ⬇ Resident Pages: 1/32768 4K/128M 0.00305% Elapsed: 0.000674 seconds As you can see, we have 1 page cached after only 2B write. It\u0026rsquo;s an important observation because if your writes are smaller than a page size, you will have 4KiB reads before your writes in order to populate Page Cache.\nAlso, we can check dirty pages by reading the current cgroup memory stat file.\nGet a current terminal cgroup:\n$ cat /proc/self/cgroup 0::/user.slice/user-1000.slice/session-4.scope $ grep dirty /sys/fs/cgroup/user.slice/user-1000.slice/session-3.scope/memory.stat file_dirty 4096 If you see 0, run the script one more time, you apparently get lucky, and the dirty pages have already been written to disk.\nFile writes with mmap() syscall # Let\u0026rsquo;s now replicate the write with mmap():\nimport mmap with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;r+b\u0026#34;) as f: with mmap.mmap(f.fileno(), 0) as mm: mm[:2] = b\u0026#34;ab\u0026#34; You can repeat the above commands with vmtouch and cgroup grep to get dirty pages, and you should get the same output. The only exception is the read ahead policy. By default, mmap() loads much more data in Page Cache, even for write requests.\nDirty pages # As we saw earlier, a process generates dirty pages by writing to files through Page Cache.\nLinux provides several options to get the number of dirty pages. The first and oldest one is to read /proc/memstat:\n$ cat /proc/meminfo | grep Dirty Dirty: 4 kB The full system information is often hard to interpret and use because we can\u0026rsquo;t determine which process and file has these dirty pages.\nThat\u0026rsquo;s why the best option in order to get dirty page info is to use cgroup:\n$ cat /sys/fs/cgroup/user.slice/user-1000.slice/session-3.scope/memory.stat | grep dirt file_dirty 4096 If your program uses mmap() to write to files, you have one more option to get dirty pages stats with a per-process granularity. procfs has the /proc/PID/smaps file. It contains memory counters for the process broken down by virtual memory areas (VMA). We can get dirty pages by finding:\nPrivate_Dirty – the amount of dirty data this process generated; Shared_Dirty – and the amount other processes wrote. This metric shows data only for referenced pages. It means the process should access pages and keep them in its page table (more details later). $ cat /proc/578097/smaps | grep file1.db -A 12 | grep Dirty Shared_Dirty: 0 kB Private_Dirty: 736 kB But what if we want to get the dirty page stats for a file? To answer this question linux kernel provides 2 files in procfs: /proc/PID/pagemap and /proc/kpageflags. I\u0026rsquo;m showing how to write our own tool with them later in the series, but for now we can use the debug tool from the linux kernel repo to get per file page info: page-types.\n$ sudo page-types -f /var/tmp/file1.db -b dirty flags page-count MB symbolic-flags long-symbolic-flags 0x0000000000000838 267 1 ___UDl_____M________________________________ uptodate,dirty,lru,mmap 0x000000000000083c 20 0 __RUDl_____M________________________________ referenced,uptodate,dirty,lru,mmap total 287 1 I filtered out all pages of our file /var/tmp/file1.db by the dirty flag. In the output, you can see that the file has 287 dirty pages or 1 MiB of dirty data, which will be persisted to storage eventually. page-type aggregates pages by flags, so that you can see 2 sets in the output. Both have the dirty flag D, and the difference between them is the presence of the referenced flag R (which I\u0026rsquo;m briefly touching on in the Page Cache eviction section later).\nSynchronize file changes with fsync(), fdatasync() and msync() # We already used sync (man 1 sync) to flush all dirty pages to disks before every test to get a fresh system without any interference. But what if we want to write a database management system, and we need to be sure that all writes will get to disks before a power outage or other hardware errors occur? For such cases, Linux provides several methods to force the kernel to run a sync of pages for the file in Page Cache:\nfsync() – blocks until all dirty pages of the target file and its metadata are synced; fdatasync() – the same as the above but excluding metadata; msync() – the same as the fsync() but for memory mapped file; open a file with O_SYNC or O_DSYNC flags to make all file writes synchronous by default and work as a corresponding fsync() and fdatasync() syscalls accordingly. NOTE\nYou still need to care about write barriers and understand how the underlying file system works because the kernel scheduler might reorder write operations. Usually, a file append operation is safe and can\u0026rsquo;t corrupt the previously written data. Other types of mutate operations may mess with your files (for instance, for ext4, even with the default journal). That\u0026rsquo;s why almost all database management systems like MongoDB, PostgreSQL, Etcd, Dgraph, etc, have write ahead logs (WAL) which are append-only. There are some exceptions though. If you\u0026rsquo;re curious more about this topic, this blog post from Dgraph is a good starting point.\nThere are some exceptions, though. For instance, lmdb (and its clones, bboltdb from etcd) uses a witty and clever idea of keeping two roots of the tree and doing a copy-on-write.\nAnd here is an example of the file sync:\nimport os with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;br+\u0026#34;) as f: fd = f.fileno() os.fsync(fd) Checking file presence in Page Cache with mincore() # Before we go any further, let’s figure out how vmtouch manages to show us how many pages of a target file Page Cache contains.\nThe secret is a mincore() syscall (man 2 mincore). mincore() stands for \u0026ldquo;memory in the core\u0026rdquo;. Its parameters are a starting virtual memory address, a length of the address space and a resulting vector. mincore() works with memory (not files), so it can be used for checking if anonymous memory was swapped out.\nman 2 mincore\nmincore() returns a vector that indicates whether pages of the calling process\u0026rsquo;s virtual memory are resident in core (RAM), and so will not cause a disk access (pagefault) if referenced. The kernel returns residency information about the pages starting at the address addr, and continuing for length bytes.\nSo to replicate vmtouch we need to map a file into the virtual memory of the process, even though we are not going to make neither reads nor writes. We just want to have it in the process memory area (more about this later in mmap() section).\nNow we have all we need to write our own simple vmtouch in order to show cached pages by file path. I\u0026rsquo;m using go here because, unfortunately, Python doesn\u0026rsquo;t have an easy way to call mincore() syscall:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;syscall\u0026#34; \u0026#34;unsafe\u0026#34; ) var ( pageSize = int64(syscall.Getpagesize()) mode = os.FileMode(0600) ) func main() { path := \u0026#34;/var/tmp/file1.db\u0026#34; file, err := os.OpenFile(path, os.O_RDONLY|syscall.O_NOFOLLOW|syscall.O_NOATIME, mode) if err != nil { log.Fatal(err) } defer file.Close() stat, err := os.Lstat(path) if err != nil { log.Fatal(err) } size := stat.Size() pages := size / pageSize mm, err := syscall.Mmap(int(file.Fd()), 0, int(size), syscall.PROT_READ, syscall.MAP_SHARED) defer syscall.Munmap(mm) mmPtr := uintptr(unsafe.Pointer(\u0026amp;mm[0])) cached := make([]byte, pages) sizePtr := uintptr(size) cachedPtr := uintptr(unsafe.Pointer(\u0026amp;cached[0])) ret, _, err := syscall.Syscall(syscall.SYS_MINCORE, mmPtr, sizePtr, cachedPtr) if ret != 0 { log.Fatal(\u0026#34;syscall SYS_MINCORE failed: %v\u0026#34;, err) } n := 0 for _, p := range cached { // the least significant bit of each byte will be set if the corresponding page // is currently resident in memory, and be clear otherwise. if p%2 == 1 { n++ } } fmt.Printf(\u0026#34;Resident Pages: %d/%d %d/%d\\n\u0026#34;, n, pages, n*int(pageSize), size) } And if we run it:\n$ go run ./main.go Resident Pages: 1024/32768 4194304/134217728 And comparing it with vmtouch output:\n$ vmtouch /var/tmp/file1.db Files: 1 LOOK HERE Directories: 0 ⬇ Resident Pages: 1024/32768 4M/128M 3.12% Elapsed: 0.000804 seconds Read next chapter → "},{"id":9,"href":"/docs/page-cache/4-page-cache-eviction-and-page-reclaim/","title":"Page Cache eviction and page reclaim","section":"Linux Page Cache series","content":" Page Cache eviction and page reclaim # So far, we have talked about adding data to Page Cache by reading and writing files, checking the existence of files in the cache, and flushing the cache content manually. But the most crucial part of any cache system is its eviction policy, or regarding Linux Page Cache, it\u0026rsquo;s also the memory page reclaim policy. Like any other cache, Linux Page Cache continuously monitors the last used pages and makes decisions about which pages should be deleted and which should be kept in the cache.\nThe primary approach to control and tune Page Cache is the cgroup subsystem. You can divide the server\u0026rsquo;s memory into several smaller caches (cgroups) and thus control and protect applications and services. In addition, the cgroup memory and IO controllers provide a lot of statistics that are useful for tuning your software and understanding the internals of the cache.\nTheory # Linux Page Cache is closely tightened with Linux Memory Management, cgroup and virtual file system (VFS). So, in order to understand how eviction works, we need to start with some basic internals of the memory reclaim policy. Its core building block is a per cgroup pair of active and inactive lists:\nthe first pair for anonymous memory (for instance, allocated with malloc() or not file backended mmap()); the second pair for Page Cache file memory (all file operations including read(), write, filemmap() accesses, etc.). The former is exactly what we are interested in. This pair is what linux uses for Page Cache evection process. The least recently used algorithm LRU is the core of each list. In turn, these 2 lists form a double clock data structure. In general, Linux should choose pages that have not been used recently (inactive) based on the fact that the pages that have not been used recently will not be used frequently in a short period of time. It is the basic idea of ​​the LRU algorithm. Both the active and the inactive lists adopt the form of FIFO (First In First Out) for their entries. New elements are added to the head of the linked list, and the elements in between gradually move toward the end. When memory reclamation is needed, the kernel always selects the page at the end of the inactive list for releasing. The following figure shows the simplified concept of the idea:\nFor example, the system starts with the following content of the lists. A user process has just read some data from disks. This action triggered the kernel to load data to the cache. It was the first time when the kernel had to access the file. Hence it added a page h to the head of the inactive list of the process\u0026rsquo;s cgroup:\nSome time has passed, and the system loads 2 more pages: i and j to the inactive list and accordingly has to evict pages a and b from it. This action also shifts all pages toward the tail of the inactive LRU list, including our page h:\nNow, a new file operation to the page h promotes the page to the active LRU list by putting it at the head. This action also ousts the page 1 to the head of the inactive LRU list and shifts all other members.\nAs time flies, page h looses its head position in the active LRU list.\nBut a new file access to the h\u0026rsquo;s position in the file returns h back to the head of the active LRU list.\nThe above figures show the simplified version of the algorithm.\nBut it\u0026rsquo;s worth mentioning that the real process of pages promotion and demotion is much more complicated and sophisticated.\nFirst of all, if a system has NUMA hardware nodes (man 8 numastat), it has twice more LRU lists. The reason is that the kernel tries to store memory information in the NUMA nodes in order to have fewer lock contentions.\nIn addition, Linux Page Cache also has special shadow and referenced flag logic for promotion, demotion and re-promotion pages.\nShadow entries help to mitigate the memory thrashing problem. This issue happens when the programs\u0026rsquo; working set size is close to or greater than the real memory size (maybe cgroup limit or the system RAM limitation). In such a situation, the reading pattern may evict pages from the inactive list before the subsequent second read request has appeared. The full idea is described in the mm/workingset.c and includes calculating refault distance. This distance is used to judge whether to put the page from the shadow entries immediately to the active LRU list.\nAnother simplification I made was about PG_referenced page flag. In reality, the page promotion and demotion use this flag as an additional input parameter in the decision algorithm. A more correct flow of the page promotion:\nflowchart LR A[Inactive LRU,\\nunreferenced] --\u003e B[Inactive LRU,\\nreferenced] B --\u003e C[Active LRU,\\nunreferenced] C --\u003e D[Active LRU,\\nreferenced] Manual pages eviction with POSIX_FADV_DONTNEED # I\u0026rsquo;ve already shown how to drop all Page Cache entries using /proc/sys/vm/drop_caches file. But what if we want for some reason to clear the cache for a file?\nEXAMPLE\nIt could sometimes be useful to evict a file from the cache in a real life situation. Assume we want to test how fast our MongoDB gets back to optimal condition after a system reboot. You can stop a replica, clean all its files from Page Cache and start it back.\nvmtouch already can do that. Its -e flag commands the kernel to evict all pages of the requested file from Page Cache:\nFor example:\n$ vmtouch /var/tmp/file1.db -e Files: 1 Directories: 0 Evicted Pages: 32768 (128M) Elapsed: 0.000704 seconds $ vmtouch /var/tmp/file1.db Files: 1. LOOK HERE Directories: 0 ⬇ Resident Pages: 0/32768 0/128M 0% Elapsed: 0.000566 seconds Let\u0026rsquo;s look under the hood and figure out how it works. In order to write our own tool we need to use the already seen posix_fadvise syscall with the POSIX_FADV_DONTNEED option.\nCode:\nimport os with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;br\u0026#34;) as f: fd = f.fileno() os.posix_fadvise(fd, 0, os.fstat(fd).st_size, os.POSIX_FADV_DONTNEED) For testing, I read the entire test file into Page Cache with dd:\n$ dd if=/var/tmp/file1.db of=/dev/null 262144+0 records in 262144+0 records out 134217728 bytes (134 MB, 128 MiB) copied, 0.652248 s, 206 MB/s $ vmtouch /var/tmp/file1.db Files: 1 LOOK HERE Directories: 0 ⬇ Resident Pages: 32768/32768 128M/128M 100% Elapsed: 0.002719 seconds And now, after running our script, we should see 0 pages in Page Cache:\n$ python3 ./evict_full_file.py $ vmtouch /var/tmp/file1.db Files: 1 LOOK HERE Directories: 0 ⬇ Resident Pages: 0/32768 0/128M 0% Elapsed: 0.000818 seconds Make your memory unevictable # But what if you want to force the kernel to keep your file memory in Page Cache, no matter what. It is called making the file memory unevictable .\nEXAMPLE\nSometimes you have to force the kernel to give you a 100% guarantee that your files will not be evicted from the memory. You can want it even with modern linux kernels and properly configured cgroup limits, which should keep the working data set in Page Cache. For example, due to issues with other processes on the system where you share disk and network IO. Or, for instance, because of an outage of a network attached storage.\nKernel provides a bunch of syscalls for doing that: mlock(), mlock2() and mlockall(). As with the mincore(), you must map the file first.\nmlock2() is a preferable syscall for Page Cache routines because it has the handy flag MLOCK_ONFAULT:\nLock pages that are currently resident and mark the entire range so that the remaining nonresident pages are locked when they are populated by a page fault.\nAnd don\u0026rsquo;t forget about limits (man 5 limits.conf). You likely need to increased it:\n$ ulimit -l 64 And finally, to get the amount of unevictable memory, please, check the cgroup memory controller stats for your cgroup:\n$ grep unevictable /sys/fs/cgroup/user.slice/user-1000.slice/session-3.scope/memory.stat unevictable 0 Page Cache, vm.swappiness and modern kernels # Now that we understand the basic reclaiming theory with 4 LRU lists (for anon and file memory) and evictable/unevictable types of memory, we can talk about the sources to refill the system free memory. The kernel constantly maintains lists of free pages for itself and user-space needs. If such lists get below the threshold, the linux kernel starts to scan LRU lists in order to find pages to reclaim. It allows the kernel to keep memory in some equilibrium state.\nThe Page Cache memory is usually evictable memory (with some rare mlock() exceptions). And thus, it maybe look obvious, that Page Cache should be the first and the only option for the memory eviction and reclaiming. Since disks already have all that data, right? But fortunately or unfortunately, in real production situations, this is not always the best choice.\nIf the system has swap (and it should have it with modern kernels), the kernel has one more option. It can swap out the anonymous (not file-backed) pages. It may seem counterintuitive, but the reality is that sometimes user-space daemons can load tons of initialization code and never use it afterward. Some programs, especially statically built, for example, could have a lot of functionality in their binaries that may be used only several times in some edge cases. In all such situations, there is not much sense in keeping them in valuable memory.\nSo, in order to control which inactive LRU list to prefer for scans, the kernel has the sysctl vm.swappiness knob.\n$ sudo sysctl -a | grep swap vm.swappiness = 60 There are a lot of blog posts, stories and forum threads about this magic setting. On top of that, the legacy cgroup v1 memory subsystem has its own per cgroup swappiness knob. All this makes information about the current vm.swappiness meaning hard to understand and change. But let me try to explain some recent changes and give you fresh links.\nFirst of all, by default vm.swappiness is set 60, the min is 0 and the max is 200:\n/* * From 0 .. 200. Higher means more swappy. */ int vm_swappiness = 60; The 100 value means that the kernel considers anonymous and Page Cache pages equally in terms of reclamation.\nSecondly, the cgroup v2 memory controller doesn\u0026rsquo;t have the swappiness knob at all:\n#ifdef CONFIG_MEMCG static inline int mem_cgroup_swappiness(struct mem_cgroup *memcg) { /* Cgroup2 doesn\u0026#39;t have per-cgroup swappiness */ if (cgroup_subsys_on_dfl(memory_cgrp_subsys)) return vm_swappiness; /* root ? */ if (mem_cgroup_disabled() || mem_cgroup_is_root(memcg)) return vm_swappiness; return memcg-\u0026gt;swappiness; Instead, the kernel developers decided to change the swappiness logic significantly. You can check it if you run git blame on mm/vmscan.c and search for the get_scan_count() function.\nFor example, at the time of writing, the anonymous memory will not be touched regardless of vm.swappiness if the inactive LRU Page Cache list has a decent amount of pages:\n/* * If there is enough inactive page cache, we do not reclaim * anything from the anonymous working right now. */ if (sc-\u0026gt;cache_trim_mode) { scan_balance = SCAN_FILE; goto out; } The full logic of making decisions about what and from which LRU to reclaim, you can find in the get_scan_count()function in mm/vmscan.c.\nAlso, please take a look at the memory.swap.high and the memory.swap.max cgroup v2 settings. You can control them if you want to correct the vm.swappiness logic for your cgroup and load pattern.\nAnother interesting topic, which you should keep in mind when dealing with the swap and Page Cache, is the IO load during the swapping in/out processes. If you have IO pressure, you can easily hit your IO limits and, for example, degrade your Page Cache writeback performance.\nUnderstanding memory reclaim process with /proc/pid/pagemap # Now it\u0026rsquo;s time for low level troubleshooting technics.\nThere is a /proc/PID/pagemap file that contains the page table information of the PID. The page table, basically speaking, is an internal kernel map between page frames (real physical memory pages stored in RAM) and virtual pages of the process. Each process in the linux system has its own virtual memory address space which is completely independent form other processes and physical memory addresses.\nThe full /proc/PID/pagemap and related file documentation, including data formats and ways to read, it can be found in the kernel documentation folder. I strongly recommend that you read it before proceeding to the sections below.\npage-types kernel page tool # page-types is the Swiss knife of every kernel memory hacker. Its source code comes with the Linux kernel sources tools/vm/page-types.c.\nIf you didn\u0026rsquo;t install it in the first chapter:\n$ wget https://github.com/torvalds/linux/archive/refs/tags/v5.13.tar.gz $ tar -xzf ./v5.13.tar.gz $ cd v5.13/vm/tools $ make Now let\u0026rsquo;s use it in order to understand how many pages of our test file /var/tmp/file1.db the kernel has placed in Active and Inactive LRU lists:\n$ sudo ./page-types --raw -Cl -f /var/tmp/file1.db foffset cgroup offset len flags /var/tmp/file1.db Inode: 133367 Size: 134217728 (32768 pages) Modify: Mon Aug 30 13:14:19 2021 (13892 seconds ago) Access: Mon Aug 30 13:15:47 2021 (13804 seconds ago) 10689 @1749 21fa 1 ___U_lA_______________________P____f_____F_1 ... 18965 @1749 24d37 1 ___U_l________________________P____f_____F_1 18966 @1749 28874 1 ___U_l________________________P____f_____F_1 18967 @1749 10273 1 ___U_l________________________P____f_____F_1 18968 @1749 1f6ad 1 ___U_l________________________P____f_____F_1 flags page-count MB symbolic-flags long-symbolic-flags 0xa000010800000028 105 0 ___U_l________________________P____f_____F_1 uptodate,lru,private,softdirty,file,mmap_exclusive 0xa00001080000002c 16 0 __RU_l________________________P____f_____F_1 referenced,uptodate,lru,private,softdirty,file,mmap_exclusive 0xa000010800000068 820 3 ___U_lA_______________________P____f_____F_1 uptodate,lru,active,private,softdirty,file,mmap_exclusive 0xa001010800000068 1 0 ___U_lA_______________________P____f_I___F_1 uptodate,lru,active,private,softdirty,readahead,file,mmap_exclusive 0xa00001080000006c 16 0 __RU_lA_______________________P____f_____F_1 referenced,uptodate,lru,active,private,softdirty,file,mmap_exclusive total 958 3 The output contains 2 sections: the first one provides per-page information, and the second aggregates all pages with the same flags and counts the summary. What we need from the output in order to answer to the LRU question are A and l flags, which, as you can guess, stand for \u0026ldquo;active\u0026rdquo; and \u0026ldquo;inactive\u0026rdquo; lists.\nAs you can see, we have:\n105 + 16 = 121 pages or 121 * 4096 = 484 KiB in inactive LRU list. 820 + 1 + 16 = 837 pages or 837 * 4096 = 3.2 MiB in active LRU list. Writing Page Cache LRU monitor tool # page-types is a really useful tool for low-level debugging and investigations, but its output format is hard to read and aggregate. I promised earlier that we would write our own vmtouch, so now we\u0026rsquo;re creating it. Our alternative version will provide even more information about pages. It will show not only how many pages are in Page Cache, but also how many of them are in the active and the inactive LRU lists.\nTo do this, we need 2 kernel files: /proc/PID/pagemap and /proc/kpageflags.\nThe full code you can find in the github repo, however here, I would like to focus on a few important moments:\n... ① err = syscall.Madvise(mm, syscall.MADV_RANDOM) ... ② ret, _, err := syscall.Syscall(syscall.SYS_MINCORE, mmPtr, sizePtr, cachedPtr) for i, p := range cached { ③ if p%2 == 1 { ④ _ = *(*int)(unsafe.Pointer(mmPtr + uintptr(pageSize*int64(i)))) } } ... ⑤ err = syscall.Madvise(mm, syscall.MADV_SEQUENTIAL) ... ① – Here, we need to disable the read ahead logic for the target file in order to protect ourselves from loading (charging) unneeded data to Page Cache by our tool; ② – Use mincore() syscall to get a vector of the pages in Page Cache; ③ – Here, we check whether a page is in Page Cache or not; ④ – If Page Cache contains a page, we need to update the corresponding process\u0026rsquo;s page table entry by referencing this page. Our tool has to do this in order to use the /proc/pid/pagemap. Otherwise the /proc/pid/pagemap file will not contain the target file pages and thus their flags. ⑤ – Here, we are turning off the harvesting of reference bits. This is required due to kernel reclaim logic. Our tool read memory and hence influences the kerne LRU lists. By using madvise() with MADV_SEQUENTIAL we notify linux kernel to ignore our operations. Let\u0026rsquo;s test our tool. We need 2 terminals. In the first one, start our tool with watch (man 1 watch) to run our tool every 100ms in an infinitive loop:\nwatch -n 0.1 \u0026#39;sudo go run ./lru.go\u0026#39; And in the second terminal, we will read the file with the dd (man 1 dd):\ndd if=/var/tmp/file1.db of=/dev/null Demo of what you should see:\nUsing the above approach, you can now perform low-level Page Cache investigations.\nRead next chapter → "},{"id":10,"href":"/docs/page-cache/5-more-about-mmap-file-access/","title":"More about mmap() file access","section":"Linux Page Cache series","content":" More about mmap() file access # Before we start the cgroup chapter, where I\u0026rsquo;m showing how to leverage memory and IO limits in order to control Page Cache eviction and improve the reliability of services, I want to delve a bit deeper into mmap() syscall. We need to understand what is happening under the hood and shed more light on the reading and writing process with mmap().\nmmap() overview # Memory mapping is one of the most interesting features of Linux systems. One of its features is the ability for software developers to work transparently with files whose size exceeds the actual physical memory of the system. In the image below, you can see what the Virtual Memory of a process looks like. Each process has its own region where mmap() maps files.\nWhat I\u0026rsquo;m not touching here is whether to use mmap() or file syscalls like read() and write() in your software. What is better, faster, or safer to use is beyond the scope of this post. But you definitely need to understand how to get the mmap() stats because almost every Page Cache user-space tool uses it.\nLet’s write one more script with mmap(). It prints a PID of the process, maps the test file and sleeps. The sleep time should be enough to play with the process.\nimport mmap import os from time import sleep print(\u0026#34;pid:\u0026#34;, os.getpid()) with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;rb\u0026#34;) as f: with mmap.mmap(f.fileno(), 0, prot=mmap.PROT_READ) as mm:f sleep(10000) Run it in one terminal window, and in another one, run pmap -x PID with the PID of the script.\npmap -x 369029 | less where 369029 is my PID.\nThe output of the pmap shows us all contiguous virtual memory areas (VMA or struct vm_area_struct) of the process. We can determine the virtual addresses of the mmaped test file file1.db. In my case:\nAddress Kbytes RSS Dirty Mode Mapping ... 00007f705cc12000 131072 0 0 r--s- file1.db We can see that we have 0 dirty pages for the file (it only shows the dirty pages of this process). The RSS column equals 0, which tells us how much memory in KiB our process has already referenced. This 0, by the way, doesn\u0026rsquo;t mean that there are no pages of the file in Page Cache. It means that our process hasn\u0026rsquo;t accessed any pages yet. NOTE\npmap can display even more detailed output with -XX. Without -XX, it uses /proc/pid/maps, but for the extended mode it shows stats from /proc/pid/smaps. More info can be found in man 5 proc and kernel documentation filesystems/proc.rst.\nSo, the most exciting part about mmap() for SRE is how it transparently loads data on accessing and writing. And I\u0026rsquo;m showing all this in the following chapters.\nWhat is a page fault? # Before we start talking about file tools, we need to understand the concept of page faults. Generally speaking, the page fault is the CPU mechanism for communicating with the Linux Kernel and its memory subsystem. The page fault is a building block of the Virtual Memory concept and demand paging. Briefly speaking, the kernel usually doesn\u0026rsquo;t allocate physical memory immediately after a memory request is done by mmap() or malloc(). Instead, the kernel creates some records it the process\u0026rsquo;s page table structure and uses it as a storage for its memory promises. In addition, the page table contains extra info for each page, such as memory permissions and page flags (we\u0026rsquo;ve already seen some of them: LRUs flags, dirty flag, etc.).\nFrom the examples in chapter 2, you can see that in order to read mmaped file at any position, the code doesn\u0026rsquo;t need to perform any seeks (man 2 lseek), unlike with file operations. We can just start reading from or writing to any point of the mapped area. For this reason, when an application wants to access a page, the page fault can occur if the target page has not been loaded to Page Cache or there are no connections between the page in the Page Cache and the process\u0026rsquo; page table.\nThere are 2 useful for us types of page faults: minor and major. A minor basically means that there will be no disk access in order to fulfill a process\u0026rsquo;s memory access. And on the other hand, a major page fault means that there will be a disk IO operation.\nFor example, if we load half of a file with dd in Page Cache and afterward access this first half from a program with mmap(), we will trigger minor page faults. The kernel doesn\u0026rsquo;t need to go to disks because these pages were already loaded to Page Cache. The kernel only needs to reference these already loaded pages with the page table entries of the process. But if the process tries to read within the same mmaped area the second half of the file, the kernel will have to go to the disk in order to load the pages, and the system will generate major page faults.\nIf you want to get more information about demand paging, Linux kernel and system internals, please watch \u0026ldquo;Introduction to Memory Management in Linux\u0026rdquo; video from Embedded Linux Conf.\nLet’s do an experiment and write a script with an infinitive random read of the file:\nimport mmap import os from random import randint from time import sleep with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;r\u0026#34;) as f: fd = f.fileno() size = os.stat(fd).st_size with mmap.mmap(fd, 0, prot=mmap.PROT_READ) as mm: try: while True: pos = randint(0, size-4) print(mm[pos:pos+4]) sleep(0.05) except KeyboardInterrupt: pass Now we need 3 terminal windows. In the first one:\n$ sar -B 1 which shows the system memory statistics per second including page faults.\nAnd in the second one, perf trace:\n$ sudo perf trace -F maj --no-syscalls which shows major page faults and corresponding file paths.\nFinally, in the 3rd terminal window, start the above python script:\n$ python3 ./mmap_random_read.py The output should be something closer to the following:\n$ sar -B 1 .... LOOK HERE ⬇ ⬇ 05:45:55 PM pgpgin/s pgpgout/s fault/s majflt/s pgfree/s pgscank/s pgscand/s pgsteal/s %vmeff 05:45:56 PM 8164.00 0.00 39.00 4.00 5.00 0.00 0.00 0.00 0.00 05:45:57 PM 2604.00 0.00 20.00 1.00 1.00 0.00 0.00 0.00 0.00 05:45:59 PM 5600.00 0.00 22.00 3.00 2.00 0.00 0.00 0.00 0.00 ... Take a look at the fault/s and majflt/s fields. They show what I\u0026rsquo;ve just explained.\nAnd from the perf trace, we can get insides about the file where we have major page faults:\n$ sudo perf trace -F maj --no-syscalls ... SCROLL ➡ LOOK HERE ⬇ 5278.737 ( 0.000 ms): python3/64915 majfault [__memmove_avx_unaligned_erms+0xab] =\u0026gt; /var/tmp/file1.db@0x2aeffb6 (d.) 5329.946 ( 0.000 ms): python3/64915 majfault [__memmove_avx_unaligned_erms+0xab] =\u0026gt; /var/tmp/file1.db@0x539b6d9 (d.) 5383.701 ( 0.000 ms): python3/64915 majfault [__memmove_avx_unaligned_erms+0xab] =\u0026gt; /var/tmp/file1.db@0xb3dbc7 (d.) 5434.786 ( 0.000 ms): python3/64915 majfault [__memmove_avx_unaligned_erms+0xab] =\u0026gt; /var/tmp/file1.db@0x18f7c4f (d.) ... The cgroup also has per cgroup stats regarding page faults:\n$ grep fault /sys/fs/cgroup/user.slice/user-1000.slice/session-3.scope/memory.stat ... pgfault 53358 pgmajfault 13 ... Subtle MADV_DONT_NEED mmap() feature # Now let\u0026rsquo;s perform another experiment. Stop all scripts and drop all caches:\n$ sync; echo 3 | sudo tee /proc/sys/vm/drop_caches Restart the script with the infinite read and start monitoring per memory area usage of the process:\nwatch -n 0.1 \u0026#34;grep \u0026#39;file1\u0026#39; /proc/$pid/smaps -A 24\u0026#34; You can now see the mmaped area of the file and its info. The reference field should be growing.\nIn the other window, try to evict pages with vmtouch:\nvmtouch -e /var/tmp/file1.db And notice that the stats from the smaps output don\u0026rsquo;t drop entirely. When you run the vmtouch -e command, the smaps should show you some decrease in memory usage. The question is, what happens? Why when we explicitly ask the kernel to evict the file pages by setting the FADVISE_DONT_NEED flag, some of them are still present in Page Cache?\nThe answer is a little confusing, but very important to understand. If the Linux kernel has no memory pressure issues, why should it drop pages from Page Cache? There is a high probability that the program will need them in the future. But if you, as a software developer, are sure that these pages are useless, there is a madvise() and MADV_DONT_NEED flag for that. It informs the kernel that it can remove these pages from the corresponding page table, and the following vmtouch -e call will successfully be able to expel the file data from Page Cache.\nIn case of the memory pressure situation, the kernel will start reclaiming memory from inactive LRU lists. Which means eventually it can drop these pages if they are good candidates for reclaiming.\nRead next chapter → "},{"id":11,"href":"/docs/page-cache/6-cgroup-v2-and-page-cache/","title":"Cgroup v2 and Page Cache","section":"Linux Page Cache series","content":" Cgroup v2 and Page Cache # The cgroup subsystem is the way to distribute and limit system resources fairly. It organizes all data in a hierarchy where the leaf nodes depend on their parents and inherit their settings. In addition, the cgroup provides a lot of helpful resource counters and statistics.\nThe control groups are everywhere. Even though you may not use them explicitly, they are already turned on by default in all modern GNU/Linux distributives and got integrated into systemd. It means that each service in a modern linux system runs under its own cgroup.\nOverview # We already touched the cgroup subsystem several times during this series, but let\u0026rsquo;s take a closer look at the entire picture now. The cgroup plays a critical role in the understanding Page Cache usage. It also helps to debug issues and configure software better by providing detailed stats. As was told earlier, the LRU lists use cgroup memory limits to make eviction decisions and to size the length of the LRU lists.\nAnother important topic in cgroup v2, which was unachievable with the previous v1, is a proper way of tracking Page Cache IO writebacks. The v1 can\u0026rsquo;t understand which memory cgroup generates disk IOPS and therefore, it incorrectly tracks and limits disk operations. Fortunately, the new v2 version fixes these issues. It already provides a bunch of new features which can help with Page Cache writeback.\nThe simplest way to find out all cgroups and their limits are to go to the /sys/fs/cgroup. But you can use more convenient ways to get such info:\nsystemd-cgls and systemd-top to understand what cgroups systemd has; below a top-like tool for cgroups https://github.com/facebookincubator/below Memory cgroup files # Now let\u0026rsquo;s review the most important parts of the cgroup memory controller from the perspective of Page Cache.\nmemory.current – shows the total amount of memory currently used by the cgroup and its descendants. It, of course, includes Page Cache size. NOTE\nIt may be tempting to use this value in order to set your cgroup/container memory limit, but wait a bit for the following chapter.\nmemory.stat – shows a lot of memory counters, the most important for us can be filtered by file keyword: $ grep file /sys/fs/cgroup/user.slice/user-1000.slice/session-3.scope/memory.stat file 19804160 ❶ file_mapped 0 ❷ file_dirty 0 ❸ file_writeback 0 ❹ inactive_file 6160384 ❺ active_file 13643776 ❺ workingset_refault_file 0 ❻ workingset_activate_file 0 ❻ workingset_restore_file 0 ❻ where\n❶ file – the size of the Page Cache; ❷ file_mapped – mapped file memory size with mmap(); ❸ file_dirty – dirty pages size; ❹ file_writeback – how much data is being flushing at the moment; ❺ inactive_file and active_file – sizes of the LRU lists; ❻ workingset_refault_file, workingset_activate_file and workingset_restore_file – metrics to better understand memory thrashing and refault logic. memory.numa_stat – shows the above stats but for each NUMA node.\nmemory.min, memory.low, memory.high and memory.max – cgroup limits. I don\u0026rsquo;t want to repeat the cgroup v2 doc and recommend you to go and read it first. But what you need to keep in mind is that using the hard max or min limits is not the best strategy for your applications and systems. The better approach you can choose is to set only low and/or high limits closer to what you think is the working set size of your application. We will talk about measuring and predicting in the next section.\nmemory.events – shows how many times the cgroup hit the above limits:\nemory.events low 0 high 0 max 0 oom 0 oom_kill 0 memory.pressure – this file contains Pressure Stall Information (PSI). It shows the general cgroup memory health by measuring the CPU time that was lost due to lack of memory. This file is the key to understanding the reclaiming process in the cgroup and, consequently, Page Cache. Let\u0026rsquo;s talk about PSI in more detail. Pressure Stall Information (PSI) # Back before PSI times, it was hard to tell whether a system and/or a cgroup has resource contention or not; whether a cgroup limits are overcommitted or under-provisioned. If the limit for a cgroup can be set lower, then where is its threshold? The PSI feature mitigates these confusions and not only allows us to get this information in real-time but also allows us to set up user-space triggers and get notifications to maximize hardware utilization without service degradation and OOM risks.\nThe PSI works for memory, CPU and IO controllers. For example, the output for memory:\nsome avg10=0.00 avg60=0.00 avg300=0.00 total=0 full avg10=0.00 avg60=0.00 avg300=0.00 total=0 where\nsome – means that at least one task was stalled on memory for some average percentage of wall-time during 10, 60 and 300 seconds. The \u0026ldquo;total\u0026rdquo; field shows the absolute value in microseconds in order to reveal any spikes; full – means the same but for all tasks in the cgroup. This metric is a good indication of issues and usually means underprovisioning of the resource or wrong software settings. EXAMPLE\nsystemd-oom daemon, which is a part of modern GNU/Linux systems, uses the PSI to be more proactive than kernel\u0026rsquo;s OOM in recognition of memory scarcity and finding targets for killing.\nI also highly recommend reading the original PSI doc.\nWriteback and IO # One of the most significant features of the cgroup v2 implementation is the possibility to track, observe and limit Page Cache async writeback for each cgroup. Nowadays, the kernel writeback process can identify which cgroup IO limit to use in order to persist dirty pages to disks.\nBut what is also important is that it works in another direction too. If a cgroup experiences memory pressure and tries to reclaim some pages by flushing its dirty pages, it will use its own IO limits and won\u0026rsquo;t harm the other cgroups. Thus the memory pressure translates into the disk IO and if there is a lot of writes, eventually, into the disk pressure for the cgroup. Both controllers have the PSI files, which should be used for proactive management and tuning your software settings.\nIn order to control dirty pages flush frequency, the linux kernel has several sysctl knobs. If you want, you can make the background writeback process more or less aggressive:\n$ sudo sysctl -a | grep dirty vm.dirty_background_bytes = 0 vm.dirty_background_ratio = 10 vm.dirty_bytes = 0 vm.dirty_expire_centisecs = 3000 vm.dirty_ratio = 20 vm.dirty_writeback_centisecs = 500 vm.dirtytime_expire_seconds = 43200 Some of the above works for cgroups too. The kernel chooses and applies what reaches first for the entire system or for a cgroup.\nThe cgroup v2 also brings new IO controllers: io.cost and io.latency. They provide 2 different approaches for limiting and guaranteeing disk operations. Please, read the cgroup v2 documentation for more details and distinctions. But I would say that if your setup is not complex, starting with less invasive io.latency makes sense.\nAs with the memory controller, the kernel also provides a bunch of files to control and observe IO:\nio.stat – the stat file with per device data; io.latency – the latency target time in microseconds; io.pressure – the PSI file; io.weight – the target weight if io.cost was chosen; io.cost.qos and io.cost.model – the configuration file of the io.cost cgroup controller. Memory and IO cgroup ownership # Several processes from multiple cgroups can obviously work with the same files. For example, cgroup1 can open and read the first 10 KiB of the file, and sometime later, another cgroup2 can append 2 KiB to the end of the same file and read the first 4KiB. The question is, whose memory and IO limits will the kernel use?\nThe logic of memory ownership (therefore and Page Cache) is built based on each page. The ownership of a page is charged on the first access (page fault) and won\u0026rsquo;t switch to any other cgroup until this page will be completely reclaimed and evicted. The term ownership means that these pages will be used to calculate the cgroup Page Cache usage and will be included in all stats.\nFor example, cgroup1 is the owner of the first 10KiB, and cgroup2 – is the owner of the last 2KiB. No matter what cgroup1 will do with the file, it can even close it, cgroup1 remains the owner of the first 4KiB (not all 10KiB) as long as cgroup2 works with this first 4KiB of the file. In this situation, kernel keeps the pages in Page Caches and keeps updating LRU lists accordingly.\nFor the cgroup IO, ownership works per inode. So for our example cgroup2 owns all writeback operations for the file. The inode is assigned to the cgroup on the first writeback, but unlike the memory ownership logic, the IO ownership may migrate to another cgroup if the kernel notices that this other cgroup generates more dirty pages.\nIn order to troubleshoot memory ownership, we should use the pair of procfs files: /proc/pid/pagemap and /proc/kpagecgroup. The page-type tool supports showing per page cgroup information, but it\u0026rsquo;s hard to use it for a directory of files and get a well-formatted output. That\u0026rsquo;s why I wrote my own cgtouch tool in order to troubleshoot cgroup memory ownership.\n$ sudo go run ./main.go /var/tmp/ -v /var/tmp/file1.db cgroup inode percent pages path - 85.9% 28161 not charged 1781 14.1% 4608 /sys/fs/cgroup/user.slice/user-1000.slice/session-3.scope -- /var/tmp/ubuntu-21.04-live-server-amd64.iso cgroup inode percent pages pat - 0.0% 0 not charged 2453 100.0% 38032 /sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/app.slice/run-u10.service -- Files: 2 Directories: 7 Resident Pages: 42640/70801 166.6M/276.6M 60.2% cgroup inode percent pages path - 39.8% 28161 not charged 1781 6.5% 4608 /sys/fs/cgroup/user.slice/user-1000.slice/session-3.scope 2453 53.7% 38032 /sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/app.slice/run-u10.service Safe ad-hoc tasks # Let\u0026rsquo;s assume we need to run the wget command or manually install some packages by calling a configuration management system (e.g. saltstack). Both of these tasks can be unpredictably heavy for disk I/O. In order to run them safely and not interact with any production load, we should not run them in the root cgroup or the current terminal cgroup, because they usually don\u0026rsquo;t have any limits. So we need a new cgroup with some limits. It would be very tedious and cumbersome to manually create a cgroup for your task and manually configure it for every ad-hoc task. But fortunately, we don\u0026rsquo;t have to, so all modern GNU/Linux distributives come with the systemd out of the box with cgroup v2. The systemd-run with many other cool features from the systemd makes our life easier and saves a lot of time.\nSo, for example, wget task can be run in the following manner:\nsystemd-run --user -P -t -G --wait -p MemoryMax=12M wget http://ubuntu.ipacct.com/releases/21.04/ubuntu-21.04-live-server-amd64.iso Running as unit: run-u2.service ⬅ LOOK HERE Press ^] three times within 1s to disconnect TTY. --2021-09-11 19:53:33-- http://ubuntu.ipacct.com/releases/21.04/ubuntu-21.04-live-server-amd64.iso Resolving ubuntu.ipacct.com (ubuntu.ipacct.com)... 195.85.215.252, 2a01:9e40::252 Connecting to ubuntu.ipacct.com (ubuntu.ipacct.com)|195.85.215.252|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 1174243328 (1.1G) [application/octet-stream] Saving to: ‘ubuntu-21.04-live-server-amd64.iso.5’ ... The run-u2.service is my brand new cgroup with a memory limit. I can get its metrics:\n$ find /sys/fs/cgroup/ -name run-u2.service /sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/app.slice/run-u2.service $ cat /sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/app.slice/run-u2.service/memory.pressure some avg10=0.00 avg60=0.00 avg300=0.00 total=70234 full avg10=0.00 avg60=0.00 avg300=0.00 total=69717 $ grep file /sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/app.slice/run-u2.service/memory.stat file 11100160 file_mapped 0 file_dirty 77824 file_writeback 0 file_thp 0 inactive_file 5455872 active_file 5644288 workingset_refault_file 982 workingset_activate_file 0 workingset_restore_file 0 As you can see from the above we have near 12MiB file memory and some refault.\nTo get all power of systemd and cgroup please read its resource control doc.\nRead next chapter → "},{"id":12,"href":"/docs/page-cache/7-how-much-memory-my-program-uses-or-the-tale-of-working-set-size/","title":"Unique set size and working set size","section":"Linux Page Cache series","content":" How much memory my program uses or the tale of working set size # Currently, in the world of containers, auto-scaling, and on-demand clouds, it\u0026rsquo;s vital to understand the resource needs of services both in norman regular situations and under pressure near the software limits. But every time someone touches on the topic of memory usage, it becomes almost immediately unclear what and how to measure. RAM is a valuable and often expensive type of hardware. In some cases, its latency is even more important than disk latency. Therefore, the Linux kernel tries as hard as it can to optimize memory utilization, for instance by sharing the same pages among processes. In addition, the Linux Kernel has its Page Cache in order to improve storage IO speed by storing a subset of the disk data in memory. Page Cache not only, by its nature, performs implicit memory sharing, which usually confuses users, but also actively asynchronously works with the storage in the background. Thus, Page Cache brings even more complexity to the table of memory usage estimation.\nIn this chapter, I\u0026rsquo;m demonstrating some approaches you can use to determine your initial values for the memory (and thus Page Cache) limits and start your journey from a decent starting point.\nIt\u0026rsquo;s all about who counts or the story of unique set size # The 2 most common questions I\u0026rsquo;ve heard about memory and Linux are:\nWhere is all my free memory? How much memory does you/my/their application/service/database use? The first question\u0026rsquo;s answer should already be obvious to the reader (whispering \u0026ldquo;Page Cache\u0026rdquo;). But the second one is much more trickier. Usually, people think that the RSS column from the top or ps output is a good starting point to evaluate memory utilization. Although this statement may be correct in some cases, it can usually lead to misunderstanding of Page Cache importance and its impact on the service performance and reliability.\nLet\u0026rsquo;s take a well-known top (man 1 top) tool as an example in order to investigate its memory consumption. It\u0026rsquo;s written in C and it does nothing but prints process\u0026rsquo; stats in the loop. top doesn\u0026rsquo;t heavily work with disks and thus Page Cache. It doesn\u0026rsquo;t touch the network. Its only purpose is to read data from the procfs and to show it to the user in a friendly format. So it should be easy to understand its working set, shouldn\u0026rsquo;t it?\nLet\u0026rsquo;s start the top process in a new cgroup:\n$ systemd-run --user -P -t -G --wait top And in another terminal let\u0026rsquo;s start our learning. Begin with the ps:\n$ ps axu | grep top USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ... vagrant 611963 0.1 0.2 10836 4132 pts/4 Ss+ 11:55 0:00 /usr/bin/top ... ⬆ LOOK HERE As you can see from the above, the top process uses ~4MiB of memory according to the ps output.\nNow let\u0026rsquo;s get more details from the procfs and its /proc/pid/smaps_rollup file, which is basically a sum of all memory areas from the /proc/pid/smaps. For my PID:\n$ cat /proc/628011/smaps_rollup 55df25e91000-7ffdef5f7000 ---p 00000000 00:00 0 [rollup] Rss: 3956 kB ⓵ Pss: 1180 kB ⓶ Pss_Anon: 668 kB Pss_File: 512 kB Pss_Shmem: 0 kB Shared_Clean: 3048 kB ⓷ Shared_Dirty: 0 kB ⓸ Private_Clean: 240 kB Private_Dirty: 668 kB Referenced: 3956 kB ⓹ Anonymous: 668 kB ⓺ ... Where we mostly care about the following rows:\n⓵ – A well know RSS metric and what we\u0026rsquo;ve seen in the ps output. ⓶ – PSS stands for the process\u0026rsquo; proportional share memory. It\u0026rsquo;s an artificial memory metric and it should give you some insights about memory sharing: The \u0026ldquo;proportional set size\u0026rdquo; (PSS) of a process is the count of pages it has in memory, where each page is divided by the number of processes sharing it. So if a process has 1000 pages all to itself and 1000 shared with one other process, its PSS will be 1500.\n⓷ Shared_Clean – is an interesting metric. As we assumed earlier, our process should not use any Page Cache in theory, but it turns out it does use Page Cache. And as you can see, it\u0026rsquo;s the predominant part of memory usage. If you open a per area file /proc/pid/smaps, you can figure out that the reason is shared libs. All of them were opened with mmap() and are resident in Page Cache. ⓸ Shared_Dirty – If our process writes to files with mmap(), this line will show the amount of unsaved dirty Page Cache memory. ⓹ Referenced - indicates the amount of memory the process has marked as referenced or accessed so far. We touched on this metric in mmap() section. And if there is no memory pressure, it should be close to RSS. ⓺ Anonymous – shows the amount of memory that does not belong to any files. From the above, we can see that, although top\u0026rsquo;s RSS is 4MiB, most of its RSS is hidden in Page Cache. And in theory, if these pages become inactive for a while, the kernel can evict them from memory.\nLet\u0026rsquo;s take a look at the cgroup stats as well:\n$ cat /proc/628011/cgroup 0::/user.slice/user-1000.slice/user@1000.service/app.slice/run-u2.service $ cat /sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/app.slice/run-u2.service/memory.stat anon 770048 file 0 ... file_mapped 0 file_dirty 0 file_writeback 0 ... inactive_anon 765952 active_anon 4096 inactive_file 0 active_file 0 ... We can not see any file memory in the cgroup. That is another great example of the cgroup memory charging feature. Another cgroup has already accounted these libs.\nAnd to finish and recheck ourselves, let\u0026rsquo;s use the page-type tool:\n$ sudo ./page-types --pid 628011 --raw flags page-count MB symbolic-flags long-symbolic-flags 0x2000010100000800 1 0 ___________M_______________r_______f_____F__ mmap,reserved,softdirty,file 0xa000010800000868 39 0 ___U_lA____M__________________P____f_____F_1 uptodate,lru,active,mmap,private,softdirty,file,mmap_exclusive 0xa00001080000086c 21 0 __RU_lA____M__________________P____f_____F_1 referenced,uptodate,lru,active,mmap,private,softdirty,file,mmap_exclusive 0x200001080000086c 830 3 __RU_lA____M__________________P____f_____F__ referenced,uptodate,lru,active,mmap,private,softdirty,file 0x8000010000005828 187 0 ___U_l_____Ma_b____________________f_______1 uptodate,lru,mmap,anonymous,swapbacked,softdirty,mmap_exclusive 0x800001000000586c 1 0 __RU_lA____Ma_b____________________f_______1 referenced,uptodate,lru,active,mmap,anonymous,swapbacked,softdirty,mmap_exclusive total 1079 4 We can see that the memory of the top process has file mmap() areas and thus uses Page Cache.\nNow let\u0026rsquo;s get a unique memory set size for our top process. The unique memory set size or USS for the process is an amount of memory which only this target process uses. This memory could be sharable but still, be in the USS if no other processes use it.\nWe can use the page-types with -N flag and some shell magic to calculate the USS of the process:\n$ sudo ../vm/page-types --pid 628011 --raw -M -l -N | awk \u0026#39;{print $2}\u0026#39; | grep -E \u0026#39;^1$\u0026#39; | wc -l 248 The above means that 248 pages or 992 KiB is the unique set size (USS) of the top process.\nOr we can use our knowledge about /proc/pid/pagemap, /proc/kpagecount and /proc/pid/maps and write our own tool to get the unique set size. The full code of such tool can be found in the github repo.\nIf we run it, we should get the same output as page-type gave us:\n$ sudo go run ./main.go 628011 248 Now that we understand how it can be hard to estimate the memory usage and the importance of Page Cache in such calculations, we are ready to make a giant leap forward and start thinking about software with more active disk activities.\nIdle pages and working set size # Readers who have gotten this far may be curious about one more kernel file: /sys/kernel/mm/page_idle.\nYou can use it to estimate the working set size of a process. The main idea is to mark some pages with the special idle flag and, after some time, check the difference-making assumptions about the working data set size.\nYou can find great reference tools in Brendan Gregg\u0026rsquo;s repository.\nLet\u0026rsquo;s run it for our top process:\n$ sudo ./wss-v1 628011 60 Watching PID 628011 page references during 60.00 seconds... Est(s) Ref(MB) 60.117 2.00 The above means that from the 4MiB of RSS data, the process uses only 2 MiB during the 60-second interval.\nFor more information, you can also read this LWN article.\nThe drawbacks of this method are the following:\nit can be slow for a process with a huge memory footprint; all measurements happen in the user space and thus consume additional CPU; it completely detached from the possible writeback pressure your process can generate. Although it could be a reasonable starting limit for your containers, I will show you a better approach using cgroup stats and pressure stall information (PSI).\nCalculating memory limits with Pressure Stall Information (PSI) # As you can see throughout the series, I emphasize that running all services in their own cgroups with carefully configured limits is very important. It usually leads to better service performance and more uniform and correct use of system resources.\nBut what is still unclear is where to start. Which value to choose? Is it good to use the memory.current value? Or use the unique set size? Or estimate the working set size with the idle page flag? Though all these ways may be useful in some situations, I would suggest using the following PSI approach for a general case.\nOne more note about the memory.current before I continue with the PSI. If a cgroup doesn\u0026rsquo;t have a memory limit and the system has a lot of free memory for the process, the memory.current simply shows all the memory (including Page Cache) that your application has touched up to that point. It can include a lot of garbage your application doesn\u0026rsquo;t need for its runtime. For example, logs records, unneeded libs, etc. Using the memory.current value as a memory limit would be wasteful for the system and will not help you in capacity planning.\nThe modern approach to address this hard question is to use PSI in order to understand how a cgroup reacts to new memory allocations and Page Cache evictions. senapi is a simple automated script that collects and parses the PSI info and adjusts the memory.high:\nLet\u0026rsquo;s experiment with my test MongoDB installation. I have 2.6GiB of data:\n$ sudo du -hs /var/lib/mongodb/ 2.4G /var/lib/mongodb/ Now I need to generate some random read queries. In mongosh I can run an infinite while loop and read a random record every 500 ms:\nwhile (true) { printjson(db.collection.aggregate([{ $sample: { size: 1 } }])); sleep(500); } In the second terminal window, I start the senpai with the mongodb service cgroup:\nsudo python senpai.py /sys/fs/cgroup/system.slice/mongodb.service 2021-09-05 16:39:25 Configuration: 2021-09-05 16:39:25 cgpath = /sys/fs/cgroup/system.slice/mongodb.service 2021-09-05 16:39:25 min_size = 104857600 2021-09-05 16:39:25 max_size = 107374182400 2021-09-05 16:39:25 interval = 6 2021-09-05 16:39:25 pressure = 10000 2021-09-05 16:39:25 max_probe = 0.01 2021-09-05 16:39:25 max_backoff = 1.0 2021-09-05 16:39:25 coeff_probe = 10 2021-09-05 16:39:25 coeff_backoff = 20 2021-09-05 16:39:26 Resetting limit to memory.current. ... 2021-09-05 16:38:15 limit=503.90M pressure=0.030000 time_to_probe= 1 total=1999415 delta=601 integral=3366 2021-09-05 16:38:16 limit=503.90M pressure=0.030000 time_to_probe= 0 total=1999498 delta=83 integral=3449 2021-09-05 16:38:16 adjust: -0.000840646891233154 2021-09-05 16:38:17 limit=503.48M pressure=0.020000 time_to_probe= 5 total=2000010 delta=512 integral=512 2021-09-05 16:38:18 limit=503.48M pressure=0.020000 time_to_probe= 4 total=2001688 delta=1678 integral=2190 2021-09-05 16:38:19 limit=503.48M pressure=0.020000 time_to_probe= 3 total=2004119 delta=2431 integral=4621 2021-09-05 16:38:20 limit=503.48M pressure=0.020000 time_to_probe= 2 total=2006238 delta=2119 integral=6740 2021-09-05 16:38:21 limit=503.48M pressure=0.010000 time_to_probe= 1 total=2006238 delta=0 integral=6740 2021-09-05 16:38:22 limit=503.48M pressure=0.010000 time_to_probe= 0 total=2006405 delta=167 integral=6907 2021-09-05 16:38:22 adjust: -0.00020961438729431614 As you can see, according to the PSI, 503.48M of memory should be enough to support my reading work load without any problems.\nThis is obviously a preview of the PSI features and for real production services, you probably should think about io.pressure as well.\n\u0026hellip; and what about writeback? # To be honest, this question is more difficult to answer. As I write this article, I do not know of a good tool for evaluating and predicting writeback and IO usage. However, the rule of thumb is to start with io.latency and then try to use io.cost if needed.\nThere is also an interesting new project resctl-demo which can help in proper limits identification.\nRead next chapter → "},{"id":13,"href":"/docs/page-cache/8-direct-io-dio/","title":"Direct IO","section":"Linux Page Cache series","content":" Direct IO (DIO) (NOT READY) # As usual, there is always an exception to any rule. And Page Cache is no different. So let\u0026rsquo;s talk about file reads and writes, which can ignore Page Cache content.\nWhy it’s good # Some applications require low-level access to the storage subsystem and the linux kernel gives such a feature by providing O_DIRECT file open flag. This IO is called the Direct IO or DIO. A program, which opens a file with this flag, bypasses the kernel Page Cache completely and directly communicates with the VFS and the underlying filesystem.\nThe pros are:\nLower CPU usage and thus higher throughput you can get; Linux Async IO (man 7 aio) works only with DIO (io_submit); zero-copy Avoiding double buffering () between Page Cache and user-space buffers; More control over the writeback. \u0026hellip; Why it’s bad and io_uring alternative # need to align read and writes to the block size; not all file systems are the same in implementing DIO; DIO without Linux AIO is slow and not useful at all; not cross-platform; DIO and buffered IO can\u0026rsquo;t be performed at the same time for the file. \u0026hellip; DIO usually makes no sense without AIO, but AIO has a lot of bad design decisions:\nSo I think this is ridiculously ugly.\nAIO is a horrible ad-hoc design, with the main excuse being \u0026ldquo;other, less gifted people, made that design, and we are implementing it for compatibility because database people - who seldom have any shred of taste - actually use it\u0026rdquo;.\nBut AIO was always really really ugly.\nLinus Torvalds Heads-up! With DIO still need to run fsync() on a file! Let\u0026rsquo;s write an example with golang and iouring-go library:\nTODO Read next chapter → "},{"id":14,"href":"/docs/page-cache/9-advanced-page-cache-observability-and-troubleshooting-tools/","title":"Advanced Page Cache observability and troubleshooting tools","section":"Linux Page Cache series","content":" Advanced Page Cache observability and troubleshooting tools # Let\u0026rsquo;s touch on some advanced tools we can use to perform low-level kernel tracing and debugging.\neBPF tools # First of all, we can use eBPF tools. The [bcc]https://github.com/iovisor/bcc and bpftrace are your friends when you want to get some internal kernel information.\nLet\u0026rsquo;s take a look at some tools which come with it.\nWriteback monitor # $ sudo bpftrace ./writeback.bt Attaching 4 probes... Tracing writeback... Hit Ctrl-C to end. TIME DEVICE PAGES REASON ms 15:01:48 btrfs-1 7355 periodic 0.003 15:01:49 btrfs-1 7355 periodic 0.003 15:01:51 btrfs-1 7355 periodic 0.006 15:01:54 btrfs-1 7355 periodic 0.005 15:01:54 btrfs-1 7355 periodic 0.004 15:01:56 btrfs-1 7355 periodic 0.005 Page Cache Top # 19:49:52 Buffers MB: 0 / Cached MB: 610 / Sort: HITS / Order: descending PID UID CMD HITS MISSES DIRTIES READ_HIT% WRITE_HIT% 66229 vagrant vmtouch 44745 44032 0 50.4% 49.6% 66229 vagrant bash 205 0 0 100.0% 0.0% 66227 root cachetop 17 0 0 100.0% 0.0% 222 dbus dbus-daemon 16 0 0 100.0% 0.0% 317 vagrant tmux: server 4 0 0 100.0% 0.0% Cache stat # [vagrant@archlinux tools]$ sudo ./cachestat HITS MISSES DIRTIES HITRATIO BUFFERS_MB CACHED_MB 10 0 0 100.00% 0 610 4 0 0 100.00% 0 610 4 0 0 100.00% 0 610 21 0 0 100.00% 0 610 624 0 0 100.00% 0 438 2 0 0 100.00% 0 438 4 0 0 100.00% 0 438 0 0 0 0.00% 0 438 19 0 0 100.00% 0 438 0 428 0 0.00% 0 546 28144 16384 0 63.21% 0 610 0 0 0 0.00% 0 610 0 0 0 0.00% 0 610 17 0 0 100.00% 0 610 0 0 0 0.00% 0 610 bpftrace and kfunc trace # Other than that, eBPF and bpftrace have recently got a new great feature named kfunc. Thus, using it, you can trace some kernel functions without kernel debugging information installed.\nIt\u0026rsquo;s still close to experimental functionality, but it looks really promising.\nPerf tool # But if you want to go deeper, I have something for you. perf allows you to set up dynamic tracing kernel probes almost at any kernel function. The only issue is the kernel debug information should be installed. Unfortunately, not all distributives provide it and, sometimes, you will need to recompile the kernel manually with some additional flags.\nBut when you get the debug info, you can perform really crazy investigations. For example, if we want to track the major page faults, we can find the kernel function which is in charge (https://elixir.bootlin.com/linux/latest/source and its search for help) and setup a probe:\nperf probe -f \u0026#34;do_read_fault vma-\u0026gt;vm_file-\u0026gt;f_inode-\u0026gt;i_ino\u0026#34; where do_read_fault is our kernel function and vma-\u0026gt;vm_file-\u0026gt;f_inode-\u0026gt;i_ino is an inode number of the file where the major page fault occurs.\nNow you can start recording events:\nperf record -e probe:do_read_fault -ag -- sleep 10 And after 10 seconds, we can grep out the inodes with perf script and bash magic:\nperf script | grep i_ino | cut -d \u0026#39; \u0026#39; -f 1,8| sed \u0026#39;s#i_ino=##g\u0026#39; | sort | uniq -c | sort -rn "}]