[{"id":0,"href":"/posts/trixter-chaos-proxy/","title":"Trixter: A Chaos Proxy for Simulating Network Faults","section":"Posts","content":" Posted: Oct 2025 Github: https://github.com/brk0v/trixter Contents\nChaos Engineering and Network Fault Injection Introducing Trixter – A Chaos Monkey for TCP Why Trixter vs GNU/Linux tc netem (Kernel Network Emulator) Using Trixter: Examples of Injecting Chaos Example 1: Adding latency and packet loss Example 2: Throttling bandwidth Example 3: Running in CI/CD with chaos Example 4: Control failures on per connection in runtime Documentation, recipes, reference Break your network before production does\nChaos Engineering and Network Fault Injection # Chaos engineering is the practice of deliberately injecting controlled failures into systems to observe their behavior and improve their resilience. One common chaos experiment is network-level fault injection, which involves artificial inserting network problems like latency, packet loss, or corruption to test how services cope. This is valuable because many modern systems are distributed (at least have clients and servers); introducing network slowness or errors can reveal hidden bugs, timeout issues, or insufficient retry logic in microservices. By simulating unreliable networks in a controlled way, SREs can ensure their applications are resilient against real-world network chaos (like spikes in latency or occasional outages).\nIntroducing Trixter – A Chaos Monkey for TCP # Trixter is a high-performance chaos proxy designed for injecting network faults at the TCP layer. In essence, it’s a TCP proxy that sits between a client and server, forwarding traffic but intentionally sabotaging it according to your specifications.\nTrixter is runtime-tunable too, so you can adjust fault parameters on the fly per connection without restarting the proxy.\nHow it works: You run Trixter as a proxy on a specified listening port, pointing it to an upstream service. Under normal conditions, it simply relays traffic. But you can configure Trixter to inject various network failures in transit. For example, add artificial delay to each packet, throttle the bandwidth, drop a connection with some probability, corrupt a percentage of packets by injecting a trash, or even terminate connections to simulate outages by some timeout or by calling a REST API call.\nNote\nThink of Trixter as a minimalist, blazing-fast layer written in Rust using the async Tokio framework for efficiency.\nInternally, it leverages the tokio-netem library (netem = network emulator), which provides pluggable async I/O adaptors to simulate latency, bandwidth limits, packet slicing, connection termination, and data corruption at the stream level.\nThis user-space approach means no kernel modules or root access is needed – the chaos is applied in the application layer (within the proxy) rather than the OS network stack.\nWhat makes Trixter interesting is its combination of performance, portability, and simplicity. Being written in Rust, it’s memory-safe and built for speed. Configuration is straightforward: Trixter uses simple cli arguments and REST JSON API for per connection runtime changes, making it easy to emulate chaos scenarios.\nThe binary size is also quite small (3.3M), making it easy to bootstrap in every test suite run:\n$ ls -lh ./target/release/trixter -rwxrwxr-x 2 user user 3.3M Oct 10 21:39 ./target/release/trixter If you want to jump straight to the examples and skip the comparison to Linux kernel capabilities.\nWhy Trixter vs GNU/Linux tc netem (Kernel Network Emulator) # If you’ve ever simulated network faults on GNU/Linux, you might know tc netem, the traffic control tool’s network emulator. netem is powerful – it can impose delay, packet loss, duplication, reordering, and bandwidth limits at the kernel level. However, using tc netem in practice has some drawbacks for developers and SREs:\nUsability: netem must be configured via the tc command syntax, which can be arcane. For example, to add 100ms latency and 5% packet loss on interface eth0, you’d run something like: tc qdisc add dev eth0 root netem delay 100ms loss 5%. It’s a manual process, and applying it to specific traffic (say, only one service or container) often requires setting up traffic control filters or isolation networks. Trixter, on the other hand, is as easy as running a proxy and setting cli arguments with the faults. You point your service or client to the proxy address, and only that traffic gets the chaos. No special networking setup needed.\nRoot Privileges: Configuring tc netem requires root (CAP_NET_ADMIN) privileges on the host. This is fine in a local VM but problematic in environments where you can’t easily get root (Kubernetes pods, developer laptops on macOS/Windows, CI pipelines, etc.). Trixter requires no root – it runs in userland. Anyone can use it in a dev environment or CI job, and it can even be packaged as a container sidecar to inject faults for a specific application.\nPortability: netem is Linux-only (built into the kernel). If you’re developing on macOS or running tests on Windows, tc netem isn’t natively available. Trixter’s user-space proxy approach works across platforms, since it’s just a Rust binary. This makes it accessible to a wider range of use cases and teams.\nDynamic Control: While you can adjust netem parameters by running more tc commands, it’s not designed for frequent on-the-fly changes (and each change affects the whole interface traffic globally). There are ways, but they are not ergonomic and usually requires some iptables configuration. Trixter is designed to be dynamically tunable. It provides an optional API endpoint you can run (e.g. REST JSON API) to tweak fault settings at runtime. In chaos experiments, being able to script fault injection (start with 0ms latency, then gradually increase to 500ms, etc.) is very useful – Trixter was built with this in mind.\nScope and Limitations: netem operates at the packet level and can affect any protocol (TCP, UDP, ICMP, etc.) on the chosen interface. Trixter focuses on TCP streams (it’s a TCP proxy), which covers common cases like HTTP calls, gRPC, database connections, etc. The benefit is that Trixter targets exactly the connection you care about, rather than messing with the entire network stack of a host.\nIn short, Trixter trades some of the breadth of kernel-level simulation for ease-of-use and flexibility. It’s an ideal choice for testing how your service handles network blips without the complexity of system-wide tools.\nUsing Trixter: Examples of Injecting Chaos # Using Trixter is straightforward. First, you’ll run the proxy pointing to a real service. Then you define what network conditions to impose. Below are a couple of examples.\nFor your test and play you have two main options:\nSetup global failures with cli arguments; and/or control failures per connection with REST JSON API. Example 1: Adding latency and packet loss # Suppose you have a service running on localhost:3000 that you want to test with high latency and some connection loss. You could run Trixter with a config like this:\n$ docker run --network host -it --rm ghcr.io/brk0v/trixter \\ --listen 0.0.0.0:8080 \\ --upstream 127.0.0.1:3000 \\ --api 127.0.0.1:8888 \\ --delay-ms 1000 \\ --terminate-probability-rate 0.001 \\ --connection-duration-ms 5000 Here the new port to connect is 8080, every read and write will be delayed by 1 second, any I/0 on 0.1% probability fails, and every connection is terminated with TCP RST packet after 5 seconds.\nThis could be used, for example, to see how your app’s retry logic handles a few lost responses or how a web UI behaves on a slow connection (does a loading spinner show up, do requests time out gracefully, etc.).\nExample 2: Throttling bandwidth # Trixter can also emulate bandwidth constraints. Let’s say you want to simulate a slow network (congested mobile network). You can configure a throttle in bytes per second. For instance:\n$ docker run --network host -it --rm ghcr.io/brk0v/trixter \\ --listen 0.0.0.0:8080 \\ --upstream 127.0.0.1:3000 \\ --api 127.0.0.1:8888 \\ --throttle-rate-bytes 1048576 This will make Trixter buffer and drip-feed data to achieve roughly 1 MB/s, regardless of how fast the real upstream is. It’s a great way to test video streaming or large file downloads under limited network conditions. Your service might start queuing or compressing data differently once this bottleneck is introduced. This is a excellent way to test your application under network backpressure conditions.\nExample 3: Running in CI/CD with chaos # A powerful way to integrate Trixter into your pipeline is to run it automatically during integration or E2E tests with periodic connection drops and random read/write bytes injections.\nThis lets you uncover non-deterministic failure patterns – but still reproduce them later.\nFor example, create an integration test with something like:\n$ docker run --network host -it --rm ghcr.io/brk0v/trixter \\ --listen 0.0.0.0:8080 \\ --upstream 127.0.0.1:3000 \\ --api 127.0.0.1:8888 \\ --terminate-probability-rate 0.001 \\ --corrupt-probability-rate 0.001 Each run it would pick a random seed for chaos parameters. If a test fails, you can open stdout.log and look for a line like:\n2025-10-10T20:38:43.925064Z INFO trixter: random seed: 10382352052268666911 Then, reproduce the exact same chaos locally:\n$ docker run --network host -it --rm ghcr.io/brk0v/trixter \\ --listen 0.0.0.0:8080 \\ --upstream 127.0.0.1:3000 \\ --api 127.0.0.1:8888 \\ --terminate-probability-rate 0.001 \\ --corrupt-probability-rate 0.01 \\ --random-seed 10382352052268666911 This pattern makes chaos deterministic, reproducible, and CI-friendly – like property-based testing for your network.\nExample 4: Control failures on per connection in runtime # Spin up the Trixter proxy without default failures:\n$ docker run --network host -it --rm ghcr.io/brk0v/trixter \\ --listen 0.0.0.0:8080 \\ --upstream 127.0.0.1:3000 \\ --api 127.0.0.1:8888 Make a connection with your client/application/service, and discover it with REST JSON API:\n$ curl -s http://127.0.0.1:8888/connections | jq [ { \u0026#34;conn_info\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;1J8UO7eCuqUMqdoP5KmvN\u0026#34;, \u0026#34;downstream\u0026#34;: \u0026#34;127.0.0.1:45528\u0026#34;, \u0026#34;upstream\u0026#34;: \u0026#34;127.0.0.1:3000\u0026#34; }, \u0026#34;delay\u0026#34;: { \u0026#34;secs\u0026#34;: 0, \u0026#34;nanos\u0026#34;: 0 }, \u0026#34;throttle_rate\u0026#34;: 0, \u0026#34;slice_size\u0026#34;: 0, \u0026#34;terminate_probability_rate\u0026#34;: 0.0, \u0026#34;corrupt_probability_rate\u0026#34;: 0.0 } ] Store the connection ID in variable to future usage:\n$ ID=$(curl -s http://127.0.0.1:8888/connections | jq -r \u0026#39;.[0].conn_info.id\u0026#39;) Now we can easily inject failures – for instance, add a 1 second latency:\n$ curl -i -X PATCH \\ http://127.0.0.1:8888/connections/$ID/delay \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;delay_ms\u0026#34;:1000}\u0026#39; And check that it applied:\n$ curl -s http://127.0.0.1:8888/connections | jq [ { \u0026#34;conn_info\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;1J8UO7eCuqUMqdoP5KmvN\u0026#34;, \u0026#34;downstream\u0026#34;: \u0026#34;127.0.0.1:45528\u0026#34;, \u0026#34;upstream\u0026#34;: \u0026#34;127.0.0.1:3000\u0026#34; }, \u0026#34;delay\u0026#34;: { \u0026#34;secs\u0026#34;: 1, # \u0026lt;-------------------------- Changed \u0026#34;nanos\u0026#34;: 0 }, \u0026#34;throttle_rate\u0026#34;: 0, \u0026#34;slice_size\u0026#34;: 0, \u0026#34;terminate_probability_rate\u0026#34;: 0.0, \u0026#34;corrupt_probability_rate\u0026#34;: 0.0 } ] This way, you can build a flexible test setup or gradually introduce latency, throttling, and other network conditions.\nDocumentation, recipes, reference # For more examples, API reference for controlling per connection settings in run time and recipes please go to https://github.com/brk0v/trixter.\nSummary # Trixter is a lightweight, blazing-fast chaos proxy for SREs and developers.\nIt bridges the gap between kernel tools like tc netem and high-level testing frameworks, letting you inject network chaos safely and precisely.\nRun it locally, use it in CI with random seeds, and reproduce failures with one command – all without root privileges.\nIf you care about resilience and performance under adverse conditions – or just want to break things the smart way – give Trixter a try.\n"},{"id":1,"href":"/docs/async-rust-tokio-io/1-async-rust-with-tokio-io-streams-backpressure-concurrency-and-ergonomics/","title":"Async Rust with Tokio I/O Streams: Backpressure, Concurrency, and Ergonomics","section":"Async Rust \u0026 Tokio I/O Streams","content":" Async Rust with Tokio I/O Streams: Backpressure, Concurrency, and Ergonomics # Last updated: Oct 2025 Contents\n1 Async Rust with Tokio IO Streams: Backpressure, Concurrency, and Ergonomics Backpressure Cancellation 2. I/O loop Backpressure propagation Concurrency 3. Tokio I/O Patterns TCP split stream Split generic AsyncRead+AsyncWrite stream Bidirectional driver for I/O without split Framed I/O Bidirectional driver for framed I/O There are many excellent, straightforward guides for getting started with Async Rust and Tokio. Most focus on the core building blocks: Tokio primitives, Rust futures, and concepts such as Pin/Unpin, and often finish with a simple TCP client-server sample. In fact, the typical tutorial example can usually be reduced to something as simple as:\nloop { tokio::select! { Some(msg) = rx.recv() =\u0026gt; { stream.write_all(\u0026amp;msg).await?; stream.flush().await?; } res = stream.read(\u0026amp;mut read_buf) =\u0026gt; { let n = res?; if n == 0 { // EOF - server closed connection eprintln!(\u0026#34;Server closed the connection.\u0026#34;); break; //exit } print!(\u0026#34;{}\u0026#34;, String::from_utf8_lossy(\u0026amp;read_buf[..n])); } else =\u0026gt; break } } The code above is easy to read, straightforward, and idiomatic.\nIt uses a Tokio mpsc receiver rx whose incoming messages are written to the I/O stream (the remote peer in TCP terms). On the other branch of the select!, it reads data into read_buf until reaching the end of file (EOF). I\u0026rsquo;m sure you\u0026rsquo;ve seen many examples that follow exactly this pattern.\nBut if we think not only about the functionality of the above code, but also about the mechanics and design behind it, some crucial details emerge. With the loop-select pattern we have concurrent reads and writes. This means the Tokio runtime scheduler multiplexes them and runs sequentially one after another, and never truly in parallel on two OS threads or CPU cores (assuming the default multithreaded runtime with #[tokio::main]). All branches of the select! are awaited for readiness and then run one by one in a pseudo-random order, unless the biased; argument is set, in which case they run in the provided order. In our example, we have two branches:\nread from rx channel; read from the I/O stream. Async Rust implicitly generates a state machine to make the polling of futures easier. This allows select! to run its branches concurrently. But when we fall inside the first branch and explicitly await inside it, we ask Rust to move the task\u0026rsquo;s state machine into a state where the only possible wake signal for the task is a wake notification from the write_all() future (or flush() later). The code logic doesn\u0026rsquo;t allow us to await on write_all()/flush() and read() (from the second branch of the select!) at the same time .\nCode written in this way naturally introduces backpressure. This side effect deserves careful attention and a clear understanding.\nBackpressure # If a write() or flush() call becomes blocked for any reason, the entire loop-select is effectively blocked as well, meaning no reads will occur. This can lead to read starvation and, if the remote peer continues writing, to an inflated receive socket buffer and eventual backpressure on the remote peer side.\nSuch backpressure can actually be useful when the code should avoid reading more data if it can\u0026rsquo;t send a timely response. In client-server communication, this makes sense: your server might not want to start processing work, or buffer it in memory, if the client isn\u0026rsquo;t ready to read the response to its request.\nBut as usual, there\u0026rsquo;s no one-size-fits-all solution, it isn\u0026rsquo;t always the desired behavior. For example, if your code acts as a proxy or bridge, you\u0026rsquo;ll likely still want to continue draining the reader buffer, because, for example, the decision logic lives outside your module.\nAnother use case is when producing data doesn\u0026rsquo;t fully depend on what you read. This often applies on the client side: a client may want to receive the response to a previously sent request as quickly as possible while concurrently sending a new, unrelated one.\nMore examples are protocols where reads unblock writes, where reads are crucial and should always be handled. For instance, TCP itself is such a protocol. TCP flow control sends window size updates with ACKs to notify the remote peer to continue sending and/or increase the amount of in-flight data. HTTP/2 also has a WINDOW_UPDATE frame used for the same purpose – the consumer controls the producer\u0026rsquo;s pace.\nExperiment # Let\u0026rsquo;s run a small experiment to reproduce TCP backpressure and explore a few related GNU/Linux tools.\nWe can write a TCP server that intentionally blocks its read calls. Also in order to trigger backpressure earlier, server sets its socket receive buffer to the minimum allowed value with setsockopt and SO_RCVBUF (the OS rounds it up to a small default value):\n... let s2 = Socket::from(std_stream); s2.set_recv_buffer_size(1)?; ... If server doesn\u0026rsquo;t read data from buffer, its receive buffer fills up and pushes back on the TCP sender. This effectively simulates slow request processing, for example, when the next request hits a slow database while the previous response is still being flushed to the socket to deliver to the client.\nasync fn handle_client(mut stream: TcpStream) -\u0026gt; std::io::Result\u0026lt;()\u0026gt; { println!(\u0026#34;start serving\u0026#34;); let mut i = 0; loop { // Don\u0026#39;t read any data to emulate backpressure. // let mut buf = [0u8; 1024]; // let n = stream.read(\u0026amp;mut buf).await?; // if n == 0 { // break; // } // println!(\u0026#34;Received {n} bytes\u0026#34;); i += 1; sleep(Duration::from_millis(10)).await; stream .write_all(format!(\u0026#34;message from server: {i}\\n\u0026#34;).as_bytes()) .await?; stream.flush().await?; println!(\u0026#34;written: {i}\u0026#34;); } } The full code can be found on github.\nAnd run it:\n$ cargo run --bin 1_server_recvbuf_set In the other console window run the client:\n$ cargo run --bin 1_client_simple As you can see from the output logs, the client stops reading from the server even while the server keeps writing. Packets pile up in the client\u0026rsquo;s receive buffer, eventually filling it and forcing TCP to apply backpressure in the opposite direction.\nIn the tcpdump output you can see the server on port 8080 sending a zero-window (win 0) ACKs, notifying the client that its receive buffer is full.\n$ sudo tcpdump -i any -n -s0 port 8080 ... 21:36:00.095027 lo In IP 127.0.0.1.8080 \u0026gt; 127.0.0.1.58090: Flags [P.], seq 5668:5693, ack 102401, win 0, options [nop,nop,TS val 2038288450 ecr 2038288438], length 25: HTTP ... The ss can help us too to find the stalled send buffer:\n$ ss -tan | grep 127.0.0.1:58090 State Recv-Q Send-Q Local Address:Port Peer Address:Port ESTAB 102400 0 127.0.0.1:8080 127.0.0.1:58090 ESTAB 869937 2628271 127.0.0.1:58090 127.0.0.1:8080 In simplified terms, the TCP connection behaves like a network of queues. Congestion control and flow control cooperate to size those queues, balance throughput, and protect each endpoint from overload.\nFigure 1. – TCP buffers for simplex communication Usually write calls to a socket complete almost instantly (0.5-5 µs depending on size) because they land in memory (userspace buffers such as BufWriter or the kernel\u0026rsquo;s TCP send buffer) without waiting for remote acknowledgements.\nThis asynchronous nature of writes can hide subtle problems: background write failures, timeouts, and uncertainty of delivery often surface only under congestion. In our toy setup, small writes and generous buffers mask pending backpressure.\nNote:\nA useful per-socket/system-wide setting to improve responsiveness and backpressure behavior is TCP_NOTSENT_LOWAT. Cloudflare has a good write-up.\nCancellation # Another interesting part of the code is how to perform cancellation while backpressure is applied. We might need to stop processing for many reasons: restart, upstream/downstream abort, timeouts, or simply because the caller no longer needs the result.\nIn the code above, simply adding a cancellation branch to select! will not work if the task is blocked on write_all():\nlet cancel = CancellationToken::new(); loop { tokio::select! { Some(msg) = rx.recv() =\u0026gt; { stream.write_all(\u0026amp;msg).await?; stream.flush().await?; println!(\u0026#34;client\u0026#39;s written\u0026#34;); } res = stream.read(\u0026amp;mut read_buf) =\u0026gt; { let n = res?; if n == 0 { // EOF - server closed connection eprintln!(\u0026#34;Server closed the connection.\u0026#34;); break; //exit } print!(\u0026#34;{}\u0026#34;, String::from_utf8_lossy(\u0026amp;read_buf[..n])); } _ = cancel.cancelled() =\u0026gt; break, // \u0026lt;---------------- cancellation else =\u0026gt; break } } At that point the state machine awaits only write_all(), remaining stuck in Poll::Pending inside the first branch.\nThe first idea is to apply the cancellation token through every await calls:\nlet cancel = CancellationToken::new(); loop { tokio::select! { Some(msg) = rx.recv() =\u0026gt; { tokio::select! { res = stream.write_all(\u0026amp;msg) =\u0026gt; res, _ = cancel.cancelled() =\u0026gt; break }?; tokio::select! { res = stream.flush() =\u0026gt; res, _ = cancel.cancelled() =\u0026gt; break }?; println!(\u0026#34;client\u0026#39;s written\u0026#34;); } res = stream.read(\u0026amp;mut read_buf) =\u0026gt; { let n = res?; if n == 0 { // EOF - server closed connection eprintln!(\u0026#34;Server closed the connection.\u0026#34;); break; //exit } print!(\u0026#34;{}\u0026#34;, String::from_utf8_lossy(\u0026amp;read_buf[..n])); } _ = cancel.cancelled() =\u0026gt; break, else =\u0026gt; break } } This works, but it scales poorly: every new await point needs another nested select!, and readability drops fast.\nLater in the post I\u0026rsquo;ll show a tidier cancellation pattern, especially if you need a simple \u0026ldquo;short-circuit\u0026rdquo; solution.\nRead next chapter → "},{"id":2,"href":"/docs/fd-pipe-session-terminal/0-sre-should-know-about-gnu-linux-shell-related-internals-file-descriptors-pipes-terminals-user-sessions-process-groups-and-daemons/","title":"GNU/Linux shell related internals","section":"GNU/Linux shell related internals","content":" What every SRE should know about GNU/Linux shell related internals: file descriptors, pipes, terminals, user sessions, process groups and daemons # Last updated: Oct 2025 Contents\nFile descriptor and open file description Pipes Process groups, jobs and sessions Terminals and pseudoterminals Despite the era of containers, virtualization, and the rising number of UI of all kinds, SREs often spend a significant part of their time in GNU/Linux shells. It could be debugging, testing, developing, or preparing the new infrastructure. It may be the good old bash, the more recent and fancy zsh, or even fish or tcsh with their interesting and unique features.\nBut it is common nowadays how little people know about the internals of their shells, terminals, and relations between processes. All are taken primarily for granted without really thinking about such aspects.\nHave you ever thought about how a shell pipe works, how pressing the CTRL+C combination delivers the interrupt signal to the currently running shell processes, or how vim rewrites the content of the console when you change the size of your terminal window?\nI want to show you some indeed neat parts of pipes, file descriptors, shells, terminals, processes, jobs, and signals in this series of posts. We’ll touch on how all of them interact with each other to build a responsible, simple, and reliable environment. And all of this, of course, will be shown in the context of the Linux kernel, its internals, and various debugging tools and approaches.\nWe are going to play with file descriptors, pipes, different tools such as nohup and pv, experiment with background and foreground processes, understand how tmux gives us the ability to continue where we stopped, why and how the CTRL+C interrupts the currently running pipeline of commands and much much more. Also, we will use strace to trace syscalls, read the Linux kernel source code, and use bpftrace to get under the hood of arbitrary kernel functions.\nPrepare environment # During the series, I’ll mix python and golang for my examples. Also, we’ll need a file for our experiments. I use /var/tmp/file1.db. You can easily generate it using the following command:\n$ dd if=/dev/random of=/var/tmp/file1.db count=100 bs=1M Dive # With all that said, let\u0026rsquo;s learn, experiment, and have fun.\nRead next chapter → "},{"id":3,"href":"/docs/page-cache/0-linux-page-cache-for-sre/","title":"Linux Page Cache for SRE","section":"Linux Page Cache series","content":" SRE deep dive into Linux Page Cache # Last updated: Oct 2025 Contents\nPrepare environment for experiments Essential Page Cache theory Page Cache and basic file operations Page Cache eviction and page reclaim More about mmap() file access cgroup v2 and Page Cache How much memory my program uses or the tale of working set size Direct IO (DIO) Advanced Page Cache observability and troubleshooting tools In this series of articles, I would like to talk about Linux Page Cache. I believe that the following knowledge of the theory and tools is essential and crucial for every SRE. This understanding can help both in usual and routine everyday DevOps-like tasks and in emergency debugging and firefighting. Page Cache is often left unattended, and its better understanding leads to the following:\nmore precise capacity planning and container limit calculations; better debugging and investigation skills for memory and disk intensive applications such as database management system and file sharing storages; building safe and predictable runtimes for memory and/or IO-bound ad-hoc tasks (for instance: backups and restore scripts, rsync one-liners, etc.). I’ll display what utils you should keep in mind when you\u0026rsquo;re dealing with Page Cache related tasks and problems, how to use them properly to understand real memory usage, and how to reveal issues with them. I will try to give you some examples of using these tools that are close to real life situations. Here are some of these tools I\u0026rsquo;m talking about below: vmtouch, perf, cgtouch, strace , sar and page-type.\nAlso, as the title says, \u0026ldquo;deep dive\u0026rdquo;, the internals of these utils will be shown with an emphasis on the Page Cache stats, events, syscalls and kernel interfaces. Here are some examples of what I’m touching on in the following post:\nprocfs files: /proc/PID/smaps, /proc/pid/pagemap, /proc/kpageflags, /proc/kpagecgroup and sysfs file: /sys/kernel/mm/page_idle; system calls: mincore(), mmap(), fsync(), msync(), posix_fadvise(), madvise() and others; different open and advise flags O_SYNC, FADV_DONTNEED, POSIX_FADV_RANDOM, MADV_DONTNEED, etc. I’ll try to be as verbose as possible with simple (almost all the way) code examples in Python, Go and a tiny bit of C.\nAnd finally, any conversations about modern GNU/Linux systems can’t be fully conducted without touching the cgroup (v2 in our case) and the systemd topics. I\u0026rsquo;ll show you how to leverage them to get the most out of the systems, build reliable, well-observed, controlled services, and sleep well at night while on-call.\nReaders should be confident if they have middle GNU/Linux knowledge and basic programming skills.\nAll code examples larger than 5 lines can be found on github: https://github.com/brk0v/sre-page-cache-article.\nRead next chapter → "},{"id":4,"href":"/docs/resolver-dual-stack-application/0-sre-should-know-about-gnu-linux-resolvers-and-dual-stack-applications/","title":"What every SRE should know about GNU/Linux resolvers and Dual-Stack applications","section":"DNS resolvers and Dual-Stack applications","content":" What every SRE should know about GNU/Linux resolvers and Dual-Stack applications # Last updated: Oct 2025 Contents\nWhat is a stub resolver? History: gethostbyname() and old good friends getaddrinfo() and POSIX spec getaddrinfo() from glibc getaddrinfo() from musl libc Dual-Stack applications Async non-blocking resolvers in C Stub resolvers in languages Dual-stack software examples systemd-resolved Querying Nameservers on dual-stack hosts The Present and the future of resolvers and DNS related features Tools for troubleshooting in one place In this series of posts, I’d like to make a deep dive into the GNU/Linux local facilities used to convert a domain name or hostname into IP addresses, specifically in the context of dual-stack applications. This process of resolution is one of the oldest forms of networking abstraction, designed to replace hard-to-remember network addresses with human-readable strings. Although it may seem simple at first glance, the entire process involving stub resolvers is filled with complexities and subtle nuances. One contributing factor to this complexity is the growing number of IPv6 addresses, which, although not increasing at the pace everyone might want, is gradually changing servers and clients to support dual-stack hosts. Thus a seamless transition to IPv6 become an important feature and should occur without degrading user experience or increasing response latency.\nFigure 1. – It’s always been DNS [©] We will start with a brief history of resolvers, exploring how they evolved, the issues and problems that the getaddrinfo() aims to resolve, and what happens under the hood: how it interacts with the name service switch (NSS), caches results, and aids in building applications suited for a dual-stack world with both IPv4 and IPv6 address families. This abstraction and address-agnostic approach are essential to modern software development, and a sloppy implementation can lead to subtle bugs that are hard to debug in production. That’s why we will cover the dual stack applications more thoroughly from both client and server perspectives, trying to understand the order of using available destination addresses from a list of IPv4 and IPv6 addresses, and exploring algorithms to improve response latency in cases of network routing instability or misconfiguration.\nWe will also examine the most feature-rich alternative C language resolver, c-ares, discussing its potential advantages and why you might consider using it. However, our discussion will not be limited to C stub resolvers; we will also cover mainstream languages such as Python, Go (Golang), Rust, Java, and NodeJS, focusing on their internals, decisions and trade-offs.\nAnother important topic is how to configure and manage /etc/resolv.conf on modern GNU/Linux systems. At first glance, managing /etc/resolv.conf might seem straightforward – simply add a nameserver and a search domain. But when a system has multiple physical interfaces (e.g., LAN and WiFi) and several virtual ones such as VPN tunnels, all configured with DHCP clients, the situation becomes more complex. Each DHCP server might provide its own nameservers and a search domain, necessitating some logic to coordinate and reconcile these changes. Modern GNU/Linux distributions usually employ systemd-resolved to address this issue, and we will explore its capabilities.\nAs usual, we will touch on related topics to dual-stack programs, such as IPv4-mapped addresses, different ways to bind sockets for dual-stack servers, and how systemd can help manage listener sockets.\nAfter we have gained a complete understanding of the resolving process, tools, and solutions, we will examine several popular load balancers: Nginx, Envoy (Envoyproxy), and HAProxy. These are excellent examples because they are designed to be dual-stack for both clients (downstreams) and backends (upstreams).\nFinally, we’ll review some new and advanced topics not always directly related to a local stub resolver and dual stack applications but certainly important for domain name resolution and promising in terms of refining the resolving process in various directions: DNS push notifications, the new DNS resource record HTTPS, DNS over TLS (DoT), DNS over HTTPS (DoH), oblivious DNS (ODNS), and DNSSEC.\nBut before we kick off, here is some preparational information.\nSetup playground # All examples in this series are runnable and represent real, working code. To follow along and experiment with the code effectively – a great way to learn – you’ll need a setup similar to mine. I use the latest LTS Ubuntu 24.04 cloud image on my macOS, managed under the lima project, which allows me to run Linux containers.\nFor testing domain name resolution, I’m using \u0026ldquo;microsoft.com\u0026rdquo; for all tests because it provides multiple A and AAAA records. Additionally, its DNS server shuffles records with every call, which can help easily determine if the answer is served from the cache or not.\nRead next chapter → "},{"id":5,"href":"/docs/fd-pipe-session-terminal/1-file-descriptor-and-open-file-description/","title":"File descriptor and open file description","section":"GNU/Linux shell related internals","content":" File descriptor and open file description # Last updated: Oct 2025 Contents\nstdin, stdout and stderr Procfs and file descriptors Sharing file descriptors between parent and child after fork() Duplication of file descriptors execve() and file descriptors Check if 2 file descriptors share the same open file description with kcmp() More ways to transfer file descriptors between processes: pidfd_getfd() and Unix datagrams. Shell redirections and file descriptors First of all, I want to touch on the two fundamental concepts of working with files:\nfile descriptor; open file description. These two abstractions are crucial for understanding the internals of a process creation, communication, and data transition.\nThe first concept is a file descriptor or fd. It’s a positive integer number used by file system calls instead of a file path in order to make a variety of operations. Every process has its own file descriptor table (see Image 1 below). The main idea of a file descriptor is to decouple a file path (or, more correctly, an inode with minor and major device numbers) from a file object inside a process and the Linux kernel. This allows software developers to open the same file an arbitrary number of times for different purposes, with various flags (for instance: O_DIRECT, O_SYNC, O_APPEND, etc.), and at different offsets.\nFor example, a program wants to read from and write to one file in two separate places. In this case, it needs to open the file twice. Thus, two new file descriptors will refer to 2 different entries in the system-wide open file description table.\nIn its turn, the open file description table is a system-wide kernel abstraction. It stores the file open status flags (man 2 open) and the file positions (we can use man 2 lseek to change this position).\nFrankly speaking, there is no such thing inside the Linux kernel where we can find the open file description table. To be more accurate, every created process in the kernel has a per-thread struct task_struct. This struct has a pointer to another structure called the files_struct, and that contains an array of pointers to a file struct. This final struct is actually what holds all file flags, a current position, and a lot of other information about the open file: such as its type, inode, device, etc. All such entries among all running threads are what we call the open file descriptor table.\nSo, now let’s see how we can create entities in these two tables. In order to create a new entry in the open file description table we need to open a file with one of the following syscalls: open, openat, create, open2 (man 2 open). These functions also add a corresponding entry in the file descriptor table of the calling process, build a reference between the open file description table entry and the file descriptor table, and return the lowest positive number not currently opened by the calling process. The latest statement is very important to remember and understand because it means that a fd number can be reused during the process life if it opens and closes files in an arbitrary order.\nLinux kernel also provides an API to create copies of a file descriptor within a process. We will discuss why this technique can be helpful in a few minutes. For now, let’s just list them here: dup, dup2, dup3 (man 2 dup) and fcntl (man 2 fcntl) with F_DUPFD flag. All these syscalls create a new reference in the fd table of the process to the existing entry in the system-wide open file description table.\nLet’s take a closer look at an example in the image below with a snapshot of a system state. The image shows us possible relations among all the above components.\nFigure 1. – Relations between process file descriptors, system-wide open file description table and files ❶ – The first three file descriptors (stdin, stdout and stderr) are special file descriptors. We will work with them later in this post. This example shows that all three point to a pseudoterminal (/dev/pts/0). These files don’t have positions due to their character device type. Thus process_1 and process_2 must be running under the terminal sessions. Please, note that the stdout of the process_2 (fd 1) points to the file on a disk /tmp/out.log. This is an example of shell redirection; we will discuss it later.\n❷ – Some file descriptors can have per-process flags. Nowadays, there is only one such flag: close-on-exec (O_CLOEXEC). We will discuss it later in this section and answer why it’s so unique. But for now, you should understand that some file descriptors may have it for the same system-wide open file description table entries. For instance: process_1 and its fd 9 and process_2 and its fd 3.\n❸ – Even though the file descriptor algorithm constantly reuses the file descriptors and allocates them sequentially from the lowest available, it doesn’t mean that there can be no gaps. For example, the fd 9 of the process_1 goes after fd 3. Some files, which used fd 4, 5, 6 and 7, could already be closed. Another way of achieving such a picture can be an explicit duplication of a file descriptor with dup2, dup3 or fcntl with F_DUPFD. Using these syscalls, we can specify the wanted file descriptor number. We will see later how it works in the chapter about the duplication of fds.\n❹ – A process can have more than one file descriptor that points to the same entry in the open file descriptions. System calls dup, dup2, dup3 and fcntl with F_DUPFD help with that. The fd 0 and fd 2 of the process_2 refer to the same pseudo terminal entry.\n❺ – Sometimes, one of the standard file descriptors might be pointed to a regular file (or pipe) and not a terminal. In this example, the stdout of the process_2 refers to a file on disk /tmp/out.txt.\n❻ – It’s possible to point file descriptors from various processes to the same entry in the system-wide open file description table. This is usually achieved by a fork call and inheriting file descriptors from the parent to its child. But there are other ways, which we’ll see later in this chapter. These descriptors could also have different int fd numbers inside processes and different process flags (O_CLOEXEC). For instance, fd 9 of process_1 and fd 3 of process_2.\n❼ – I put the file path here for simplicity. Instead, Linux kernel uses inode numbers, minor and major numbers of a device.\n❽ – Often, for a shell, the 0,1 and 2 file descriptors are pointed to a pseudo-terminal.\n❾ – Multiple open file descriptor entries can be linked with the same file on disk. The kernel allows us to open a file with different flags and at various offset positions.\nstdin, stdout and stderr # The first three file descriptors of processes are treated differently by shells and other programs. These fds also have well-known aliases:\n0 – stdin 1 – stdout 2 – stderr For a process started and running within a terminal session, these fds can be pointed to a pseudoterminal, a terminal, a file, a pipe, etc. For classical-UNIX-style daemons, they usually refer to a /dev/null device.\nLater in this series, I\u0026rsquo;ll show how this works in shells and why we must be careful with these three fds when working with long-running background processes.\nProcfs and file descriptors # The kernel exposes all open file descriptors of a process with the virtual procfs file system. So in order to get information about the open files for the current shell process, we can use a shell variable $$ with its PID. For instance:\n$ ls -l /proc/$$/fd/ lrwx------ 1 vagrant vagrant 64 Jul 9 21:15 0 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 9 21:15 1 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 9 21:15 2 -\u0026gt; /dev/pts/0 We can see only pseudoterminal /dev/pts/0 here. We will talk more about them a bit later.\nAnother useful directory in the procfs is the fdinfo folder under the process directory. It contains per file descriptor info. For example, for the stdin of the current shell process:\n$ cat /proc/$$/fdinfo/0 pos:\t0 flags: 02 mnt_id: 28 Keep in mind that the flags section here contains only the status flags (man 2 open). Let’s use it to write a tool to decode this flag mask to human-readable flags:\nimport os import sys pid = sys.argv[1] fd = sys.argv[2] with open(f\u0026#34;/proc/{pid}/fdinfo/{fd}\u0026#34;, \u0026#34;r\u0026#34;) as f: flags = f.readlines()[1].split(\u0026#34;\\t\u0026#34;)[1].strip() print(f\u0026#34;Flags mask: {flags}\u0026#34;) flags = int(flags, 8) # check status flags if flags \u0026amp; os.O_RDONLY: print(\u0026#34;os.O_RDONLY is set\u0026#34;) if flags \u0026amp; os.O_WRONLY: print(\u0026#34;os.O_WRONLY is set\u0026#34;) if flags \u0026amp; os.O_RDWR: print(\u0026#34;os.O_RDWR is set\u0026#34;) if flags \u0026amp; os.O_APPEND: print(\u0026#34;os.O_APPEND is set\u0026#34;) if flags \u0026amp; os.O_DSYNC: print(\u0026#34;os.O_DSYNC is set\u0026#34;) if flags \u0026amp; os.O_RSYNC: print(\u0026#34;os.O_RSYNC is set\u0026#34;) if flags \u0026amp; os.O_SYNC: print(\u0026#34;os.O_SYNC is set\u0026#34;) if flags \u0026amp; os.O_NDELAY: print(\u0026#34;os.O_NDELAY is set\u0026#34;) if flags \u0026amp; os.O_NONBLOCK: print(\u0026#34;os.O_NONBLOCK is set\u0026#34;) if flags \u0026amp; os.O_ASYNC: print(\u0026#34;os.O_ASYNC is set\u0026#34;) if flags \u0026amp; os.O_DIRECT: print(\u0026#34;os.O_DIRECT is set\u0026#34;) if flags \u0026amp; os.O_NOATIME: print(\u0026#34;os.O_NOATIME is set\u0026#34;) if flags \u0026amp; os.O_PATH: print(\u0026#34;os.O_PATH is set\u0026#34;) # check close on exec if flags \u0026amp; os.O_CLOEXEC: print(\u0026#34;os.O_CLOEXEC is set\u0026#34;) Out test program, which opens a file with some status flags:\nimport os import sys import time file_path = sys.argv[1] print(os.getpid()) fd = os.open(file_path, os.O_APPEND | os.O_RSYNC | os.O_NOATIME ) with os.fdopen(fd, \u0026#34;r+\u0026#34;) as f: print(f.fileno()) time.sleep(9999) Let’s run it:\n$ python3 ./open.py /tmp/123.txt 925 3 And run our tool:\n$ python3 ./flags.py 925 3 Flags mask: 07112000 os.O_APPEND is set os.O_DSYNC is set os.O_RSYNC is set os.O_SYNC is set os.O_NOATIME is set os.O_CLOEXEC is set Some flags in the kernel are aliases to other flags. That’s why we see more flags here.\nAnother example is if we run our tool with a socket fd (I used nginx process):\n$ sudo python3 ./flags.py 943 6 Flags mask: 02004002 os.O_RDWR is set os.O_NDELAY is set os.O_NONBLOCK is set os.O_CLOEXEC is set We can see that the socket is in nonblocking mode: O_NONBLOCK is set.\nSharing file descriptors between parent and child after fork() # Another important concept of file descriptors is how they behave with fork() (man 2 fork) and clone() (man 2 clone) system calls.\nAfter a fork() or a clone() (without CLONE_FILES set) call, a child and a parent have an equal set of file descriptors, which refer to the same entries in the system-wide open file description table. It means they share identical file positions, status flags and process fd flags (O_CLOEXEC)\nLet’s start with an example where 2 processes are not relatives. Both open the same file and get the same integer number for their fd. But because they both call open() independently, these two references to the open file description table will differ. After the file opening, the first example process makes a lseek() (man 2 lseek) at one position, and another program makes a lseek() call for the same file but at a different place. These actions don’t affect each other.\nCode:\nimport time import os import sys print(f\u0026#34;pid: {os.getpid()}\u0026#34;) with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;r\u0026#34;) as f: print(f.fileno()) f.seek(int(sys.argv[1])) time.sleep(99999) Run them in 2 different terminals:\n$ python3 ./file1.py 100 # \u0026lt;----------- lseek() to 100 bytes pid: 826 3 $ python3 ./file1.py 200 # \u0026lt;----------- lseek() to 200 bytes pid: 827 3 Now check procfs:\n$ ls -l /proc/826/fd lrwx------ 1 vagrant vagrant 64 Jul 9 21:18 0 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 9 21:18 1 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 9 21:18 2 -\u0026gt; /dev/pts/0 lr-x------ 1 vagrant vagrant 64 Jul 9 21:18 3 -\u0026gt; /var/tmp/file1.db \u0026lt;--------- $ ls -l /proc/827/fd lrwx------ 1 vagrant vagrant 64 Jul 9 21:18 0 -\u0026gt; /dev/pts/1 lrwx------ 1 vagrant vagrant 64 Jul 9 21:18 1 -\u0026gt; /dev/pts/1 lrwx------ 1 vagrant vagrant 64 Jul 9 21:18 2 -\u0026gt; /dev/pts/1 lr-x------ 1 vagrant vagrant 64 Jul 9 21:18 3 -\u0026gt; /var/tmp/file1.db \u0026lt;--------- We have the same file path and the same file descriptor number. Now verify that the positions are different because we have unrelated open file descriptions:\n$ cat /proc/826/fdinfo/3 pos:\t100 \u0026lt;------------------------ flags: 02100000 mnt_id: 26 $ cat /proc/827/fdinfo/3 pos:\t200 \u0026lt;------------------------ flags: 02100000 mnt_id: 26 Let’s now see how the file positions will behave after a fork() call between a parent process and its child. We open a file in a parent process, fork(), make lseek() in the child, and check whether the positions are the same or not.\nimport time import os import sys with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;r\u0026#34;) as f: print(f.fileno()) print(f\u0026#34;parent pid: {os.getpid()}\u0026#34;) pid = os.fork() if not pid: # child print(f\u0026#34;child pid: {os.getpid()}\u0026#34;) f.seek(int(sys.argv[1])) time.sleep(99999) os.waitpid(pid, 0) Run it:\n$ python3 ./file2.py 100 # \u0026lt;----------- lseek() to 100 bytes 3 parent pid: 839 child pid: 840 Our procfs picture:\n$ ls -l /proc/839/fd/ lrwx------ 1 vagrant vagrant 64 Jul 9 21:23 0 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 9 21:23 1 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 9 21:23 2 -\u0026gt; /dev/pts/0 lr-x------ 1 vagrant vagrant 64 Jul 9 21:23 3 -\u0026gt; /var/tmp/file1.db \u0026lt;--------- $ ls -l /proc/840/fd/ lrwx------ 1 vagrant vagrant 64 Jul 9 21:23 0 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 9 21:23 1 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 9 21:23 2 -\u0026gt; /dev/pts/0 lr-x------ 1 vagrant vagrant 64 Jul 9 21:23 3 -\u0026gt; /var/tmp/file1.db \u0026lt;--------- $ cat /proc/839/fdinfo/3 pos:\t100 \u0026lt;--------- 100 bytes flags: 02100000 mnt_id: 26 $ cat /proc/840/fdinfo/3 pos:\t100 \u0026lt;--------- 100 bytes flags: 02100000 mnt_id: 26 The primary purpose of such sharing is to protect files from being overwritten by children and its parent process. If all relatives start writing to a file simultaneously, the Linux kernel will sort this out and won’t lose any data because it’ll hold the lock and update the offset after each write. It’s worth mentioning that the data can appear in the file in a mixed way due to the CPU scheduler, arbitrary sizes of write buffers, and the amount of data to write.\nIf it’s not what you want, you should close all file descriptors after a successful fork(), including the three standard ones. This is basically how the classical daemons usually start. We will talk about them later in this series of posts.\nDuplication of file descriptors # We already know that we can open a new file in order to create a new file descriptor within the process. But it’s not always needed. Usually it’s handy to copy the existing fd to another one.\nLet’s start with the existing kernel API. We have a bunch of syscalls to duplicate fd:\ndup() – creates a new fd using the lowest unused int number. It usually follows the close() syscall for the one of standard fd (stdin, stdout, stderr) in order to replace it. dup2() – does the same as above but has a second argument. Here we can specify the target fd. If the target fd already exists, the dup2() closes it first. All dup2() operations are atomic. dup3() – does the same as the dup2() but has a third parameter, where the O_CLOEXEC flag can be set. fcntl() with F_DUPFD flag behaves as dup2() with one exception: if the target fd exists, it uses the next one instead of closing it. When dup(), dup2(), or fcntl() are used to create a duplicate of a file descriptor, the close-on-exec (O_CLOEXEC) flag is always reset for the duplicate fd.\nWe can in theory open the file twice with the O_APPEND flag and don’t use the duplication syscalls at all. In the following example O_APPEND flag preserves the strace tool from overwriting data in the results.log file by its concurrent writes from the stdout and stderr:\n$ strace 1\u0026gt;\u0026gt;results.log 2\u0026gt;\u0026gt;results.log where 1\u0026gt;\u0026gt; and 2\u0026gt;\u0026gt; are append shell redirections for stdout and stderr.\nBut if we use a shell pipe, the following example will only work with fd duplication logic. Pipes don’t have O_APPEND open flag, and they are much convenient for the redirection task (I’m covering the power of pipes later in the chapter 2 where you can find more justifications for the below technique):\n$ strace 2\u0026gt;\u0026amp;1 | less Let’s write an example that shows all the power of fd duplication:\nimport os import time print(f\u0026#34;{os.getpid()}\u0026#34;) fd1 = os.open(\u0026#34;/var/tmp/file1.db\u0026#34;, os.O_RDONLY, 777) fd2 = os.dup(fd1) fd3 = os.dup2(fd1, 999) os.lseek(fd3, 100, 0) time.sleep(9999) We opened one file, duplicate it several times, change the file position and it’s changed for all of the fs:\n$ ls -la /proc/2129/fd lrwx------ 1 vagrant vagrant 64 Aug 6 19:52 0 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Aug 6 19:52 1 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Aug 6 19:52 2 -\u0026gt; /dev/pts/0 lr-x------ 1 vagrant vagrant 64 Aug 6 19:52 3 -\u0026gt; /var/tmp/file1.db lr-x------ 1 vagrant vagrant 64 Aug 6 19:52 4 -\u0026gt; /var/tmp/file1.db lr-x------ 1 vagrant vagrant 64 Aug 6 19:52 999 -\u0026gt; /var/tmp/file1.db $ cat /proc/2129/fdinfo/999 pos:\t100 \u0026lt;------------ position flags: 0100000 mnt_id: 26 $ cat /proc/2129/fdinfo/3 pos:\t100 \u0026lt;------------ position flags: 02100000 mnt_id: 26 $ cat /proc/2129/fdinfo/4 pos:\t100 \u0026lt;------------ position flags: 02100000 mnt_id: 26 execve() and file descriptors # Now let’s talk what may happen with file descriptors during the execve() system call (man 2 execve).\nJust to start, execve() is the only way the Linux kernel can start a new program. This syscall executes a binary file if the first argument is an ELF compiled file and has an executable bit set, or starts an interpreter with the content of the file if the argument has a hashbang (for example: #!/usr/bin/python) on the first line of the file and has an exec bit set.\nAfter an execve() call a file offsets and flags are copied and shared if the close-on-exec (O_CLOEXEC) flag is not set.\nLet’s prove it with an example. We need 2 files: sleep.py and exec.py. The second one will execute the first one.\n#!/usr/bin/python3 import time print(\u0026#34;sleep\u0026#34;) time.sleep(99999) Don’t forget to set an exec bit on it:\n$ chmod +x ./sleep.py The exec.py opens a file, duplicates it with dup2() syscall, clearing the close-on-exec (O_CLOEXEC) flag.\nimport os with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;r\u0026#34;) as f: print(f.fileno()) print(f\u0026#34;parent {os.getpid()}\u0026#34;) os.dup2(f.fileno(), 123) pid = os.fork() if not pid: # child print(f\u0026#34;child {os.getpid()}\u0026#34;) os.execve(\u0026#34;./sleep.py\u0026#34;, [\u0026#34;./sleep.py\u0026#34;], os.environ) f.seek(234) os.waitpid(-1, 0) If we run it in the console, the output should be something like the following:\n$ python3 ./exec.py 3 parent 6851 child 6852 sleep If we check the procfs. First, we will not be able to see the fd 3 for the child. This happens because python, by default, opens all files with the O_CLOEXEC flag (but dup2 resets this flag for 123 fd) We can get this info by running out script under strace:\n$ strace -s0 -f python3 ./exec.py And in the output we can find the following:\nopenat(AT_FDCWD, \u0026#34;/var/tmp/file1.db\u0026#34;, O_RDONLY|O_CLOEXEC) = 3 That’s why we used dup2(). It resets the O_CLOEXEC flag and allows us to check whether the fd sharing is established.\nThe parent process:\n$ ls -l /proc/6851/fd/ lrwx------ 1 vagrant vagrant 64 Jul 11 20:07 0 -\u0026gt; /dev/pts/1 lrwx------ 1 vagrant vagrant 64 Jul 11 20:07 1 -\u0026gt; /dev/pts/1 lr-x------ 1 vagrant vagrant 64 Jul 11 20:07 123 -\u0026gt; /var/tmp/file1.db \u0026lt;--- lrwx------ 1 vagrant vagrant 64 Jul 11 20:07 2 -\u0026gt; /dev/pts/1 lr-x------ 1 vagrant vagrant 64 Jul 11 20:07 3 -\u0026gt; /var/tmp/file1.db \u0026lt;--- The child has only fd 123:\n$ ls -l /proc/6852/fd/ lrwx------ 1 vagrant vagrant 64 Jul 11 20:07 0 -\u0026gt; /dev/pts/1 lrwx------ 1 vagrant vagrant 64 Jul 11 20:07 1 -\u0026gt; /dev/pts/1 lr-x------ 1 vagrant vagrant 64 Jul 11 20:07 123 -\u0026gt; /var/tmp/file1.db \u0026lt;---- lrwx------ 1 vagrant vagrant 64 Jul 11 20:07 2 -\u0026gt; /dev/pts/1 Check the positions in the parent’s fds:\n$ cat /proc/6851/fdinfo/3 pos:\t234 \u0026lt;------------------- flags: 02100000 mnt_id: 26 $ cat /proc/6851/fdinfo/123 pos:\t234 \u0026lt;------------------- flags: 0100000 mnt_id: 26 And the child:\n$ cat /proc/6852/fdinfo/123 pos:\t234 \u0026lt;------------------- flags: 0100000 mnt_id: 26 The reasonable question you may ask now is how we can protect ourselves from leaking file descriptors from a parent to children, keeping in mind that we usually execute a binary that we didn’t write. For instance, a shell starts programs like ls, ping, strace, etc.\nBack in the past (before Linux 5.9), people iterated over all possible file descriptors and tried to close them. In order to find out the upper boundary, the ulimit limit for open files was used (RLIMIT_NOFILE).\nSome people open the /proc/self/fd/ in their programs after fork() and close all fd from it.\nBut there is a more elegant way of doing this in the modern Linux kernels. It’s a close_range() syscall (man 2 close_range). It allows us to avoid heavy user space iterations and use a kernel help instead.\nThe fixed version:\n... pid = os.fork() if not pid: # child print(f\u0026#34;child {os.getpid()}\u0026#34;) max_fd = os.sysconf(\u0026#34;SC_OPEN_MAX\u0026#34;) # \u0026lt;---- added os.closerange(3, max_fd) # \u0026lt;---/ os.execve(\u0026#34;./sleep.py\u0026#34;, [\u0026#34;./sleep.py\u0026#34;], os.environ) ... O_CLOEXEC # And finally 2 sentences about the O_CLOEXEC flag, and why we need it in the first place if we can close all unneeded file descriptors? The main issue is libraries. You should always open all files with it because it’s hard to track opened files from the main program.\nAnother crucial case is a situation when the exec() fails (due to permissiom issues, wrong path, etc), and we still need some previously opened files (for instance, to write logs). Usually, reopening them after such an error is quite hard.\nAs I showed earlier, for some modern programming language it\u0026rsquo;s a default behavior for their open file functions.\nCheck if 2 file descriptors share the same open file description with kcmp() # Let’s continue our journey with more unusual and elegant system calls.\nYou can use the kcmp() syscall (man 2 kcmp) to test whether 2 fds refer to the same open file description.\nNOTE\nWe have this syscall instead of a full view of the open file description table due to security reasons. The kernel developers don’t feel good about exporting all this information to the user space https://lwn.net/Articles/845448/.\nLet’s write a tool that we can use to identifies identical file descriptors for two processes. This system call is not widely used, so many programming languages don’t have a wrapper in their standard libraries. But it’s not a problem for us. First of all, we need to find a number of this syscall. For example, we can find it in the Linux kernel sources syscall_64.tbl:\n... 311\t64\tprocess_vm_writev\tsys_process_vm_writev 312\tcommon\tkcmp\tsys_kcmp 313\tcommon\tfinit_module\tsys_finit_module ... The full code of our tool (if something is not clear, please read the man 2 kcmp):\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;syscall\u0026#34; ) const ( SYS_KCMP = 312 KCMP_FILE = 0 ) func kcmp_files(pid1, pid2, fd1, fd2 int) (int, error) { r1, _, err := syscall.Syscall6(SYS_KCMP, uintptr(pid1), uintptr(pid2), KCMP_FILE, uintptr(fd1), uintptr(fd2), 0) return int(r1), err } func main() { var ( pid1, pid2, fd1, fd2 int err error ) pid1, err = strconv.Atoi(os.Args[1]) pid2, err = strconv.Atoi(os.Args[2]) fd1, err = strconv.Atoi(os.Args[3]) fd2, err = strconv.Atoi(os.Args[4]) if err != nil { panic(err) } r1, err := kcmp_files(pid1, pid2, fd1, fd2) fmt.Println(r1, err) } For the targets, we will use the exec.py program from the previous chapter:\n$ go run ./kcmp.go 1957 1958 123 123 0 errno 0 $ go run ./kcmp.go 1957 1958 3 123 0 errno 0 $ go run ./kcmp.go 1957 1958 3 2 1 errno 0 $ go run ./kcmp.go 1957 1958 1 1 0 errno 0 As we can see, the parent and the child shared the fd 123, and the fd 3 in the parent is the copy of the 123 in the child. Also both stdout refer to the same shell pseudoterminal.\nMore ways to transfer file descriptors between processes: pidfd_getfd() and Unix datagrams. # So far, we’ve seen file descriptors sharing only from the parent to the child with the fork() call.\nOn some occasions, we want to send an fd to a target process or processes. For example, for a zero downtime program upgrades, where we want to preserve the file descriptor of a listening socket and transfer it to the new process with a new binary.\nWe have two options to do that in modern Linux kernels.\nThe first one is pretty standard and old. It works over a Unix socket. With a special UDP message, one process can pass an fd to another process. This, of course, works only locally (that’s why it’s a UNIX domain socket). The code for such transferring is massive and if you’re wondering how to write such a tool, please check out this detailed blog post.\nThe second option is quite new and allows a process to steal an fd from another process. I’m talking about the pidfd_getfd() system call (man 2 pidfd_getfd).\nIn order to leverage it, we need to open a process with another syscall: pidfd_open() (man 2 pidfd_open). Also, we would need a special set of ptrace permission: PTRACE_MODE_ATTACH_REALCREDS.\nWe can allow it system-wide in your test box, but please don’t do it in production. For production environments, please review the man 2 ptrace.\necho 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope Let’s run our old python example which opens a file with fd 3:\n$ python3 ./file2.py 123 parent pid: 3155 3 child pid: 3156 And our stealing fd tool:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;syscall\u0026#34; \u0026#34;time\u0026#34; ) const ( sys_pidfd_open = 434 // from kernel table sys_pidfd_getfd = 438 ) func pidfd_open(pid int) (int, error) { r1, _, err := syscall.Syscall(sys_pidfd_open, uintptr(pid), 0, 0) if err != 0 { return -1, err } return int(r1), nil } func pidfd_getfd(pidfd, targetfd int) (int, error) { r1, _, err := syscall.Syscall(sys_pidfd_getfd, uintptr(pidfd), uintptr(targetfd), 0) if err != 0 { return -1, err } return int(r1), nil } func main() { var ( pid, fd int err error ) pid, err = strconv.Atoi(os.Args[1]) fd, err = strconv.Atoi(os.Args[2]) if err != nil { panic(err) } fmt.Println(\u0026#34;pid:\u0026#34;, os.Getpid()) pidfd, err := pidfd_open(pid) if err != nil { panic(err) } newFd, err := pidfd_getfd(pidfd, fd) if err != nil { panic(err) } fmt.Println(newFd) time.Sleep(time.Hour) } If we run it:\n$ go run ./getfd.go 3155 3 pid: 4009 4 And check procfs:\n$ ls -la /proc/4009/fd/ lrwx------ 1 vagrant vagrant 64 Jul 10 13:24 0 -\u0026gt; /dev/pts/2 lrwx------ 1 vagrant vagrant 64 Jul 10 13:24 1 -\u0026gt; /dev/pts/2 lrwx------ 1 vagrant vagrant 64 Jul 10 13:24 2 -\u0026gt; /dev/pts/2 lrwx------ 1 vagrant vagrant 64 Jul 10 13:24 3 -\u0026gt; \u0026#39;anon_inode:[pidfd]\u0026#39; lr-x------ 1 vagrant vagrant 64 Jul 10 13:24 4 -\u0026gt; /var/tmp/file1.db \u0026lt;-------------- lrwx------ 1 vagrant vagrant 64 Jul 10 13:24 5 -\u0026gt; \u0026#39;anon_inode:[eventpoll]\u0026#39; lr-x------ 1 vagrant vagrant 64 Jul 10 13:24 6 -\u0026gt; \u0026#39;pipe:[43607]\u0026#39; l-wx------ 1 vagrant vagrant 64 Jul 10 13:24 7 -\u0026gt; \u0026#39;pipe:[43607]\u0026#39; File is with the same position:\n$ cat /proc/4009/fdinfo/4 pos:\t123 \u0026lt;-------------- flags: 02100000 mnt_id: 26 By the way, if we check the file descriptor of the pidfd object, we can observe some additional info about the opened pid:\n$ cat /proc/4009/fdinfo/3 pos:\t0 flags: 02000002 mnt_id: 15 Pid:\t3155 \u0026lt;------------------- NSpid: 3155 Shell redirections and file descriptors # Now it’s time to talk about file descriptors and shells. We start with some basics, but later in this chapter you’ll find several really nit examples which could significantly improve your shell experience and performance.\nFor all examples, I’ll use GNU Bash 5.1. But I’m sure, the same concerts and techniques are available in your favorite shell.\nLet’s start with simple and well-known redirections.\nInstead of stdin read, we can use a file:\n$ cat \u0026lt; /tmp/foo Some text The same we can do for the stdout:\n$ echo \u0026#34;123\u0026#34; \u0026gt; /tmp/foo # redirected stdout $ cat /tmp/foo 123 \u0026gt;\u0026gt; appends to a file instead of overwriting it:\n$ echo \u0026#34;123\u0026#34; \u0026gt;\u0026gt; /tmp/foo # append to a file $ cat /tmp/foo 123 123 In order to write stderr to file, we need to specify the file descriptor number:\n$ cat \u0026#34;123\u0026#34; 2\u0026gt; /tmp/foo # write stderr to a file $ cat /tmp/foo cat: 123: No such file or directory We can use the same file for both stdout and stderr:\ncat \u0026#34;123\u0026#34; \u0026gt; /tmp/foo 2\u0026gt;\u0026amp;1 All of the above internally opens a target file with the open() syscall and uses dup2() calls to overwrite the standard file descriptors with the fd of the file. For the latest one, the shell runs dup2() twice for the stdout() and stderr()\nThe general syntax for the redirection:\n\u0026gt; fd \u0026gt; file_name \u0026gt;\u0026amp; fd \u0026gt;\u0026amp; fd With bash we aren\u0026rsquo;t restricted by the standard fds and can open new ones. For instance to open an fd 10:\n$ exec 10\u0026lt;\u0026gt; /tmp/foo Check the procfs:\n$ ls -la /proc/$$/fd lrwx------ 1 vagrant vagrant 64 Jul 9 21:17 0 -\u0026gt; /dev/pts/2 lrwx------ 1 vagrant vagrant 64 Jul 9 21:17 1 -\u0026gt; /dev/pts/2 lrwx------ 1 vagrant vagrant 64 Jul 10 14:56 10 -\u0026gt; /tmp/foo \u0026lt;--------- lrwx------ 1 vagrant vagrant 64 Jul 9 21:17 2 -\u0026gt; /dev/pts/2 lrwx------ 1 vagrant vagrant 64 Jul 10 14:56 255 -\u0026gt; /dev/pts/2 If we run strace we can see how it works:\n... openat(T_FDCWD, \u0026#34;/tmp/foo\u0026#34;, O_RDWR|O_CREAT, 0666) = 3 # open dup2(3, 10) = 10 # duplicate close(3) = 0 # close unneded initial fd ... Now we can write there:\necho \u0026#34;123\u0026#34; \u0026gt;\u0026amp;10 And read from it:\n$ cat \u0026lt;\u0026amp;10 123 And when we finish, we can close it:\n$ exec 10\u0026lt;\u0026amp;- Fun fact: if you close the stdin, you’ll lose your ssh connection:\n$ exec 0\u0026lt;\u0026amp;- This happens because your bash is a session leader and a controlling terminal process. When the controlling terminal closes its terminal, the kernel sends a SIGHUP signal to it, and the shell exits. We will talk about sessions, leaders and terminals later in next series of posts.\nWe also can use \u0026ldquo;-\u0026rdquo; (dash, minus) char instead of a file name for some tools. It means to read a file content from the stdin. For example, it may be really useful with diff:\n$ echo \u0026#34;123\u0026#34; | diff -u /tmp/file1.txt - --- /tmp/file1.txt 2022-07-10 21:42:02.256998049 +0000 +++ - 2022-07-10 21:42:15.733486844 +0000 @@ -1 +1 @@ -124 +123 Another advanced feature of the bash is a process substitution, which involves the duplication of file descriptors. Long story short, you can create tmp files with on demand and use them in other tools awaiting file parameters.\nProcess substitution uses /dev/fd/\u0026lt;n\u0026gt; files to send the results of the process(es) within parentheses to another process.\nI like the following two examples. This approach helps improve my shell experience and saves me from creating temporary files. The first one is a diff example:\n$ diff -u \u0026lt;(cat /tmp/file.1 | sort | grep \u0026#34;string\u0026#34;) \u0026lt;(echo \u0026#34;string2\u0026#34;) --- /dev/fd/63 2022-07-10 21:53:39.960846984 +0000 +++ /dev/fd/62 2022-07-10 21:53:39.960846984 +0000 @@ -1 +1 @@ -string1 +string2 And the following one helps with strace and grep:\n$ strace -s0 -e openat -o \u0026gt;(grep file1.db) python3 ./dup.py 2243 openat(AT_FDCWD, \u0026#34;/var/tmp/file1.db\u0026#34;, O_RDONLY|O_CLOEXEC) = 3 Read next chapter → "},{"id":6,"href":"/docs/async-rust-tokio-io/2-io-loop/","title":"I/O loop","section":"Async Rust \u0026 Tokio I/O Streams","content":" I/O loop # Last updated: Oct 2025 Contents\nBackpressure propagation Concurrency Before changing the code, let\u0026rsquo;s outline our goals for I/O loops:\nPreserve backpressure propagation: blocked writes should slow down the local producer over the rx channel. Retain concurrency: other work (e.g., cancellation, timeouts) should make progress while reads/writes are pending. Reads and writes should not block each other. Backpressure propagation # To address the first issue we could flip the perspective: instead of eagerly pulling from the rx channel, wait until the writer proves it has capacity. Some async primitives support this. For example, tokio::sync::mpsc::Sender offers reserve(), which awaits for a slot to become available to write. When the permission is granted, we can dequeue the message from our rx channel. But one important note here is that mpsc is slot-based, and not byte-sized, which is not what we usually need when working with I/O streams.\nTokio\u0026rsquo;s TCP stream provides something closer to the reserve() – the writable() method, which uses epoll and EPOLLOUT internally to notify when the socket can accept writes.\nYou can reasonably ask: how much capacity does \u0026ldquo;writable\u0026rdquo; guarantee? On Linux, there is SO_SNDLOWAT, and it is effectively fixed at one byte for TCP and cannot be tuned, so readiness alone isn\u0026rsquo;t a full solution.\nYou can for sure build I/O loops using this signal, but it\u0026rsquo;s usually not optimal because:\nAs you can see from the documentation and the example, the writability doesn\u0026rsquo;t always mean that the send finishes successfully. Readiness doesn\u0026rsquo;t provide an available size, which could be less than the size of the encoded message to send. Additional logic is required to handle partial writes. There is a way to query this size, but it\u0026rsquo;s a syscall which is expensive to make. The API is TCP-specific and will not work out of the box for a generic Tokio stream I/O, which is typically bound to the AsyncRead+AsyncWrite traits. Another idea is to use a smart write buffer that wraps around the stream I/O writer. It should be able to report when it\u0026rsquo;s ready to write, and expose an async readiness-like call you can await. Its \u0026ldquo;ready to write\u0026rdquo; signal effectively triggers a data flush and propagates backpressure. This is exactly how the Sink trait is designed, with its poll_ready() and start_send() methods. Tokio already provides several implementations of this pattern, some of which we\u0026rsquo;ll use later when discussing Framed I/O.\nAnother approach (which we are not going to use) is to handle backpressure on the consumer\u0026rsquo;s side. In this model, the consumer explicitly requests N records or bytes by pulling them from the producer. For example, the Java Apple ServiceTalk framework uses this pattern. Another example is HTTP/2 streams, where the consumer dictates how much data it wants to receive with WINDOW_UPDATE frames. However, this approach is not available for plain TCP streams or generic I/O streams, so we will not cover it further.\nConcurrency # The concurrency feature might look like low-hanging fruit since we\u0026rsquo;re already using the Tokio framework. But the reality is harsher. Rust\u0026rsquo;s ownership model and borrow checker prevent us from writing a simple, naïve fix:\nloop { tokio::select! { Some(msg) = rx.recv() =\u0026gt; { tokio::select! { // \u0026lt; --------- change res = stream.write_all(\u0026amp;msg) =\u0026gt; {...}, res = stream.read(\u0026amp;mut read_buf) =\u0026gt; {...}, } stream.flush().await?; println!(\u0026#34;client\u0026#39;s written\u0026#34;); } res = stream.read(\u0026amp;mut read_buf) =\u0026gt; { let n = res?; if n == 0 { // EOF - server closed connection eprintln!(\u0026#34;Server closed the connection.\u0026#34;); break; //exit } print!(\u0026#34;{}\u0026#34;, String::from_utf8_lossy(\u0026amp;read_buf[..n])); } else =\u0026gt; break } } The compiler error:\ncannot borrow stream as mutable more than once at a time second mutable borrow occurs here (rustc E0499) The AsyncRead and AsyncWrite trait methods require a mutable reference to self (more precisely, Pin\u0026amp;\u0026lt;\u0026amp;mut Self\u0026gt;, though Pin/Unpin is out of scope for this post). We can\u0026rsquo;t place both operations in select! expressions and await them concurrently, because that would require borrowing the same value mutably twice. So we need a different approach that satisfies the Rust compiler.\nAn additional interesting question is what concurrency actually means for our network application.\nA high-load network application, such as a web server or a network proxy, that must handle thousands of simultaneous connections (the C10k problem) has not many practical options and usully use an event loop (for example, epoll on Linux).\nNote:\nThere is also an io_uring approach, but the current Tokio version offers only limited support for it.\nAs a rule of thumb, there is typically one event loop per CPU core, and the application maps sockets to these threads according to its scheduling strategy.\nThe implications of such a design are:\nParallelism is bounded by the number of epoll threads. True parallel execution cannot exceed the number of event-loop threads available. Developers must decide how to handle socket I/O. The read and write halves can run concurrently on the same thread, or in true parallel fashion, where two independent threads operate on the same socket. The answer to the latter is not obvious. While parallel execution may appear superior, it introduces several overheads and limitations:\nCross-thread synchronization overhead at the system level Asynchronous task-scheduler overhead (e.g., the Tokio runtime) CPU cache effects, including misses and cache-line contention For an I/O-intensive application, it is often more efficient to keep both halves of a socket within a single scheduled task (thread). This is the default approach recommended by both the Rust Tokio runtime (as we saw earlier with the select! macro) and Go. In practice, stream I/O is typically handled inside a single actor, which then communicates with other parts of the application through asynchronous channels or other low-overhead synchronization primitives.\nLet\u0026rsquo;s take a quick look at some concurrency primitives Tokio provides:\nselect! macro; join! and try_join! macros; spawn() function. Note:\nfutures::stream::FuturesUnordered can be very useful too, but it\u0026rsquo;s out of scope here.\nKey distinction: spawn() launches a future on the runtime\u0026rsquo;s thread pool (Tokio task). The other primitives run futures in the current Tokio task concurrently.\nOne more thing to note before we start is that try_join!, like all try_-prefixed macros, operates on a Result and returns as soon as the first error occurs, signalling the remaining child futures to stop. This behavior is useful for implementing short-circuit logic.\nRead next chapter → "},{"id":7,"href":"/docs/page-cache/1-prepare-environment-for-experiments/","title":"Prepare environment for experiments","section":"Linux Page Cache series","content":" Prepare environment for experiments # Last updated: Oct 2025 Before starting, I want to be on the same page with the reader so that any example or code snippet can be executed, compiled, and checked. Therefore we need a modern GNU/Linux installation to play with code and kernel.\nIf you are using Windows or Mac OS, I would suggest installing Vagrant with Virtual Box. For the GNU/Linux distributive, I\u0026rsquo;d like to use Arch Linux. Arch is a good example of an actual modern version of the GNU/Linux system (BTW, I use Arch Linux). It supports the latest kernels, systemd and cgroup v2.\nIf you\u0026rsquo;re already on Linux, you know what to do 😉.\nCan I use docker?\nUnfortunately, no. We need a system where we can go nuts and play around with cgroup limits, debug programs with low-level tools and run code as a root user without any limitations.\nSo below, I\u0026rsquo;m showing all you need to install on Arch.\nArch Linux provisioning # When you get your Arch running, please update it and install the following packages:\n$ pacman -Sy git, base-devel, go We need to install yay (https://github.com/Jguer/yay) in order to be able to setup software from community-driven repositories:\n$ cd ~ $ git clone https://aur.archlinux.org/yay.git $ cd yay $ makepkg -si Install vmtouch tool from aur:\n$ yay -Sy vmtouch We will need page-type tool from the kernel repo, so the easiest way to install it is to download the linux kernel release and make it manually:\n$ mkdir kernel $ cd kernel $ wget https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/snapshot/linux-5.14.tar.gz $ tar -xzf linux-5.14.tar.gz $ cd linux-5.14/tools/vm $ make $ sudo make install Now we are almost ready. We need to generate a test data file, which will be used in our experiments with Page Cache:\n$ dd if=/dev/random of=/var/tmp/file1.db count=128 bs=1M And the final step is dropping all linux caches in order to get a clean box:\n$ sync; echo 3 | sudo tee /proc/sys/vm/drop_caches Read next chapter → "},{"id":8,"href":"/docs/resolver-dual-stack-application/1-what-is-a-stub-resolver/","title":"What is a stub resolver?","section":"DNS resolvers and Dual-Stack applications","content":" 1. What is a stub resolver? # Last updated: Oct 2025 First of all, let’s shed some light on what a stub resolver is.\nWhenever someone begins talking about hostname resolution issues or nameserver changes, the first thing most people think of is the /etc/resolv.conf configuration file. Indeed, /etc/resolv.conf is a core and fundamental part of the local resolver system, and we will discuss it in detail later in this series, including how it’s managed on modern GNU/Linux distributions with systemd. However, it’s far from being the only component involved in converting a hostname string into a list of IP addresses. Often, other lesser-known parts of the system may cause unpredictable behaviors and have their own unique features and limitations.\nAt its core, a stub resolver is a \u0026ldquo;simple\u0026rdquo; DNS client designed primarily to send DNS requests to a recursive name server. Rather than starting with system configuration files, I’d like to begin our journey from the perspective of an application.\nAs you might expect, when any application (whether it’s a client or server, CLI or desktop, web or native) needs to resolve a domain name to IP addresses, it must query a DNS server. Typically, all programming languages include functions or classes in their standard libraries to facilitate such requests. These components, known as stub resolvers, read various system configuration files, decide which record types to request (A and/or AAAA), make the DNS queries, and sort and format the returned IP addresses for the caller.\nHistorically, in UNIX and POSIX environments where the C language has been foundational, the basic function for domain name resolution was gethostbyname(), which was later deprecated and replaced by getaddrinfo().\nSo, let’s begin here with a bottom-up approach, starting with the gethostbyname() function and its successor, getaddrinfo(), from the libc specification.\nRead next chapter → "},{"id":9,"href":"/docs/resolver-dual-stack-application/2-history-gethostbyname/","title":"History: gethostbyname() and old good friends","section":"DNS resolvers and Dual-Stack applications","content":" 2. History: gethostbyname() and old good friends # Last updated: Oct 2025 Please do not use any of the code snippets from this chapter in your projects. They are provided solely for historical and educational purposes. Instead, you should use getaddrinfo(). The gethostbyname (man 3 gethostbyname) function first appeared in the 1980s and has been a part of the networking landscape ever since. Despite its obsoletion, some programs still use it. It was deprecated in POSIX.1-2001, over two decades ago, due to its internal design limitations and limited functionality. However, for a long time, it was the preferred and standardized helper function for resolving a domain name into a list of IP addresses.\nThe number of drawbacks and problems which made its usage obsolete:\nThe lack of IPv6 support. Although there is a Linux-specific gethostbyname2(), which can resolve IPv6 addresses, the standard gethostbyname() function is limited to IPv4 only. struct hostent *host_info = gethostbyname2(hostname, address_family); if (host_info == NULL) { fprintf(stderr, \u0026#34;Error: Could not resolve hostname %s\\n\u0026#34;, hostname); exit(EXIT_FAILURE); } for (char **addr_list = host_info-\u0026gt;h_addr_list; *addr_list != NULL; addr_list++) { char ip[INET6_ADDRSTRLEN]; void *address = (host_info-\u0026gt;h_addrtype == AF_INET) ? (void *) ((struct in_addr *) *addr_list) : (void *) ((struct in6_addr *) *addr_list); const char *result = inet_ntop(host_info-\u0026gt;h_addrtype, address, ip, sizeof(ip)); if (result == NULL) { perror(\u0026#34;inet_ntop\u0026#34;); exit(EXIT_FAILURE); } printf(\u0026#34; %s\\n\u0026#34;, ip); } In theory, you could use gethostbyname2(), but it still lacks the capability to combine IPv4 and IPv6 results, properly sort them according to the standard RFC 6724 (which we will discuss later), and suffers from a legacy internal design.\nNon-Reentrant: gethostbyname() is not thread-safe. It uses internal static data structures, meaning that subsequent calls to gethostbyname() overwrite the data returned by previous calls. Limited Information: The hostent structure returned by gethostbyname() provides limited information, primarily focusing on the IP address and not on other details like the service or protocol. This often leads to additional string concatenations and the creation of new data structures, which result in more boilerplate code for subsequent socket API calls. It’s worth mentioning that gethostbyname() has a unique feature: it returns a list of IP addresses in a semi-random order with each call, essentially providing a round-robin DNS. This allows for the simple implementation of a client-side load balancer among the returned A records.\nAnother UNIX libc function, getipnodebyname (man 3 getipnodebyname), has been removed from GNU/Linux but may still exist on other platforms. Here is how it appears in Python 3.12 (cpython) for some other platforms. File Modules/getaddrinfo.c:\n#ifdef ENABLE_IPV6 if (af == AF_UNSPEC) { hp = getipnodebyname(hostname, AF_INET6, AI_ADDRCONFIG|AI_ALL|AI_V4MAPPED, \u0026amp;h_error); } else hp = getipnodebyname(hostname, af, AI_ADDRCONFIG, \u0026amp;h_error); #else hp = gethostbyname(hostname); h_error = h_errno; #endif Read next chapter → "},{"id":10,"href":"/docs/fd-pipe-session-terminal/2-pipes/","title":"Pipes","section":"GNU/Linux shell related internals","content":" Pipes # Last updated: Oct 2025 Contents\nHow shells internally create pipes Pipe and write buffer SIGPIPE signal $pipestatus, $? and pipefail FIFO or Named pipes pv tool Pipe usage Packets pipe mode (O_DIRECT) Pipe Nonblocking I/O Partial writes and syscall restarts Pipe performance: splice(), vmsplice() and tee() The pipe is a neat feature of the Linux kernel that allows us to build one-directional communication channels between related processes (often a parent and a child).\nPipes are usually well known from shells, where we use \u0026ldquo;|\u0026rdquo; symbol to build command pipelines. But first of all, the pipe is a system call, or actually, there are 2 of them: pipe() and pipe2() (man 2 pipe).\nYou can think of a pipe as a memory buffer with a byte stream API. Thus, by default, there are no messages or strict boundaries. The situation has changed since the Linux kernel 3.4 where the O_DIRECT flag and the packet mode were introduced. We will touch all variant of working with pipes in this chapter.\nAnother important feature of pipes is the max size of an atomic write. The PIPE_BUF constant (man 7 pipe) determines it and sets it to 4096 bytes. Please, read the man carefully if you want to rely on this guarantee.\nAs a result of a streaming nature, a reader and a writer can use completely different user-space buffer sizes if they want. All written bytes are read sequentially, so making the lseek() syscall for a pipe is impossible.\nThe pipes also provide a convenient notification API for both ends. The write calls to a pipe block if the internal kernel buffer is full. The writer will block or return EAGAIN (if it’s in nonblocking mode) until sufficient data has been read from the pipe to allow the writer to complete. On the other hand, if all pipe readers close their read file descriptors, the writer will get the SIGPIPE signal from the kernel, and all subsequent write() calls will return the EPIPE error.\nFrom a reader’s perspective, a pipe can return a zero size read (end-of-file, EOF) if all writers close all their write pipe file descriptors. A reader blocks if there is nothing to read until data is available (you can change this by opening a pipe in the nonblocking mode).\nFigure 2. – Using a pipe to connect 2 processes Pipes are widely used in shells. The elegance of such an approach is that processes don’t have to know that they use pipes. They continue working with their standard file descriptors (stdin, stdout and stderr) as usual. Developers also don’t need to make any changes in their program\u0026rsquo;s source code in order to support this concept. It makes the process of connecting 2 programs composite, flexible, fast and reliable. Of course, in order to support such communication, shells have to do some additional work before spawning new commands (more details and examples below).\nInternally, a pipe buffer is a ring buffer with slots. Each slot has a size of a PIPE_BUF constant. The number of slots is variable, and the default number is 16. So, if we multiply 16 by 4KiB, we can get a default size of 64KiB for a pipe buffer.\nWe can control the capacity of a pipe by calling the fcntl() with the F_SETPIPE_SZ flag. A pipe\u0026rsquo;s system max size limit can be found in the /proc/sys/fs/pipe-max-size (man 7 pipe).\nWe can get the size of unread bytes in a pipe by calling ioctl() with FIONREAD operation. We’ll write an example later.\nThe usual question about pipes is, do we really need them? Can we use regular files instead? There are several issues and lacking of API with using files instead of pipes:\nThere is no easy way to notify a writer that a reader has stopped reading. For a reader, we can set up inotify (man 7 inotify) to efficiently track whether new changes appear. Also, regular files don’t have nonblocking API (this is changing with io_uring, but still, it’s much harder to use it in comparison with the pipe() syscall). One final introduction remark is that a pipe can be used by more than 2 processes. It’s possible to have multiple writers and readers for a single pipe. It’s not usuall because of the streaming nature of pipes and no clear boundaries by default, but with the new packet mode, it\u0026rsquo;s become more useful in some situations.\nHow shells internally create pipes # With shells we usually use pipes to connect the stdout and/or the stderr of a process and stdin of another process. For example:\nstdout to stdin:\n$ command1 | command2 stdout and stderr to stdin:\n$ command1 |\u0026amp; command2 or\ncommand1 2\u0026gt;\u0026amp;1 | command2 So let’s understand how shells connect the following 2 commands internally.\n$ ls -la | wc -l As we already know, a shell process has three special standard open file descriptors. Thus, all its children inherit them by default because of the fork() syscalls. The following simple program shows how a shell can create a pipe and connect 2 programs. It creates a pipe in the parent process, then makes a fork() call twice in order to run execve() for the ls and wc binaries. Before the execve() calls, the children duplicate the needed standart fd with one of the ends of the pipe.\nimport sys import os r, w = os.pipe() ls_pid = os.fork() if not ls_pid: # child os.close(r) os.dup2(w, sys.stdout.fileno()) os.close(w) os.execve(\u0026#34;/bin/ls\u0026#34;, [\u0026#34;/bin/ls\u0026#34;, \u0026#34;-la\u0026#34;, ], os.environ) wc_pid = os.fork() if not wc_pid: # child os.close(w) os.dup2(r, sys.stdin.fileno()) os.close(r) os.execve(\u0026#34;/usr/bin/wc\u0026#34;, [\u0026#34;/usr/bin/wc\u0026#34;, \u0026#34;-l\u0026#34;], os.environ) os.close(r) os.close(w) for i in range(2) : pid, status = os.waitpid(-1, 0) And if we run it:\n$ python3 ./simple_pipe.py 12 The one important note about the above code is how I close all not needed file descriptors of the pipe. We have to close them to allow the kernel to send us correct signals, block operations, and return EOF when there are no more writers.\nPipe and write buffer # Modern programming languages (for example, python) often buffer all their writes in memory before the actual write syscall executes. The main idea of such buffering is to get better I/O performance. It’s cheaper to make one big write() call than several smaller ones. There are 2 types of buffers that are widely used:\nBlock buffer For example, if its size is 4KiB, the buffer will write its content (flush) to the underlying fd only when it fills up completely or the explicit flush() call is invoked. Line buffer This buffer type flushes its content when the new line character write occurs to the buffer. The python (and other programming languages) changes the buffer type depending on the type of the underlying file descriptor. If the fd is a terminal, the buffer will be a line buffer. That makes sense because when we are in the interactive shell, we want to get the output as soon as possible. However, a block buffer will be used for pipes and regular files because it’s usually OK to postpone the flush for better performance.\nThe libc function isatty() (man 3 isatty) tests whether a file descriptor refers to a terminal.\nLet’s demonstrate this behavior with 2 scripts connected by a pipe. The first one will print 10 lines to stdout, and the other one will consumethese lines from its stdin.\nThe printing script print.py:\nimport time for i in range(4): print(f\u0026#34;{i}\u0026#34;) time.sleep(0.1) And the consumer script: stdin.py\nimport fileinput for i, line in enumerate(fileinput.input()): print(f\u0026#34;{i+1}: {line.rstrip()}\u0026#34;) If you run the print.py, you should see how the output will be printed in a line-by-line manner:\n$ python3 ./print.py 0 1 2 3 Now, if we run these 2 scripts with a pipe, you should see that the output freezes for a second, and it prints all lines at once afterwards:\n$ python3 ./print.py | python stdin.py 1: 0 2: 1 3: 2 4: 3 Now let’s make it smoother. We need to add a flush() call after each print() in the print.py:\nimport time import sys for i in range(4): print(f\u0026#34;{i}\u0026#34;) sys.stdout.flush() time.sleep(0.1) And rerun it. Now, you should be able to see that the lines appear smoothly one-by-one: $ python3 ./print.py | python stdin.py 1: 0 2: 1 3: 2 4: 3 It’s worth knowing that some core utilities have an option to control their buffering. For example, the grep can be forced to use a per line buffer with the --line-buffered option. Of course, this will give you a more interactive experience with some performance penalties. You can play with it and compare the outputs:\n$ strings /var/tmp/file1.db | grep --line-buffered -E \u0026#34;^sek\u0026#34; | cat sek.^ \\ sekA sekt $ strings /var/tmp/file1.db | grep -E \u0026#34;^sek\u0026#34; | cat sek.^ \\ sekA sekt SIGPIPE signal # One of the exciting aspects of the pipes is their notification and synchronization features.\nWe intentionally closed all unused fd in the above code with fork() and execve() calls. The reason for doing that was not only our desire to save file descriptors and write a cleaner code but also to support the pipe notification features.\nIf all readers close their fd of the pipe and a writer tries to send data into it, the writer process will get the SIGPIPE signal from the kernel. This is a brilliant idea. Let’s assume we want to grep a huge nginx access log (for example, 500GiB) in order to find a target string and care only about the first three results:\n$ cat /var/log/nginx/access.log | grep some_string | head -3 So, if we assume that the log file has all three target lines somewhere at the beginning of the file, the head command will exit almost immediately. Thus we don’t need to continue reading the file. As so, when the head util exits, it closes all its fd, including the stdin (which is a pipe). The subsequent writes from the grep will cause the kernel to send the SIGPIPE signal to it. The default handler for the SIGPIPE signal is to terminate, so grep will exit and close all its fd, including its stdin. And in its turn, the cat command will exit after receiving its own SIGPIPE signal. So the exit of the head starts the cascading exit of the whole shell pipeline.\nA shell are usually waiting on the processes with the waitpid() syscall and collects all return codes. When it sees that all the process pipeline has finished, the shell sets the exit status variable $? to the returned code of the last command in the pipeline (head in our case) and populates the $PIPESTATUS (bash) or $pipestatus (zsh) array variable with all return codes of the piplene.\nLet me demonstrate it. As you can see, all the above works without any support in the cat, grep or head tools. It’s the beauty of the pipes and shells collaboration.\nNow we are ready to write our own prove of the above:\nimport signal import os import sys def signal_handler(signum, frame): print(f\u0026#34;[pipe] signal number: {signum}\u0026#34;, file=sys.stderr) os._exit(signum) signal.signal(signal.SIGPIPE, signal_handler) for i in range(9999): print(f\u0026#34;{i}\u0026#34;) And run it:\n$ python3 ./print.py | head -3 0 1 2 [pipe] signal number: 13 \u0026lt;-------------- $ echo ${PIPESTATUS[@]} 13 0 $pipestatus, $? and pipefail # We are ready to take a bit closer look at the exit statuses of a bash pipeline. By default, the last command in the pipe is used for the $? variable, which could sometimes lead to unexpected results. For instance:\n$ echo \u0026#39;some text\u0026#39; | grep no_such_text | cut -f 1 $ echo $? 0 $ echo \u0026#39;some text\u0026#39; | grep no_such_text | cut -f 1 $ echo ${PIPESTATUS[@]} 0 1 0 But fortunately, we can change this behavior with a pipefail bash option:\n$ set -o pipefail $ echo \u0026#39;some text\u0026#39; | grep no_such_text | cut -f 1 $ echo $? 1 FIFO or Named pipes # So far, we have been talking about pipes in the context of related processes (a parent and its children), but we also have the option to share a pipe easily among any number of unrelated processes. We can create a disk reference for a pipe which is called a named pipe or a FIFO file.\nThere is one high-level man 7 fifo and a tool to create a fifo file mkfifo (man 1 mkfifo)).\nThe permission control is based on regular file permissions. So, if a process has \u0026ldquo;write\u0026rdquo; permissions, it can write to this named pipe.\nAll other aspects are identical to a regular pipe. The kernel internally creates the same pipe object and doesn’t store any data on disk.\nThe FIFO file could be helpful when you need to build a connection between completely unrelated programs or daemons without changing their source code.\npv tool # pv or pipe viewer (man 1 pv) is a nifty tool to work with pipes and file descriptors. We can insert it in any pipeline place, and it will show additional info such as ETA, write rate and amount of transferred data with an incredible visual progress bar.\nHere is the basic usage with a file shows us how fast the reading strings command can consume a file:\n$ pv /var/tmp/file1.db | strings \u0026gt; /dev/null 100MiB 0:00:01 [67.9MiB/s] [===========================================\u0026gt;] 100% It also can rate limit a pipe, which is really useful for tests:\n$ cat /var/tmp/file1.db | pv --rate-limit=1K | strings Another neat feature is monitoring a process’s progress for every open file descriptor. Under the hood it uses procfs and fdinfo folders to get the positions for all opened files:\n$ pv -d 6864 3:/var/tmp/file1.db: 234 B 0:00:01 [0.00 B/s] [\u0026gt; ] 0% ETA 0:00:00 123:/var/tmp/file1.db: 234 B 0:00:01 [0.00 B/s] [\u0026gt; ] 0% ETA 0:00:00 Pipe usage # We can get the size of unread bytes in a pipe by calling ioctl() with FIONREAD and a pipe fd. But how to get pipe usage from an unrelated process that doesn’t have the pipe file descriptor, for instance from a monitoring tool. Or, for example, we started a long running pipeline and not sure if the consumer of the pipe is reading data:\n$ dd if=/dev/urandom | strings \u0026gt; /dev/null We can, of course, use strace and check the read() syscalls in its output, but the reader could a read() syscall with a huge buffer that we can miss in the strace output.\nSo, in order to achieve the goal, we need to get the pipe file descriptor somehow. The most elegant solution (but not without drawbacks) is to steal the fd with sys_pidfd_getfd() system call and then use ioctl to get usage information.\nThe code can be something like the following:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;syscall\u0026#34; \u0026#34;golang.org/x/sys/unix\u0026#34; ) const ( sys_pidfd_open = 434 sys_pidfd_getfd = 438 FIONREAD = 0x541B ) func pidfd_open(pid int) (int, error) { r1, _, err := syscall.Syscall(sys_pidfd_open, uintptr(pid), 0, 0) if err != 0 { return -1, err } return int(r1), nil } func pidfd_getfd(pidfd, targetfd int) (int, error) { r1, _, err := syscall.Syscall(sys_pidfd_getfd, uintptr(pidfd), uintptr(targetfd), 0) if err != 0 { return -1, err } return int(r1), nil } func main() { var ( pid, fd int err error ) pid, err = strconv.Atoi(os.Args[1]) fd, err = strconv.Atoi(os.Args[2]) if err != nil { panic(err) } pidfd, err := pidfd_open(pid) if err != nil { panic(err) } newFd, err := pidfd_getfd(pidfd, fd) if err != nil { panic(err) } for { size, err := unix.IoctlGetInt(newFd, FIONREAD) if err != nil { panic(err) } fmt.Printf(\u0026#34;size:\\t%d\\n\u0026#34;, size) } } Run our target pipeline:\n$ dd if=/dev/urandom | pv --rate-limit 30K | strings \u0026gt; /dev/null ^ KiB 0:00:16 [27.6KiB/s] And run our tool:\n$ sudo go run ./pipe_capacity.go 19990 1 size: 62464 size: 62464 size: 63488 size: 63488 size: 63488 size: 63488 size: 63488 The main drawback of such a technique is that we are holding the write end of the pipe open. This can lead to an extended life of the reader because it will block on an empty pipe instead of getting EOF (see the pipe notification feature explained above).\nPackets pipe mode (O_DIRECT) # Since the Linux kernel 3.4, a pipe can be created with the O_DIRECT flag. It puts it into packet mode. From my point of view, this mode can be successfully used only with writes and reads that are less or equal to the PIPE_BUF size (4KiB) because atomicity is guaranteed only in this case.\nThe packet mode is different from the default stream mode in the following ways:\nThe kernel doesn’t try to merge writes into one ring buffer slot here and here. It, of course, leads to the underutilization of the pipe buffer, but provides guarantee of boundaries instead. Readers with read() of PIPE_BUF size get the same messages as the writers wrote; If the reader’s buffer is less than the data in the slot (some misconfigured reader infiltrated), then the remaining data is discarded to protect the boundaries of messages. Now let’s write an example with 2 writers and 2 readers. Every writer writes less than 4KiB, so readers will get one full message on every read:\nimport sys import os import time PIPE_BUF = 4096 print(f\u0026#34;supervisor: {os.getpid()}\u0026#34;, file=sys.stderr) r, w = os.pipe2(os.O_DIRECT) # fork 2 writers for instance in range(2): writer_pid = os.fork() if not writer_pid: print(f\u0026#34;writer{instance}: {os.getpid()}\u0026#34;, file=sys.stderr) os.close(r) pid = os.getpid() for i in range(100): os.write(w, f\u0026#34;writer{instance}: {i}\u0026#34;.encode()) time.sleep(1) # fork 2 readers for instance in range(2): reader_pid = os.fork() if not reader_pid: print(f\u0026#34;reader{instance}: {os.getpid()}\u0026#34;, file=sys.stderr) os.close(w) pid = os.getpid() for i in range(100): data = os.read(r, PIPE_BUF) if not len(data): break print(f\u0026#34;reader{instance}: {data}\u0026#34;) os.close(r) os.close(w) for i in range(4): os.waitpid(-1,0) Run it:\n$ python3 ./packets.py supervisor: 1200 writer0: 1201 reader0: 1203 reader0: b\u0026#39;writer0: 0\u0026#39; writer1: 1202 reader0: b\u0026#39;writer1: 0\u0026#39; reader1: 1204 reader0: b\u0026#39;writer1: 1\u0026#39; reader0: b\u0026#39;writer0: 1\u0026#39; reader0: b\u0026#39;writer0: 2\u0026#39; reader0: b\u0026#39;writer1: 2\u0026#39; reader0: b\u0026#39;writer0: 3\u0026#39; reader1: b\u0026#39;writer1: 3\u0026#39; reader0: b\u0026#39;writer0: 4\u0026#39; reader1: b\u0026#39;writer1: 4\u0026#39; If we remove the O_DIRECT flag and rerun it, we can see how readers start to break the boundaries of messages and, from time to time, get 2 messages instead of 1. The situation could be even worse, and the boundaries could be violated if a reader reads a buffer less than a writer’s written.\n… reader0: b\u0026#39;writer0: 2writer1: 2\u0026#39; reader1: b\u0026#39;writer0: 3writer1: 3\u0026#39; reader1: b\u0026#39;writer1: 4writer0: 4\u0026#39; reader0: b\u0026#39;writer0: 5\u0026#39; reader1: b\u0026#39;writer1: 5\u0026#39; reader0: b\u0026#39;writer0: 6writer1: 6\u0026#39; reader1: b\u0026#39;writer1: 7\u0026#39; reader0: b\u0026#39;writer0: 7\u0026#39; PIPE Nonblocking I/O # Unlike regular files, pipes natively support nonblocking I/O. You can create a new pipe or switch an existing pipe to the nonblocking I/O mode. The most important outcome of doing this is the ability to poll a pipe using poll(), select() and epoll() event notification facilities. Nonblocking mode saves the CPU (if correctly written) and provides a unified API for programs and developers.\nNonblocking mode might be also useful to write user space busy loops in order to get better throughput by trading more CPU usage. The idea is to skip some kernel wake up logic and return from kernel mode as soon as possible.\nThe following example shows that even with python, where exceptions are slow, we can get a better throughput with a busy loop:\nNo busy loop code:\nimport os rand = os.getrandom(1\u0026lt;\u0026lt;16-1) while True: os.write(1, rand) Max throughput: in my virtual machine:\n$ python3 ./no_busy_loop.py | pv | strings \u0026gt; /dev/null 631MiB 0:00:10 [74.5MiB/s] With busy loop:\nimport os import fcntl flags = fcntl.fcntl(1, fcntl.F_GETFL, 0) fcntl.fcntl(1, fcntl.F_SETFL, flags | os.O_NONBLOCK) rand = os.getrandom(1\u0026lt;\u0026lt;16-1) while True: try: n = os.write(1, rand) except BlockingIOError: continue I was able to get 10% better throughput in my test vm:\n$ python3 ./busy_loop.py | pv | strings \u0026gt; /dev/null 799MiB 0:00:11 [82.7MiB/s] Partial writes and syscall restarts # Now we are ready to delve into the kernel internals a bit deeper. Let’s assume we want to write 512 MiB of some data to a pipe. We already have it all in memory and call the write() syscall:\ndata = os.getrandom(1\u0026lt;\u0026lt;29) # 512 MiB os.write(1, data) # where stdout is a pipe in a shell pipeline We know from the above that the size of a pipe by default is 64KiB, but the man 7 pipe says:\nApplications should not rely on a particular capacity: an application should be designed so that a reading process consumes data as soon as it is available, so that a writing process does not remain blocked. It means that, in default blocking I/O mode, our write() call should block until all bytes have not been transferred through a pipe. It makes sense, the userspace application should not, in theory, care about the underlying kernel machinery. We have a userspace buffer with data, and we should be able to write it in one blocking call. But fortunately or unfortunately, things are a bit more complicated.\nOne theory we also need to recall here is that the kernel puts a process into a sleep state if a syscall blocks. There are 2 sleep types: interruptible (S) sleep and uninterruptible (D) sleep.\nFor example, the above code snippet with the write() syscall puts a process into the interruptible state (because it writes to a pipe):\n$ ps axu | grep write2.py vagrant S+ 20:36 0:15 python3 ./write2.py where S informs us that the process is in the interruptible sleep (waiting for an event to complete).\nSuch processes are removed from the kernel scheduler list and are put in a dedicated queue waiting for a particular event.\nThe interruptible sleep state differs from the uninterruptible in that the kernel can deliver signals to the process. Rephrasing, it’s possible to receive and handle signals during the blocking syscall in the interruptible sleep state. But the reasonable question is, what happens after the signal is handled in the middle of such a syscall? Let’s figure it out.\nWe start with the pipe_write() kernel function, where we can find the following code:\nif (signal_pending(current)) { if (!ret) ret = -ERESTARTSYS; break; } The above confirms that the signal could interrupt the process during the blocking pipe write() syscall. The interesting part here is the ret variable. If it doesn’t have anything, the kernel sets it to the -ERESTARTSYS error. Otherwise, the kernel leaves it as is. In both cases, the kernel exits from the infinitive for loop. This infinitive for loop is what keeps the write() syscall in the blocking state, and it’s in charge of data transferring between usersapace buffer (512 MiB in our case) and kernel space pipe ring buffer.\nIn turn, the ret variable stores the number of transferred through the pipe bytes. It can be much bigger than the 64KiB default pipe size because there is always at least one consumer that reads from this pipe.\nOne more piece of information that will help us understand the following examples and behavior is the ERESTARTSYS error. It signals that the kernel can safely restart a syscall because it hasn\u0026rsquo;t done any meaningful work and has no side effects.\nWith all the above said, we are ready to do some coding and debugging in order to answer the question of whether it is sufficient to do one write() to a pipe.\nIn our tests we’ll use bpftrace. It’s a handy tool that allows us to trace kernel functions via eBPF trampolines, which allows kernel code to call into BPF programs with practically zero overhead. We’ll be tracing a pipe_write() kernel function to get insides about the actual pipe writes.\nLet’s start with a producer of data. Here we have a signal handler for the SIGUSR1 signal, which prints the signal code, a buffer with random 512 MiB, and one write() syscall to stdout fd.\nimport signal import sys import os def signal_handler(signum, frame): print(f\u0026#34;signal {signum} {frame}\u0026#34; ,file=sys.stderr) signal.signal(signal.SIGUSR1, signal_handler) print(os.getpid(), file=sys.stderr) rand = os.getrandom(1\u0026lt;\u0026lt;29) print(\u0026#34;generated\u0026#34;, file=sys.stderr) n = os.write(1, rand) print(f\u0026#34;written: {n}\u0026#34;, file=sys.stderr) Now we need to write a consumer for a pipe. It will sleep for 30 seconds and afterward reads all data from the stdin in 4KiB chunks.\nimport time import sys print(\u0026#34;start sleeping\u0026#34;, file=sys.stderr) time.sleep(30) print(\u0026#34;stop sleeping\u0026#34;, file=sys.stderr) r = sys.stdin.buffer.read(4096) while len(r) \u0026gt; 0: r = sys.stdin.buffer.read(4096) print(\u0026#34;finished reading\u0026#34;, file=sys.stderr) We are ready for experiments. Let’s launch bpftrace first. We’re looking for python3 commands and want to print the return value of the pipe_write() kernel function. Please, run the following in a terminal window.\n$ sudo bpftrace -e \u0026#39;kretfunc:pipe_write /comm == \u0026#34;python3\u0026#34;/ { printf(\u0026#34;%d\\n\u0026#34;, retval);}\u0026#39; Attaching 1 probe... In another terminal window, we need to start our shell pipeline under strace tool for the writer. strace logs all write() syscalls into log.txt.\n$ strace --output log.txt -s0 -e write -- python3 ./write2.py | python3 ./slow_reader.py start sleeping 1791 generated We are in a situation where the buffer is full, the writer is in the interruptible sleep state (S), and the reader is still sleeping. It’s time to open one more console and send the SIGUSR1 signal to the blocked writer:\n$ kill -USR1 1791 In the console with the pipeline, you should eventually see something like the following:\n$ strace --output log.txt -s0 -e write -- python3 ./write2.py | python3 ./slow_reader.py start sleeping 1791 generated signal 10 \u0026lt;frame at 0x7f59b135da40, file \u0026#39;/home/vagrant/data/blog/post2/./write2.py\u0026#39;, line 15, code \u0026lt;module\u0026gt;\u0026gt; written: 65536 stop sleeping finished reading The writer received the signal and exited. It also printed that it had successfully transferred only 65536 bytes (doesn\u0026rsquo;t look familiar?).\nThe console with the bpftrace confirms the above. The pipe_write() syscall managed to write only 64KiB of data.\n$ sudo bpftrace -e \u0026#39;kretfunc:pipe_write /comm == \u0026#34;python3\u0026#34;/ { printf(\u0026#34;%d\\n\u0026#34;, retval);}\u0026#39; Attaching 1 probe... 65536 The strace log shows the same:\n$ cat log.txt write(1, \u0026#34;\u0026#34;..., 536870912) = 65536 --- SIGUSR1 {si_signo=SIGUSR1, si_code=SI_USER, si_pid=806, si_uid=1000} --- It looks like it is not sufficient to have only one syscall. If we now open the write() syscall documentation (man 2 write):\nNote that a successful write() may transfer fewer than count bytes. Such partial writes can occur for various reasons; for example, because there was insufficient space on the disk device to write all of the requested bytes, or because a blocked write() to a socket, pipe, or similar was interrupted by a signal handler after it had transferred some, but before it had transferred all of the requested bytes. In the event of a partial write, the caller can make another write() call to transfer the remaining bytes. The subsequent call will either transfer further bytes or may result in an error (e.g., if the disk is now full). The documentation answers our initial question, but there is something, even more, to show here.\nAs we saw, the pipe_write() function can also return the ERESTARTSYS error if no bytes are written. It is an interesting case, and the kernel can be asked to restart such syscalls automatically without any userspace retries. It makes total sense; the syscall didn’t have any chances to do its work, so the state is the same. The configuration of the kernel restart is done by setting the SA_RESTART flag. By default, it is already enabled in python. You can check it with the strace:\nrt_sigaction(SIGUSR1, {sa_handler=0x45c680, sa_mask=~[], sa_flags=SA_RESTORER|SA_ONSTACK|SA_RESTART|SA_SIGINFO, sa_restorer=0x45c7c0}, NULL, 8) = 0 Now we are finished with all the theory and experiments. But still have the task unresolved. What is the recommended way to write such big buffers into a pipe? We can find the answer in the python source code and its buffer writer implementation:\ndef _flush_unlocked(self): … while self._write_buf: … n = self.raw.write(self._write_buf) … del self._write_buf[:n] The above snippet shows how python restarts the write() syscall in case of a partial write.\nNow let’s rewrite our producer to use a buffered writer and demonstrate two restart concepts:\nthe automatic syscall restart; the restart after a partial write. import signal import sys import os def signal_handler(signum, frame): print(f\u0026#34;signal {signum} {frame}\u0026#34; ,file=sys.stderr) signal.signal(signal.SIGUSR1, signal_handler) print(os.getpid(), file=sys.stderr) rand = os.getrandom(1\u0026lt;\u0026lt;29) print(\u0026#34;generated\u0026#34;, file=sys.stderr) n = sys.stdout.buffer.write(rand) # \u0026lt;------------------- changed print(f\u0026#34;written: {n}\u0026#34;, file=sys.stderr) Start a pipeline:\n$ strace --output log.txt -s0 -e write -- python3 ./write2.py | python3 ./slow_reader.py start sleeping 19058 generated This time let’s send 4 signals:\n$ kill -USR1 19058 $ kill -USR1 19058 $ kill -USR1 19058 $ kill -USR1 19058 The output has changed. Now the writer was able to write all the data.\n$ strace --output log.txt -s0 -e write -- python3 ./write2.py | python3 ./slow_reader.py start sleeping 19058 generated signal 10 \u0026lt;frame at 0x7f21a4705a40, file \u0026#39;./write2.py\u0026#39;, line 15, code \u0026lt;module\u0026gt;\u0026gt; signal 10 \u0026lt;frame at 0x7f21a4705a40, file \u0026#39;./write2.py\u0026#39;, line 15, code \u0026lt;module\u0026gt;\u0026gt; signal 10 \u0026lt;frame at 0x7f21a4705a40, file \u0026#39;./write2.py\u0026#39;, line 15, code \u0026lt;module\u0026gt;\u0026gt; signal 10 \u0026lt;frame at 0x7f21a4705a40, file \u0026#39;./write2.py\u0026#39;, line 15, code \u0026lt;module\u0026gt;\u0026gt; stop sleeping written: 536870912 finished reading The bpftrace logs show what we expected. For the first write we see the default pipe buffer size, next we see three ERESTARTSYS errors which reflect our 4 signals, and a final big write with all remaining data.\n$ sudo bpftrace -e \u0026#39;kretfunc:pipe_write /comm == \u0026#34;python3\u0026#34;/ { printf(\u0026#34;%d\\n\u0026#34;, retval);}\u0026#39; 65536 -512 -512 -512 536805376 In strace log we can also see the information on syscall restarts, and it confirms what we saw in bpftrace log.\n$ cat log.txt write(1, \u0026#34;\u0026#34;..., 536870912) = 65536 --- SIGUSR1 {si_signo=SIGUSR1, si_code=SI_USER, si_pid=14994, si_uid=1000} --- write(1, \u0026#34;\u0026#34;..., 536805376) = ? ERESTARTSYS (To be restarted if SA_RESTART is set) --- SIGUSR1 {si_signo=SIGUSR1, si_code=SI_USER, si_pid=14994, si_uid=1000} --- write(1, \u0026#34;\u0026#34;..., 536805376) = ? ERESTARTSYS (To be restarted if SA_RESTART is set) --- SIGUSR1 {si_signo=SIGUSR1, si_code=SI_USER, si_pid=14994, si_uid=1000} --- write(1, \u0026#34;\u0026#34;..., 536805376) = ? ERESTARTSYS (To be restarted if SA_RESTART is set) --- SIGUSR1 {si_signo=SIGUSR1, si_code=SI_USER, si_pid=14994, si_uid=1000} --- write(1, \u0026#34;\u0026#34;..., 536805376) = 536805376 Also, if we sum the returns of the write() syscalls, we’ll get the initial random bytes buffer:\n536805376 + 65536 = 536870912 The first write() restart was done by the python buffered writer due to a partial write of 64KiB, and all other 3 were restarted by the kernel due to the ERESTARTSYS error and the SA_RESTART flag.\nPipe performance: splice(), vmsplice() and tee() # Generally, it uses double buffering when a program makes reads()/writes() calls to a regular file, a socket or a pipe. One buffer is allocated in the user space and then copied to the kernel. Such a situation leads to the loss of performance and undesirable memory allocations. Another potential performance penalty for high-performance tools is the number and duration of system calls for one unit of program iteration. For instance, if we want to replace every 2nd line of a file, we need to read the file in some chunks (1 read() syscall), make changes, and write the changed buffer back (1 write() syscall). These sequences of operations should be while we don\u0026rsquo;t reach the EOF.\nBut luckily, there are 3 syscalls that can significantly improve your code, especially if you are going to use stdin or stdout:\nsplice() – moves data from the buffer to an arbitrary file descriptor, or vice versa, or from one buffer to another (man 2 splice) vmsplice() – \u0026ldquo;copies\u0026rdquo; data from user space into the buffer (man 2 vmsplice) tee() - allocates internal kernel buffer (man 2 tee) The main idea here is what if we use a pipe not as a channel between 2 or more processes but just as an in-kernel ring buffer? Yes, of course, if you need to work with stdin and stdout, you can get a win-win situation because you don’t have to create an artificial pipe and use your real one.\nSo, for example, golang uses pipes (or pool of them) in some zero-copy operations between sockets when the io.Copy()/io.CopyN()/io.CopyBuffer()/io.ReaderFrom() are called.\nSo, the usage:\nr,w = pipe() # allocate a kernel ring buffer of 64KB size. for ;; { n = splice(socket1, w) if n \u0026lt; 0 { break // error, need to check it more carefully here } m = splice(r, socket2) if m \u0026lt; 0 { break // error, need to check it more carefully here } if m \u0026lt; n { // partial write // need to resend the rest of the buffer } } The above code is, of course, a pseudo code and doesn\u0026rsquo;t cover interruption errors and partial writes. But the main idea should be clear.\nIn theory, we can also increase the pipe buffer size, but it depends on the system and the CPU cache size. But in some cases, the bigger buffer might lead to performance degradations. So do your performance tests carefully.\nYou can also use this approach to copy a file to another place. But there is even a better solution – the copy_file_range syscall (man 2 copy_file_range). This one syscall does all the work for copying a file. As I mentioned earlier, fewer syscalls lead to better performance.\nvmsplice() is another beast that could be useful when you want to make changes in the user space and then move this memory to the kernel pipe buffer without copying it. The example:\nr,w = pipe() // allocate a kernel ring buffer of 64KB size. n = read(file, buf) // read data from the file into the user space buffer modify_data(buf) // apply some business logic to the data chunk m = vmsplice(w, buf) // transfer buffer to the kernel buffer The above code is a simplified version of what can be done. But unfortunately, in reality, dealing with vmsplice is complex, and bad documentation doesn\u0026rsquo;t help at all. If you want to go this way, please read the kernel source code first in order to understand all possible problems with vmsplice and zero data coping.\nThe last syscall that we have is tee(). It’s usually used with splice(). You probably know about the tee cli tool. (man 1 tee). The purpose of the util is to copy data from one pipe to another one while duplicating the data to a file. The coreutils implementation of tee uses read() and write() system calls to work with the pipes. But we are going to write our own version with 2 pretty new syscalls: tee() and splice() instead.\nThe tee()system call \u0026ldquo;copies\u0026rdquo; data from one buffer (pipe) to another. In reality, no real coping happens. Under the hood, the kernel just changes buffer references for pipe memory. Thus, tee() syscall does not consume any data, so the subsequent splice() call can get this data from a pipe.\nSo our homebrew implementation could be the following:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;syscall\u0026#34; ) const ( SPLICE_F_MOVE = 1 SPLICE_F_NONBLOCK = 2 ) func main() { file_path := os.Args[1] fmt.Println(\u0026#34;pid:\u0026#34;, os.Getpid()) file, err := os.OpenFile(file_path, os.O_RDWR|os.O_CREATE|os.O_TRUNC, 0755) if err != nil { panic(err) } for { n, err := syscall.Tee(0, 1, 1\u0026lt;\u0026lt;32-1, SPLICE_F_NONBLOCK) if err == syscall.EAGAIN { continue } if err != nil { panic(err) } if n == 0 { break } for n \u0026gt; 0 { slen, err := syscall.Splice(0, nil, int(file.Fd()), nil, int(n), SPLICE_F_MOVE) if err != nil { panic(err) } n -= slen } } } Let’s test it:\n$ cat /var/tmp/file1.db |./tee /tmp/tee.log | strings | grep -E \u0026#34;^ss1\u0026#34; ss1T ss1vg ss1j1; ss1* And verify that the file are identical with md5sum:\n$ md5sum /var/tmp/file1.db 737f4f46feed57b4c6bdde840945948e /var/tmp/file1.db $ md5sum /tmp/tee.log 737f4f46feed57b4c6bdde840945948e /tmp/tee.log And a final note, I would suggest reading the archive of Linus’s emails about splice(), tee() and vmsplice() here https://yarchive.net/comp/linux/splice.html. You can find there a lot of design questions and solution for all these syscalls.\nRead next chapter → "},{"id":11,"href":"/docs/async-rust-tokio-io/3-tokio-io-patterns/","title":"Tokio I/O patterns","section":"Async Rust \u0026 Tokio I/O Streams","content":" Tokio I/O Patterns # Last updated: Oct 2025 Contents\nTCP split stream Split generic AsyncRead+AsyncWrite stream Bidirectional driver for I/O without split Backpressure Cancellation Framed I/O Bidirectional driver for framed I/O Now let\u0026rsquo;s look at the possible solutions and weigh their pros and cons.\nTCP split stream # The first obvious solution, suggested by the documentation, is to split the TCP stream into two parts: a reader and a writer.\nThe Tokio TcpStream API provides split() and into_split(). They differ in ownership and allocations. Because we stay in one Tokio task, the borrowing split() variant is cheaper.\nlet mut stream = TcpStream::connect(\u0026amp;addr).await?; let (mut r, mut w) = stream.split(); let reader = async { loop { let n = r.read(\u0026amp;mut read_buf).await?; if n == 0 { // EOF - server closed connection eprintln!(\u0026#34;Server closed the connection.\u0026#34;); break; } print!(\u0026#34;{}\u0026#34;, String::from_utf8_lossy(\u0026amp;read_buf[..n])); } Ok::\u0026lt;_, io::Error\u0026gt;(()) }; let writer = async { while let Some(msg) = rx.recv().await { w.write_all(\u0026amp;msg).await?; w.flush().await?; println!(\u0026#34;client\u0026#39;s written\u0026#34;); } Ok::\u0026lt;_, io::Error\u0026gt;(()) }; tokio::try_join!(reader, writer)?; Ok(()) The code creates two futures: reader and writer, and runs them concurrently with try_join!.\nBackpressure on writes occurs naturally in a blocking manner. Each message is first read from rx channel, then written in its own child future, so the reader future itself remains unblocked by it.\nLet\u0026rsquo;s add cancellation that works even under backpressure without contorting the existing futures.\nWe can leverage try_join!\u0026rsquo;s short-circuiting by introducing a dedicated cancellation future. If the cancel future errors, try_join! returns immediately and the other tasks stop:\ntokio::try_join!(reader, writer, cancel)?; where cancel is:\nlet cancel = { async move { tokio::select! { _ = cancel_token.cancelled() =\u0026gt; Err(cancel_err), _ = finish_rx.recv() =\u0026gt; Ok(()), } } }; and where:\ncancel_token – is Tokio CancellationToken useful for signalling a cancellation request to one or more tasks. finish_rx – is a way to exit the cancel future without short-circuiting the try_join!. The reader and the writer need to close (drop) their parts of the finish_tx channel on exit (no matter on success or on error).\nlet (finish_tx, mut finish_rx) = mpsc::channel::\u0026lt;()\u0026gt;(1); let _guard = finish_tx.clone(); let reader = async move { let _guard = _guard; loop { let n = r.read(\u0026amp;mut read_buf).await?; if n == 0 { // EOF - server closed connection eprintln!(\u0026#34;Server closed the connection.\u0026#34;); break; } print!(\u0026#34;{}\u0026#34;, String::from_utf8_lossy(\u0026amp;read_buf[..n])); } Ok::\u0026lt;_, io::Error\u0026gt;(()) }; let _guard = finish_tx.clone(); let writer = async { let _guard = _guard; while let Some(msg) = rx.recv().await { w.write_all(\u0026amp;msg).await?; w.flush().await?; } Ok::\u0026lt;_, io::Error\u0026gt;(()) }; drop(finish_tx); // drop the origin The full code could be found on github\nThe above code works, but before pushing it to prod, consider the following improvement and limitations:\nThink about Cancellation safety. It\u0026rsquo;s not a simple topic overall, but in our case the message dequeued from the rx channel might be lost during cancelation in backpressure situation. More info here and a lot of examples here. On cancellation or write error – drain the reader buffer until an error or EOF. This can be useful to: Continue reading the response on a best-effort basis. For example, you might try to read the HTTP header from the buffer for debugging or to forward a truncated response. Reuse the underlying TCP/TLS connection. Trigger custom cleanup logic when cancellation occurs. I leave this refinement to the reader.\nSplit generic AsyncRead+AsyncWrite stream # The split approach works for TcpStream and other basic types from tokio::io. But the real applications ususally favor generics instead: most adapters accept AsyncRead+AsyncWrite so they compose well. For instance:\nasync fn handle\u0026lt;IO: AsyncRead + AsyncWrite\u0026gt;(stream: IO) -\u0026gt; io::Result\u0026lt;()\u0026gt; { ... } You can still split such a stream with tokio::io::split(). The example becomes:\nasync fn handle\u0026lt;IO: AsyncRead + AsyncWrite + Unpin\u0026gt;( stream: IO, mut rx: mpsc::Receiver\u0026lt;Bytes\u0026gt;, ) -\u0026gt; io::Result\u0026lt;()\u0026gt; { let mut read_buf = vec![0u8; 8192]; let (mut r, mut w) = tokio::io::split(stream); let reader = async { loop { let n = r.read(\u0026amp;mut read_buf).await?; if n == 0 { // EOF - server closed connection eprintln!(\u0026#34;Server closed the connection.\u0026#34;); break; } print!(\u0026#34;{}\u0026#34;, String::from_utf8_lossy(\u0026amp;read_buf[..n])); } Ok::\u0026lt;_, io::Error\u0026gt;(()) }; let writer = async { while let Some(msg) = rx.recv().await { w.write_all(\u0026amp;msg).await?; w.flush().await?; println!(\u0026#34;client\u0026#39;s written\u0026#34;); } Ok::\u0026lt;_, io::Error\u0026gt;(()) }; tokio::try_join!(reader, writer)?; Ok(()) } It looks almost the same (omitting cancellation). But there is one caveat: tokio::io::split() uses an internal mutex. However if we try to evaluate the possible contention of this mutex, we can see that, as long as both halves remain in the same task (i.e., you don\u0026rsquo;t spawn()), contention is negligible and the overhead remains minimal.\nThe cancellation case mirrors the TCP-specific version.\nBidirectional driver for I/O without split # One important caveat of tokio::io::split(): splitting is not always safe for every stream implementation. The full story is covered in this GitHub issue, but the TL;DR is:\nSome streams (AsyncRead+AsyncWrite), including TLS transports, do not keep reads and writes independent. A read may need to trigger a write (alerts, key updates, etc.), and vice versa.\nBecause of that coupling, simply splitting into separate read and write tasks can lead to deadlocks, \u0026ldquo;stuck\u0026rdquo; futures, or missed wakeups (i.e. the read half might block waiting on a write event that never gets woken, etc.).\nEven though this issue is specific to Tokio tasks and does not affect concurrent child futures (there is a single waker per task, so concurrent readers and writers are fine), you might still want to build a lock-free, fully controlled, and highly performant communication layer. In such cases, using io::split() can be unnecessary, or even overkill. For example, the h2 HTTP/2 and the Hyper HTTP/1 crates do not rely on io::split() at all, and instead, they adopt a connection-driver approach.\nThe connection-driver approach (sometimes called a connection task or connection future) is a pattern where instead of splitting AsyncRead/AsyncWrite and work with its halves, the single Connection future is created that drives the entire protocol state machine. It\u0026rsquo;s called a driver because polling it drives the connection forward.\nLet\u0026rsquo;s write our own concurrent connection-driver.\nFirst, remember that if we keep the stream\u0026rsquo;s read and write halves in one task, the I/O is multiplexed on readiness and run one after another. This simple model works well for I/O-bound workloads where the time between events is much longer than the processing time.\nSecond, we can\u0026rsquo;t directly use concurrent helpers because the stream implements both AsyncRead and AsyncWrite, and borrowing it twice would violate Rust\u0026rsquo;s rules. So instead, we need to implement a custom made future that owns the I/O, makes reads/writes manually, and proxies messages to/from an actor-like interface.\nTokio\u0026rsquo;s tokio::io::copy_bidirectional is a great example: it copies between two streams by juggling buffers and states, preserving backpressure: no mutexes or helper macros are used, just raw async primitives.\nAnother example is the linkerd2 proxy\u0026rsquo;s duplex copy, which follows similar design ideas.\nThus let\u0026rsquo;s reproduce the logic for our own use case with one additional feature. So the recap of our driver is:\nThe driver acts as an actor around an I/O stream. It should provide a channel to feed outbound messages and another channel to collect ready-to-consume reads. As a bonus, expose a write barrier command that guarantees all previous writes are flushed. The user API:\nlet stream = TcpStream::connect(\u0026#34;127.0.0.1:8080\u0026#34;).await?; let stream = BufStream::new(stream); let (conn, tx, mut rx) = Connection::new(stream)?; tokio::spawn(async move { if let Err(err) = conn.await { panic!(\u0026#34;err: {err}\u0026#34;); } }); tx.send(message).await.unwrap(); while let Some(buf) = rx.recv().await { println!(\u0026#34;recv from server: {:?}\u0026#34;, buf); } where the message is a tuple of a Bytes buffer and an optional callback – to trigger a flush call:\ntype Ack = oneshot::Sender\u0026lt;()\u0026gt;; type Outgoing = (Bytes, Option\u0026lt;Ack\u0026gt;); ... let (ack_tx, ack_rx) = oneshot::channel(); message: Outgoing = (Bytes::from_static(b\u0026#34;message\u0026#34;), Some(ack_tx)); tx.send(message).await.unwrap(); _ = ack_rx.await.unwrap(); // \u0026lt;--------- barrier ... The full code can be found on github. But here I just want to show some important parts.\nThe main structure of the Connection with buffers and states:\npub struct Connection\u0026lt;T\u0026gt; { stream: T, read_buf: BytesMut, // reusable buffer for read IO calls // read from IO inbound_buf: Option\u0026lt;Bytes\u0026gt;, tx: PollSender\u0026lt;Bytes\u0026gt;, // write to IO outbound_buf: Option\u0026lt;Outgoing\u0026gt;, rx: mpsc::Receiver\u0026lt;Outgoing\u0026gt;, read_state: ReadState, write_state: WriteState, } The core of the driver lies in its implementation of the Future trait:\nimpl\u0026lt;T\u0026gt; Future for Connection\u0026lt;T\u0026gt; where T: AsyncRead + AsyncWrite + Unpin, { type Output = Result\u0026lt;(), io::Error\u0026gt;; fn poll(mut self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Self::Output\u0026gt; { if matches!(self.read_state, ReadState::Reading) { _ = self.poll_read(cx)?; } if matches!(self.write_state, WriteState::Writing) { _ = self.poll_write(cx)?; } if matches!(self.write_state, WriteState::Flushing(_)) { _ = self.poll_flush(cx)?; } if matches!(self.write_state, WriteState::ShuttingDown) { _ = self.poll_shutdown(cx)?; } if matches!(self.read_state, ReadState::Done) \u0026amp;\u0026amp; matches!(self.write_state, WriteState::Done) { return Poll::Ready(Ok(())); } Poll::Pending } } The future sequentially checks readiness and performs reads and writes on every wake-up. If either side returns Poll::Pending, the driver continues to the next step instead of yielding immediately. The first error stops the driver.\nThe write side runs in three phases: read from the channel of available to write buffers and write them to I/O stream, the flush, then the shutdown. The dedicated Flushing state lets us implement write barriers: it forces an I/O flush and runs a callback once the flush completes.\nBackpressure # We need to apply backpressure to both sides of the driver communication:\nRead side only pulls from the socket when the upstream channel has capacity. If it\u0026rsquo;s full, reading stops and the kernel TCP window naturally pushes back on the peer. Write side only pulls the next message after the current one is fully written (and optionally flushed + acked). If the socket can\u0026rsquo;t accept bytes, writing yields with the remaining bytes kept in place. Read backpressure: \u0026ldquo;don\u0026rsquo;t read unless you can forward\u0026rdquo; # Key idea: buffer at most one message (inbound_buf) until the mpsc receiver tx reserves a slot. If the channel is full, we yield and stop reading.\nfn poll_read(\u0026amp;mut self, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;io::Result\u0026lt;()\u0026gt;\u0026gt; { loop { // 1. If we already have a message to forward, try to reserve capacity upstream. if self.inbound_buf.is_some() { ready!( self.tx .poll_reserve(cx) .map_err(|_| io::Error::new(io::ErrorKind::BrokenPipe, \u0026#34;Channel closed\u0026#34;))? ); // Capacity is guaranteed now; forward the buffered chunk. self.tx .send_item(self.inbound_buf.take().expect(\u0026#34;buffered message missing\u0026#34;)) .map_err(|_| io::Error::new(io::ErrorKind::BrokenPipe, \u0026#34;Channel closed\u0026#34;))?; } // 2. Ensure read buffer has enough capacity, then read from the socket. if self.read_buf.capacity() \u0026lt; READ_BUF_CAPACITY { self.read_buf.reserve(READ_BUF_CAPACITY); } let n = ready!(tokio_util::io::poll_read_buf( Pin::new(\u0026amp;mut self.stream), cx, \u0026amp;mut self.read_buf ))?; // 3. EOF, finish reads \u0026amp; close upstream channel. if n == 0 { self.read_state = ReadState::Done; self.tx.close(); return Poll::Ready(Ok(())); } // 4. Move the just-read bytes into a message to forward next loop iteration. self.inbound_buf = Some(self.read_buf.split().freeze()); } } Write backpressure: \u0026ldquo;finish what you started, then maybe flush \u0026amp; ack\u0026rdquo; # Key idea: keep the current (Bytes, Option\u0026lt;Ack\u0026gt;) in outbound_buf. Don\u0026rsquo;t pull the next message until this one is completely written (and optionally flushed + acked). If the socket can\u0026rsquo;t take bytes now, yield with progress saved.\nfn poll_write(\u0026amp;mut self, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;io::Result\u0026lt;()\u0026gt;\u0026gt; { loop { // 1. If nothing buffered, try to pull from the bounded mpsc. if self.outbound_buf.is_none() { match self.rx.poll_recv(cx) { Poll::Ready(Some(msg)) =\u0026gt; self.outbound_buf = Some(msg), Poll::Ready(None) =\u0026gt; { // sender closed, finish by flushing then shutdown self.write_state.set_flushing(None, FromFlushingTo::ShuttingDown); return Poll::Ready(Ok(())); } Poll::Pending =\u0026gt; { // nothing to send right now, make sure we flush kernel/TLS buffers self.write_state.set_flushing(None, FromFlushingTo::Writing); return Poll::Pending; } } } // 2. Drain the current buffer into the socket, handling partial writes. if let Some((buf, ack)) = self.outbound_buf.as_mut() { while !buf.is_empty() { match ready!(Pin::new(\u0026amp;mut self.stream).poll_write(cx, buf)?) { 0 =\u0026gt; return Poll::Ready(Err(io::Error::new(io::ErrorKind::WriteZero, \u0026#34;write zero bytes\u0026#34;))), n =\u0026gt; buf.advance(n), } } // 3. Optional message-level completion: flush, then send ack. if let Some(ack) = ack.take() { self.write_state.set_flushing(Some(ack), FromFlushingTo::Writing); cx.waker().wake_by_ref(); // the Poll::Pending is returning without installed waker, so wake manually return Poll::Pending; } // 4. Fully written try the next message. self.outbound_buf = None; } } } Cancellation # A short-circuit cancellation is pretty straightforward in this case:\nlet cancel = CancellationToken::new(); tokio::spawn(async move { tokio::select! { Err(err) = conn =\u0026gt; panic!(\u0026#34;err: {err}\u0026#34;), _ = cancel.cancelled() =\u0026gt; return, else =\u0026gt; return } }); Improvements # There are several improvements to consider and play with:\nOnce again about cancellation safety: understand how critical it is for you to lose buffered data. Improve read-buffer allocation and copying; you can tune strategies based on expected frame sizes. Randomize the order of read and write calls to avoid starving one side. A simple round-robin toggle is often enough. On write errors, drain reads before returning – just like the earlier split example. The channel logic uses a single buffer: Try to use poll_recv_many() to get more at once instead of poll_recv(). Try to use try_reserve_many() in the sender part. Or as an alternative for the above, change the channel to work with multiple buffers instead of one: let (tx, rx) = mpsc::channel::\u0026lt;(Vec\u0026lt;Bytes\u0026gt;, Ack)\u0026gt;(128) Framed I/O # So far we\u0026rsquo;ve passed raw Bytes through the I/O. But usually it\u0026rsquo;s not what we do in real apps where we work with structures. In practice, streams decode bytes into domain-specific frames/messages/structures and encode frames back to bytes. The encoder/decoder pair is usually called a codec. Tokio provides helpful traits and utilities in tokio_util::codec crate to build composable framed APIs.\nWe can wrap an I/O stream with Framed::new(). If the I/O implements AsyncRead+AsyncWrite, the resulting framed stream also offers split() (from StreamExt) because it implements both Stream and Sink. Internally this behaves like io::split, but uses a specialized BiLock optimized for two owners mutex lock.\nHere\u0026rsquo;s how it fits together when we use length prefix codec for meesages:\nasync fn handle\u0026lt;IO: AsyncRead + AsyncWrite + Unpin\u0026gt;( stream: IO, rx: mpsc::Receiver\u0026lt;Bytes\u0026gt;, ) -\u0026gt; io::Result\u0026lt;()\u0026gt; { let stream = Framed::new(stream, LengthDelimitedCodec::new()); // https://docs.rs/futures/latest/futures/lock/struct.BiLock.html let (writer, reader) = stream.split(); let reader = reader.try_for_each(|msg| async move { print!(\u0026#34;{}\u0026#34;, String::from_utf8_lossy(\u0026amp;msg)); Ok(()) }); let writer = ReceiverStream::new(rx).map(Ok).forward(writer); tokio::try_join!(reader, writer)?; Ok(()) } This mirrors the earlier io::split() example but gains the convenience of stream adapters.\nLook how elegant, composable, and compatible the code looks.\nThe full code can be found on github.\n3.5 Bidirectional driver for framed I/O # Finally, we can upgrade the bytes-based driver to a framed one.\nThe client API changed a bit:\nlet (conn, tx, mut rx) = ConnectionFramed::new(stream, LengthDelimitedCodec::new())?; tokio::spawn(async move { if let Err(err) = conn.await { panic!(\u0026#34;err: {err}\u0026#34;); } }); It is almost identical to the bytes driver, except that it works with encoder/decoder generics and generic inbound/outbound types.\n#[pin_project] pub struct ConnectionFramed\u0026lt;T, C, In, Out\u0026gt; where T: AsyncRead + AsyncWrite + Unpin, C: Decoder\u0026lt;Item = In, Error = io::Error\u0026gt; + Encoder\u0026lt;Out, Error = io::Error\u0026gt;, In: Send + fmt::Debug, Out: Send + fmt::Debug, { #[pin] stream: Framed\u0026lt;T, C\u0026gt;, // read from IO inbound_buf: Option\u0026lt;In\u0026gt;, tx: PollSender\u0026lt;In\u0026gt;, // write to IO outbound_buf: Option\u0026lt;Outgoing\u0026lt;Out\u0026gt;\u0026gt;, rx: mpsc::Receiver\u0026lt;Outgoing\u0026lt;Out\u0026gt;\u0026gt;, read_state: ReadState, write_state: WriteState, } Everything else follows the same pattern.\nThe full code you can find here\nPutting it together # Tokio offers several levels of control. Splitting a stream is often sufficient for simple TCP code, especially when paired with try_join! for simple cancellation. Generic I/O benefits from the same pattern via tokio::io::split, while custom drivers provide fine-grained control when splitting is unsafe or when you need customization and additional control. Choose the lightest abstraction that preserves backpressure and cancellation for your use case, and reach for a dedicated driver only when you need the extra guarantees.\n"},{"id":12,"href":"/docs/page-cache/2-essential-page-cache-theory/","title":"Essential Linux Page Cache theory","section":"Linux Page Cache series","content":" Essential Page Cache theory # Last updated: Oct 2025 Contents\nRead requests Write requests First of all, let’s start with a bunch of reasonable questions about Page Cache:\nWhat is the Linux Page Cache? What problems does it solve? Why do we call it «Page» Cache ? In essence, the Page Cache is a part of the Virtual File System (VFS) whose primary purpose, as you can guess, is improving the IO latency of read and write operations. A write-back cache algorithm is a core building block of the Page Cache.\nNOTE\nIf you\u0026rsquo;re curious about the write-back algorithm (and you should be), it\u0026rsquo;s well described on Wikipedia, and I encourage you to read it or at least look at the figure with a flow chart and its main operations.\n\u0026ldquo;Page\u0026rdquo; in the Page Cache means that linux kernel works with memory units called pages. It would be cumbersome and hard to track and manage bites or even bits of information. So instead, Linux\u0026rsquo;s approach (and not only Linux\u0026rsquo;s, by the way) is to use pages (usually 4K in length) in almost all structures and operations. Hence the minimal unit of storage in Page Cache is a page, and it doesn\u0026rsquo;t matter how much data you want to read or write. All file IO requests are aligned to some number of pages.\nThe above leads to the important fact that if your write is smaller than the page size, the kernel will read the entire page before your write can be finished.\nThe following figure shows a bird\u0026rsquo;s-eye view of the essential Page Cache operations. I broke them down into reads and writes.\nFigure 1. – Linux Page Cache (pagecache) reads and writes As you can see, all data reads and writes go through Page Cache. However, there are some exceptions for Direct IO (DIO), and I\u0026rsquo;m talking about it at the end of the series. For now, we should ignore them.\nNOTE\nIn the following chapters, I\u0026rsquo;m talking about read(), write(), mmap() and other syscalls. And I need to say, that some programming languages (for example, Python) have file functions with the same names. However, these functions don\u0026rsquo;t map exactly to the corresponding system calls. Such functions usually perform buffered IO. Please, keep this in mind.\nRead requests # Generally speaking, reads are handled by the kernel in the following way:\n① – When a user-space application wants to read data from disks, it asks the kernel for data using special system calls such as read(), pread(), vread(), mmap(), sendfile(), etc.\n② – Linux kernel, in turn, checks whether the pages are present in Page Cache and immediately returns them to the caller if so. As you can see kernel has made 0 disk operations in this case.\n③ – If there are no such pages in Page Cache, the kernel must load them from disks. In order to do that, it has to find a place in Page Cache for the requested pages. A memory reclaim process must be performed if there is no free memory (in the caller\u0026rsquo;s cgroup or system). Afterward, kernel schedules a read disk IO operation, stores the target pages in the memory, and finally returns the requested data from Page Cache to the target process. Starting from this moment, any future requests to read this part of the file (no matter from which process or cgroup) will be handled by Page Cache without any disk IOP until these pages have not been evicted.\nWrite requests # Let\u0026rsquo;s repeat a step-by-step process for writes:\n(Ⅰ) – When a user-space program wants to write some data to disks, it also uses a bunch of syscalls, for instance: write(), pwrite(), writev(), mmap(), etc. The one big difference from the reads is that writes are usually faster because real disk IO operations are not performed immediately. However, this is correct only if the system or a cgroup doesn\u0026rsquo;t have memory pressure issues and there are enough free pages (we will talk about the eviction process later). So usually, the kernel just updates pages in Page Cache. it makes the write pipeline asynchronous in nature. The caller doesn’t know when the actual page flush occurs, but it does know that the subsequent reads will return the latest data. Page Cache protects data consistency across all processes and cgroups. Such pages, that contain un-flushed data have a special name: dirty pages.\n(II) – If a process\u0026rsquo; data is not critical, it can lean on the kernel and its flush process, which eventually persists data to a physical disk. But if you develop a database management system (for instance, for money transactions), you need write guarantees in order to protect your records from a sudden blackout. For such situations, Linux provides fsync(), fdatasync() and msync() syscalls which block until all dirty pages of the file get committed to disks. There are also open() flags: O_SYNC and O_DSYNC, which you also can use in order to make all file write operations durable by default. I\u0026rsquo;m showing some examples of this logic later.\nRead next chapter → "},{"id":13,"href":"/docs/resolver-dual-stack-application/3-getaddrinfo-and-posix-spec/","title":"getaddrinfo() and POSIX spec","section":"DNS resolvers and Dual-Stack applications","content":" 3. getaddrinfo() and POSIX spec # Last updated: Oct 2025 Contents\n3.1 Resolving hostname (node) 3.2 Resolving ports (services) Thus, instead of the deprecated gethostbyname(), getaddrinfo() should be used within libc. The getaddrinfo() function is a POSIX-standardized function and is defined in RFC 3943. It is IP version agnostic and returns data structures that can be easily reused in subsequent socket API calls (such as socket(), connect(), sendto()).\nFirst of all, if you have a codebase that uses gethostbyname() and you are looking to migrate to the modern getaddrinfo(), I have bad news: it’s not a drop-in replacement. You need to understand the new data structures, logic and flags.\nLet’s now take a closer look at its parameters and ways to call them, and understand why it is far superior to its predecessor.\n3.1 Resolving hostname (node) # We begin with a simple, typical example of client code that resolves a hostname into IP addresses:\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;netdb.h\u0026gt; #include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; void print_ip_addresses(const char *hostname) { struct addrinfo hints, *res, *p; int status; char ipstr[INET6_ADDRSTRLEN]; memset(\u0026amp;hints, 0, sizeof hints); // \u0026lt;--- ① hints.ai_family = AF_UNSPEC; // \u0026lt;--- ② hints.ai_socktype = SOCK_STREAM; // \u0026lt;--- ③ if ((status = getaddrinfo(hostname, /* ---\u0026gt; ④ \u0026lt;--- */ NULL, \u0026amp;hints, \u0026amp;res)) != 0) { fprintf(stderr, \u0026#34;getaddrinfo: %s\\n\u0026#34;, gai_strerror(status)); return; } printf(\u0026#34;IP addresses for %s:\\n\\n\u0026#34;, hostname); for(p = res; p != NULL; p = p-\u0026gt;ai_next) { // \u0026lt;--- ⑤ void *addr; char *ipver; if (p-\u0026gt;ai_family == AF_INET) { // IPv4 struct sockaddr_in *ipv4 = (struct sockaddr_in *)p-\u0026gt;ai_addr; addr = \u0026amp;(ipv4-\u0026gt;sin_addr); ipver = \u0026#34;IPv4\u0026#34;; } else { // IPv6 struct sockaddr_in6 *ipv6 = (struct sockaddr_in6 *)p-\u0026gt;ai_addr; addr = \u0026amp;(ipv6-\u0026gt;sin6_addr); ipver = \u0026#34;IPv6\u0026#34;; } // Convert the IP to a string and print it: inet_ntop(p-\u0026gt;ai_family, addr, ipstr, sizeof ipstr); printf(\u0026#34; %s: %s\\n\u0026#34;, ipver, ipstr); } freeaddrinfo(res); // free the linked list } int main(int argc, char *argv[]) { if (argc != 2) { fprintf(stderr, \u0026#34;usage: %s hostname\\n\u0026#34;, argv[0]); return 1; } print_ip_addresses(argv[1]); return 0; } ① – the hints parameter is the primary place to control the behavior of getaddrinfo(). We will be experimenting with it extensively.\nThe memset() function sets all fields to 0, which means none of the members are NULL and, as a result, no default values from getaddrinfo() will be used. ② – the address family is set to AF_UNSPEC, which means it returns all existing addresses, including both IPv6 and IPv4.\n④ – the service field (essentially the port) is set to NULL because, for a simpler resolver, we are not intending to connect to the remote host.\n③ – here, we set the hints.ai_socktype to SOCK_STREAM because, as I mentioned earlier, the main purpose of getaddrinfo() is to work with socket API functions and prepare data structures for future use. In our case, when we only need IP addresses, the socket type we set here doesn’t really matter. However, we don’t want to set it to NULL. If we did, a getaddrinfo() call would return three times more addresses due to its internal logic, preparing sockets for TCP, UDP, and RAW connections.\n⑤ – the result of the call is a linked list of addrinfo data structures, ready to be used in subsequent socket API calls such as socket (man 2 socket), connect (man 2 connect), bind (man 2 bind), etc.\nCompile it with glibc:\n$ sudo apt-get install gcc $ gcc -o getaddrinfo ./getaddrinfo.c or with musl libc:\n$ sudo apt install musl musl-tools $ musl-gcc -o getaddrinfo ./getaddrinfo.c and run:\n$ ./getaddrinfo microsoft.com IP addresses for microsoft.com: IPv4: 20.76.201.171 IPv4: 20.112.250.133 IPv4: 20.231.239.246 IPv4: 20.70.246.20 IPv4: 20.236.44.162 IPv6: 2603:1020:201:10::10f IPv6: 2603:1030:c02:8::14 IPv6: 2603:1030:b:3::152 IPv6: 2603:1030:20e:3::23c IPv6: 2603:1010:3:3::5b Please note that if you have a global scope IPv6 address on your machine, IPv6 addresses will be shown first. This is due to the default behavior described in RFC 6724, which we will discuss later.\nIf you don’t have an IPv6 address but want to experiment with different resolver logic for dual-stack applications, we can work around this by assigning a random global scope IPv6 address to one of the interfaces (not the loopback interface):\n$ sudo ip a add 2001:db8:123:456:6af2:68fe:ff7c:e25c dev eth0 When you want to delete it to rollback to the IPv4 only global scope addresses:\n$ sudo ip a del 2001:db8:123:456:6af2:68fe:ff7c:e25c dev eth0 For the purposes of our experiments with stub resolvers, it’s acceptable that this address will be without proper network routing.\nGenerally speaking, there are several reasons why getaddrinfo() returns a linked list. According to the documentation from man 3 gettaddrinfo:\nThere are several reasons why the linked list may have more than one addrinfo structure, including: the network host is multihomed, accessible over multiple protocols (e.g., both AF_INET and AF_INET6); or the same service is available from multiple socket types (one SOCK_STREAM address and another SOCK_DGRAM address, for example). Normally, the application should try using the addresses in the order in which they are returned. The sorting function used within getaddrinfo() is defined in RFC 3484; the order can be tweaked for a particular system by editing /etc/gai.conf (available since glibc 2.5)\n3.2 Resolving ports (services) # An interesting aspect of getaddrinfo() is that it also can resolve service names to ports using the /etc/services file. For example, if we change the above example code by:\nadding a \u0026ldquo;domain\u0026rdquo; service (it’s a DNS port 53); removing the line ③ with hints.ai_socktype = SOCK_STREAM. status = getaddrinfo(hostname, \u0026#34;domain\u0026#34;, \u0026amp;hints, \u0026amp;res) we will get every IP address twice in the output. The reason is the /etc/services for domain service contains ports for UDP and TCP protocols, hence getaddrinfo() prepares two sockets of each protocol for every address returned from a nameserver:\n$ grep domain /etc/services domain 53/tcp # Domain Name Server domain 53/udp Read next chapter → "},{"id":14,"href":"/docs/fd-pipe-session-terminal/3-process-groups-jobs-and-sessions/","title":"Process groups, jobs and sessions","section":"GNU/Linux shell related internals","content":" Process groups, jobs and sessions # Last updated: Oct 2025 Contents\nProcess groups Sessions Controlling terminal, controlling process, foreground and background process groups Shell job control kill command Terminating shell nohup and disown Daemons A new process group is created every time we execute a command or a pipeline of commands in a shell. Inside a shell, a process group is usually called a job. In its turn, each process group belongs to a session. Linux kernel provides a two-level hierarchy for all running processes (look at figure 3 below). As such, a process group is a set of processes, and a session is a set of related process groups. Another important limitation is that a process group and its members can be members of a single session.\n$ sleep 100 # a process group with 1 process $ cat /var/log/nginx.log | grep string | head # a process group with 3 processes Process groups # A process group has its process group identificator PGID and a leader who created this group. The PID of the group leader is equal to the corresponding PGID. As so, the type of PID and PGID are the same, and is (pid_t)[https://ftp.gnu.org/old-gnu/Manuals/glibc-2.2.3/html_node/libc_554.html]. All new processes created by the group members inherit the PGID and become the process group members. In order to create a group, we have setpgid() and setpgrp() syscalls (man 2 getpgrp()).\nA process group lives as long as it has at least one member. It means that even if the group leader terminates, the process group is valid and continues carrying out its duties. A process can leave its process group by:\njoining another group; creating its own new group; terminating. Linux kernel can reuse PIDs for new processes if only the process group with that PGID doesn’t have members. It secures a valid hierarchy of processes.\nTwo interesting features of process groups are:\na parent process can wait() for its children using the process group id; a signal can be sent to all members of a process group by using killpg() or kill() with a negative PGID parameter. The below command sends a SIGTERM(15) to all members of the process group 123:\n$ kill -15 -123 The following 2 scripts demonstrate this feature. We have 2 long-running scripts in a process group (it was created for us automatically by shell) connected by a pipe.\nprint.py import signal import os import sys import time def signal_handler(signum, frame): print(f\u0026#34;[print] signal number: {signum}\u0026#34;, file=sys.stderr) os._exit(signum) signal.signal(signal.SIGTERM, signal_handler) print(f\u0026#34;PGID: {os.getpgrp()}\u0026#34;, file=sys.stderr) for i in range(9999): print(f\u0026#34;{i}\u0026#34;) sys.stdout.flush() time.sleep(1) and\nstdin.py import fileinput import signal import os import sys def signal_handler(signum, frame): print(f\u0026#34;[stdin] signal number: {signum}\u0026#34;, file=sys.stderr) os._exit(signum) signal.signal(signal.SIGTERM, signal_handler) for i, line in enumerate(fileinput.input()): print(f\u0026#34;{i+1}: {line.rstrip()}\u0026#34;) Start the pipeline, and in the middle of the execution, run a kill command in a new terminal window.\n$ python3 ./print.py | python3 ./stdin.py PGID: 9743 1: 0 2: 1 3: 2 4: 3 [stdin] signal number: 15 [print] signal number: 15 And kill it by specifying the PGID:\n$ kill -15 -9743 Sessions # For its part, a session is a collection of process groups. All members of a session identify themselves by the identical SID. It’s also the pid_t type, and as a process group, also inherited from the session leader, which created the session. All processes in the session share a single controlling terminal (we’ll talk about this later).\nA new process inherits its parent’s session ID. In order to start a new session a process should call setsid() (man 2 setsid). The process running this syscall begins a new session, becomes its leader, starts a new process group, and becomes its leader too. SID and PGID are set to the process’ PID. That’s why the process group leader can’t start a new session: the process group could have members, and all these members must be in the same session.\nBasically, a new session is created in two cases:\nWhen we need to log in a user with an interactive shell. A shell process becomes a session leader with a controlling terminal (about this later). A daemon starts and wants to run in its own session in order to secure itself (we will touch daemons in more detail later). The following image shows a relationship between a session, its process groups and processes.\nFigure 3. – 2 level hierarchy of processes ❶ – Session id (SID) is the same as the session leader process (bash) PID. ❷ – The session leader process (bash) has its own process group, where it’s a leader, so PGID is the same as its PID. ❸, ❹ – The session has 2 more process groups with PGIDs 200 and 300. ❺, ❻ – Only one group can be a foreground for a terminal. All other process groups are background. We will touch on these terms in a minute. ❼, ❽, ❾ – All members of a session share a pseudoterminal /dev/pts/0. In order to get all the above information for a running process, we can read the /proc/$PID/stat file. For example, for my running bash shell $$ porcess:\n$ cat /proc/$$/stat | cut -d \u0026#34; \u0026#34; -f 1,4,5,6,7,8 | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | paste \u0026lt;(echo -ne \u0026#34;pid\\nppid\\npgid\\nsid\\ntty\\ntpgid\\n\u0026#34;) - pid 8415 # PID ppid\t8414 # parent PID pgid\t8415 # process group ID sid 8415 # sessions ID tty 34816 # tty number tpgid\t9348 # foreground process group ID where (man 5 procfs https://man7.org/linux/man-pages/man5/proc.5.html):\npid – the process id. ppid – the PID of the parent of this process. pgrp – the process group id of the process. sid – the session id of the process. tty – the controlling terminal of the process. (The minor device number is contained in the combination of bits 31 to 20 and 7 to 0; the major device number is in bits 15 to 8.) tpgid – the id of the foreground process group of the controlling terminal of the process. Controlling terminal, controlling process, foreground and background process groups # A controlling terminal is a terminal (tty, pty, console, etc) that controls a session. There may not be a controlling terminal for a session. It is usual for daemons.\nIn order to create a controlling terminal, at first, the session leader (usually a shell process) starts a new session with setsid(). This action drops a previously available terminal if it exists. Then the process needs to open a terminal device. On this first open() call, the target terminal becomes the controlling terminal for the session. From this point in time, all existing processes in the session are able to use the terminal too. The controlling terminal is inherited by fork() call and preserved by execve(). A particular terminal can be the controlling terminal only for one session.\nA controlling terminal sets 2 important definitions: a foreground process group and a background process group. At any moment, there can be only one foreground process group for the session and any number of background ones. Only processes in the foreground process group can read from the controlling terminal. On the other hand, writes are allowed from any process by default. There are some tricks with terminals, we touch them later, when we will talk solely about terminals.\nA terminal user can type special signal-generating terminal characters on the controlling terminal. The most famous ones are CTRL+C and CTRL+Z. As its name suggests, a corresponding signal is sent to the foreground process group. By default, the CTRL+C triggers a SIGINT signal, and CTRL+Z a SIGTSTP signal.\nAlso, opening the controlling terminal makes the session leader the controlling process of the terminal. Starting from this moment, if a terminal disconnection occurs, the kernel will send a SIGHUP signal to the session leader (usually a shell process).\nThe tcsetpgrp() (man 3 tcsetpgrp) is a libc function to promote a process group to the foreground group of the controlling terminal. There is also the tcgetpgrp() function to get the current foreground group. These functions are used primarily by shells in order to control jobs. On linux, we can also use ioctl() with TIOCGPGRP and TIOCSPGRP operations to get and set the foreground group.\nLet\u0026rsquo;s write a script that emulates the shell logic of creating a process group for a pipeline.\npg.py import os print(f\u0026#34;parent: {os.getpid()}\u0026#34;) pgpid = os.fork() # ⓵ if not pgpid: # child os.setpgid(os.getpid(), os.getpid()) # ⓶ os.execve(\u0026#34;./sleep.py\u0026#34;, [\u0026#34;./sleep.py\u0026#34;, ], os.environ) print(f\u0026#34;pgid: {pgpid}\u0026#34;) pid = os.fork() if not pid: # child os.setpgid(os.getpid(), pgpid) # ⓷ os.execve(\u0026#34;./sleep.py\u0026#34;, [\u0026#34;./sleep.py\u0026#34;, ], os.environ) pid = os.fork() if not pid: # child os.setpgid(os.getpid(), pgpid) # ⓷ os.execve(\u0026#34;./sleep.py\u0026#34;, [\u0026#34;./sleep.py\u0026#34;, ], os.environ) for i in range(3) : pid, status = os.waitpid(-1, 0) ⓵ – Create the first process in the shell pipeline.\n⓶ – Start a new process group and store its PID as a new PGID for all future processes.\n⓷ – Start new processes and move them into the process group with PGID.\nSo when we run it we can see the following output:\npython3 ./pg.py parent: 8429 pgid: 8430 8431 sleep 8432 sleep 8430 sleep The full state of processes:\n$ cat /proc/8429/stat | cut -d \u0026#34; \u0026#34; -f 1,4,5,6,7,8 | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | paste \u0026lt;(echo -ne \u0026#34;pid\\nppid\\npgid\\nsid\\ntty\\ntpgid\\n\u0026#34;) - pid 8429 ppid 8415 pgid 8429 sid 8415 tty 34816 tpgid 8429 $ cat /proc/8430/stat | cut -d \u0026#34; \u0026#34; -f 1,4,5,6,7,8 | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | paste \u0026lt;(echo -ne \u0026#34;pid\\nppid\\npgid\\nsid\\ntty\\ntpgid\\n\u0026#34;) - pid 8430 ppid 8429 pgid 8430 sid 8415 tty 34816 tpgid 8429 $ cat /proc/8431/stat | cut -d \u0026#34; \u0026#34; -f 1,4,5,6,7,8 | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | paste \u0026lt;(echo -ne \u0026#34;pid\\nppid\\npgid\\nsid\\ntty\\ntpgid\\n\u0026#34;) - pid 8431 ppid 8429 pgid 843 sid 8415 tty 34816 tpgid 8429 $ cat /proc/8432/stat | cut -d \u0026#34; \u0026#34; -f 1,4,5,6,7,8 | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | paste \u0026lt;(echo -ne \u0026#34;pid\\nppid\\npgid\\nsid\\ntty\\ntpgid\\n\u0026#34;) - pid 8432 ppid 8429 pgid 8430 sid 8415 tty 34816 tpgid 8429 The only problem with the above code is we didn\u0026rsquo;t transfer the foreground group to our newly created process group. The tpgid in the above output shows that. The 8429 PID is a PGID of the parent pg.py script, not the newly created process group 8430.\nNow, if we press CTRL+C to terminate the processes, we’ll stop only the parent with PID 8429. It happens because it’s in the foreground group from the perspective of the controlling terminal. All processes in the 8430 group will continue running in the background. If they try to read from the terminal (stdin), they will be stopped by the controlling terminal by sending them a SIGTTIN signal. It is a result of trying to read from the controlling terminal without acquiring the foreground group. If we log out or close the controlling terminal, this group will not get a SIGHUP signal, because the bash process (the controlling process) doesn’t know that we started something in the background.\nIn order to fix this situation, we need to notify the controlling terminal that we want to run another process group in the foreground. Let’s modify the code and add the tcsetpgrp() call.\nimport os import time import signal print(f\u0026#34;parent: {os.getpid()}\u0026#34;) pgpid = os.fork() if not pgpid: # child os.setpgid(os.getpid(), os.getpid()) os.execve(\u0026#34;./sleep.py\u0026#34;, [\u0026#34;./sleep.py\u0026#34;, ], os.environ) print(f\u0026#34;pgid: {pgpid}\u0026#34;) pid = os.fork() if not pid: # child os.setpgid(os.getpid(), pgpid) os.execve(\u0026#34;./sleep.py\u0026#34;, [\u0026#34;./sleep.py\u0026#34;, ], os.environ) pid = os.fork() if not pid: # child os.setpgid(os.getpid(), pgpid) os.execve(\u0026#34;./sleep.py\u0026#34;, [\u0026#34;./sleep.py\u0026#34;, ], os.environ) tty_fd = os.open(\u0026#34;/dev/tty\u0026#34;, os.O_RDONLY) # ⓵ os.tcsetpgrp(tty_fd, pgpid) # ⓶ for i in range(3): # ⓷ os.waitpid(-1, 0) h = signal.signal(signal.SIGTTOU, signal.SIG_IGN) # ⓸ os.tcsetpgrp(tty_fd, os.getpgrp()) # ⓹ signal.signal(signal.SIGTTOU, h) # ⓺ print(\u0026#34;got foreground back\u0026#34;) time.sleep(99999) ⓵ – In order to run the tcsetpgrp(), we need to know the current controlling terminal path. The safest way to do that is to open a special virtual file /dev/tty. If a process has a controlling terminal, it returns a file descriptor for that terminal. We, in theory, can use one of the standard file descriptors too. But it\u0026rsquo;s not sustanable because the caller can redirects all of them.\n⓶ – Put the new process group into the foreground group of the controlling terminal.\n⓷ – Here, we wait for the processes to exit. It is where we should call CTRL+C.\n⓸ – Before we command the controlling terminal to return into the foreground session we need to silence the SIGTTOU signal. The man page says: If tcsetpgrp() is called by a member of a background process group in its session, and the calling process is not blocking or ignoring SIGTTOU, a SIGTTOU signal is sent to all members of this background process group. We don’t need this signal, so it’s OK to block it.\n⓹ – Returning to the foreground.\n⓺ – Restoring the SIGTTOU signal handler.\nAnd if we now run the script and press CTRL+C, everything should work as expected.\n$ python3 ./pg.py parent: 8621 pgid: 8622 8622 sleep 8624 sleep 8623 sleep ^C \u0026lt;------------------- CTRL+C was pressed Traceback (most recent call last): File \u0026#34;/home/vagrant/data/blog/post2/./sleep.py\u0026#34;, line 7, in \u0026lt;module\u0026gt; Traceback (most recent call last): File \u0026#34;/home/vagrant/data/blog/post2/./sleep.py\u0026#34;, line 7, in \u0026lt;module\u0026gt; Traceback (most recent call last): File \u0026#34;/home/vagrant/data/blog/post2/./sleep.py\u0026#34;, line 7, in \u0026lt;module\u0026gt; time.sleep(99999) KeyboardInterrupt time.sleep(99999) KeyboardInterrupt time.sleep(99999) KeyboardInterrupt got foreground back \u0026lt;----------------- back to foreground Shell job control # Now it’s time to understand how shells allow us to run multiple commands simultaneously and how we can control them.\nFor instance, when we run the following pipeline:\n$ sleep 999 | grep 123 The shell here:\ncreates a new process group with the PGID of the first process in the group; puts this group in the foreground group by notifying the terminal with tcsetpgrp(); stores the PIDs and sets up a waitpid() syscall. The process group is also known as a shell job. The PIDs:\n$ ps a | grep sleep 9367 pts/1\tS+ 0:00 sleep 999 $ ps a | grep grep 9368 pts/1\tS+ 0:00 grep 123 And if we get the details for sleep:\n$ cat /proc/9367/stat | cut -d \u0026#34; \u0026#34; -f 1,4,5,6,7,8 | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | paste \u0026lt;(echo -ne \u0026#34;pid\\nppid\\npgid\\nsid\\ntty\\ntpgid\\n\u0026#34;) - pid 9367 ppid\t6821 pgid\t9367 sid 6821 tty 34817 tpgid\t9367 and for grep:\n$ cat /proc/9368/stat | cut -d \u0026#34; \u0026#34; -f 1,4,5,6,7,8 | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | paste \u0026lt;(echo -ne \u0026#34;pid\\nppid\\npgid\\nsid\\ntty\\ntpgid\\n\u0026#34;) - pid 9368 ppid\t6821 pgid\t9367 sid 6821 tty 34817 tpgid\t9367 While waiting for the foreground job to finish, we can move this job to the background by pressing Ctrl+Z. It is a control action for the terminal, which sends a SIGTSTP signal to the foreground process group. The default signal handler for a process is to stop. In its turn, bash gets a notification from the waitpid(), that the statuses of the monitoring processes have changed. When bash sees that the foreground group has become stopped, it returns the foreground back to shell by running tcsetpgrp():\n^Z [1]+ Stopped sleep 999 | grep 123 $ We can get the current statuses of all known jobs by using the built-in jobs command:\n$ jobs -l [1]+ 7962 Stopped sleep 999 7963 | grep 123 We may resume a job in the background by calling bg shell built-in with the ID of the job.When we use bg with a background stopped job, the shell uses killpg and SIGCONT signal.\n$ bg %1 [1]+ sleep 999 | grep 123 \u0026amp; If we check the status now, we can see that it’s running in the background.\n$ jobs -l [1]+ 7962 Running sleep 999 7963 | grep 123 \u0026amp; If we want, we can move the job back in the foreground by calling fg built-in shell command:\n$ fg %1 sleep 999 | grep 123 We also can start a job in the background by adding an ampersand (\u0026amp;) char in the end of the pipeline:\n$ sleep 999 | grep 123 \u0026amp; [1] 9408 $ kill command # kill is usually a shell built-in for at least two reasons:\nShell usually allows to kill jobs by their job ids. So we need to be able to resolve internal job IDs into process group IDs (the %job_id syntaxis). Allow users to send signals to processes if the system hits the max running process limit. Usually, during emergencies and system misbehaviour. For example, we can check how bash does it – int kill_builtin() and zsh – int bin_kill().\nAnother helpful piece of knowledge about the kill command and system calls is a \u0026ldquo;-1\u0026rdquo; process group. It\u0026rsquo;s a special group, and the signal to it will fan out the signal to all processes on the system except the PID 1 process (it\u0026rsquo;s almost always a systemd process on all modern GNU/Linux distributions):\n[remote ~] $ sudo kill -15 -1 Connection to 192.168.0.1 closed by remote host. Connection to 192.168.0.1 closed. [local ~] $ Terminating shell # When a controlling process loses its terminal connection, the kernel sends a SIGHUP signal to inform it of this fact. If either the controlling process or other members of the session ignores this signal, or handle it and do nothing, then the further read from and write to the closed terminal (ususally /dev/pts/*) calls will return the end-of-file (EOF) zero bytes.\nShell processes (which are usually control terminals) have a handler to catch SIGHUP signals. Receiving a signal starts a fan-out process of sending SIGHUP signals to all jobs it has created and know about (remember the fg, bg and waitpid()). The default action for the SIGHUP is terminate.\nnohup and disown # But suppose we want to protect our long-running program from being suddenly killed by a broken internet connection or low laptop battery. In that case, we can start a program under nohup tool or use bash job control disownbuilt-in command. Let’s understand how they work and where they are different.\nThe nohup performs the following tricks:\nChanges the stdin fd to /dev/null. Redirects the stdout and stderr to a file on disk. Set an ignore SIG_IGN flag for SIGHUP signal. The interesting moment here is that the SIG_IGN is preserved after the execve() syscall. Run the execve(). All the above make the program immune to the SIGHUP signal and can’t fail due to writing to or reading from the closed terminal.\n$ nohup ./long_running_script.py \u0026amp; [1] 9946 $ nohup: ignoring input and appending output to \u0026#39;nohup.out\u0026#39; $ jobs -l [1]+ 9946 Running nohup ./long_running_script.py \u0026amp; As you can see from the output, the bash knows about this process and can show it in jobs.\nAnother way we have to achieve long-running programs to survive the controlling terminal closure is a built-in disown of the bash shell. Instead of ignoring the SIGHUP signal, it just removes the job\u0026rsquo;s PID from the list of known jobs. Thus no SIGHUP signal will be sent to the group.\n$ ./long_running_script.py \u0026amp; [1] 9949 $ jobs -l [1]+ 9949 Running ./long_running_script.py \u0026amp; $ disown 9949 $ jobs -l $ ps a | grep 9949 9949 pts/0\tS 0:00 /usr/bin/python3 ./long_running_script.py 9954 pts/0\tS+ 0:00 grep 9949 The drawback of the above solution is we don’t overwrite and close the terminal standard fd. So if the tool decides to write to or read from the closed terminal, it could fail.\nThe other conclusion we can make is that the shell doesn’t send SIGHUP to processes or groups it did not create, even if the process is in the same session where the shell is a session leader. Daemons # A daemon is a long living process. It is often started at the system’s launch and service until the OS shutdown. Daemon runs in the background without a controlling terminal. The latest guarantees that the process never gets terminal-related signals from the kernel: SIGINT, SIGTSTP, and SIGHUP.\nThe classic \u0026ldquo;unix\u0026rdquo; way of spawning daemons is performed by a double-fork technique. After both fork() calls the parents exit immediately.\nThe first fork() is needed: to become a child of the systemd process with PID 1; if a daemon starts manually from a terminal, it puts itself into the background and a shell doesn’t know about it, so it can’t terminate the daemon easily; the child is guaranteed not to be a process group leader, so the following setsid() call starts a new session and breaks a possible connection to the existing controlling terminal. The second fork() is done in order to stop being the session leader. This step protects a daemon from opening a new controlling terminal, as only a session leader can do that. The gnu provides a convininet libc function to demonize our program: daemon() man 3 daemon.\nBut nowadays, systems with systemd tend not to follow the double-fork trick. Instead developers highly rely on systemd features:\nsystemd can starts a new process session for daemons; it can swap the standard file descriptors for stdin, stdout and stderr with regular files or sockets instead of manually close or redirect them to syslog. For example nginx code: ... fd = open(\u0026#34;/dev/null\u0026#34;, O_RDWR); ... if (dup2(fd, STDIN_FILENO) == -1) { ngx_log_error(NGX_LOG_EMERG, log, ngx_errno, \u0026#34;dup2(STDIN) failed\u0026#34;); return NGX_ERROR; } ... So, a daemon can continue safely write to the stderr and stdout and don’t be afraid of getting the EOF because of a closed terminal. The following setting controls that:\nStandardOutput= StandardError= For instance, etcd service doesn\u0026rsquo;t do a double-fork and fully rely on the systemd. That’s why its PID is a PGID and SID, so it’s a session leader.\n$ cat /proc/10350/stat | cut -d \u0026#34; \u0026#34; -f 1,4,5,6,7,8 | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | paste \u0026lt;(echo -ne \u0026#34;pid\\nppid\\npgid\\nsid\\ntty\\ntpgid\\n\u0026#34;) - pid 10350 ppid\t1 pgid\t10350 sid 10350 tty 0 tpgid\t-1 Also systemd has a lot of other features for modern service developers such as helpers for live upgrades, socket activation, sharing sockets, cgroup limits, etc\u0026hellip;\nRead next chapter → "},{"id":15,"href":"/docs/resolver-dual-stack-application/4-getaddrinfo-from-glibc/","title":"getaddrinfo() from glibc","section":"DNS resolvers and Dual-Stack applications","content":" 4. getaddrinfo() from glibc # Last updated: Oct 2025 Contents\n4.1 Internals and design 4.2 Name Service Switch (nss) 4.3 Name service cache daemon NSCD 4.4 Thread safety issues with getaddrinfo() 4.5. /etc/resolv.conf 4.6. /etc/hosts The standard in POSIX describes only the behavior and interface of the getaddrinfo() function. However, the actual implementation can vary between different frameworks. In this chapter, we will examine the internals of the getaddrinfo() implementation from glibc version 2.39. In the GNU/Linux world the glibc remains the default C library for the overwhelming majority of systems.\n4.1 Internals and design # Even though the main purpose of a stub resolver is to send DNS queries to a recursive server, the reality is more complex than that. The chart below illustrates the main steps that the glibc getaddrinfo() function performs each time you call it.\nFigure 2. – glibc getaddrinfo() internals with Name Service Switch (NSS). ① – The first step in the process might come as a surprise: getaddrinfo() attempts to connect to a hardcoded path of a Unix socket each time it is called. This socket is a part of the Name Service Switch system and related to a cache daemon. We are touching it later in this section.\n② – The next step involves reading the Name Service Switch configuration file, /etc/nsswitch.conf. This file sets the order of sources for various services, not just domain resolution. The line of interest for domain name resolution starts with the keyword \u0026ldquo;hosts\u0026rdquo;. Subsequent words identify NSS modules, which are queried from left to right in the specified order.\nOne important note about/etc/nsswitch.conf is that it includes special notation for error handling: for instance, whether to fail immediately, retry, or ignore the error and move to the next module.\nIn the example shown in the chart above, the getaddrinfo() call should first check the /etc/hosts file, then query the DNS name server, and finally run a custom module, which we are going to write.\n③ – NSS modules are shared libraries that adhere to the NSS Modules Interface and are named accordingly to a template defined in the man 5 nsswitch.conf.\n④ – The final step is sorting destination addresses according to ten rules from RFC 6724, which we will discuss in more detail in the chapter on dual-stack implementation.\n4.2 Name Service Switch (nss) # Name service switch (NSS) is a framework used to manage the sources from which various name service information is obtained and is which order it’s requested. The system configuration file for NSS is located at /etc/nsswitch.conf (man 5 nsswitch.conf).\nThe Name Service Switch (NSS) configuration file, /etc/nsswitch.conf, is used by the GNU C Library and certain other applications to determine the sources from which to obtain name-service information in a range of categories, and in what order. Each category of information is identified by a database name.\nIt’s possible to write your own NSS module. The module is a shared library that adheres to a defined API and should be named and placed according to the following format from the man 5 nsswitch.conf:\n/lib/libnss_custom.so.2 for a module named custom.\nNow we are ready to write our own NSS module for the host subsystem. We will need a great Rust library libnss-rs.\nThe step-by-step instructions:\nInstall rust and gcc:\ncurl --proto \u0026#39;=https\u0026#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh sudo apt-get install gcc We are naming our module gildor 🧝‍♂:\nDependencies:\n[lib] name = \u0026#34;nss_gildor\u0026#34; crate-type = [ \u0026#34;cdylib\u0026#34; ] [dependencies] libc = \u0026#34;0.2.0\u0026#34; libnss = \u0026#34;0.8.0\u0026#34; Code:\nuse libnss::host::{AddressFamily, Addresses, Host, HostHooks}; use libnss::libnss_host_hooks; use libnss::interop::Response; use std::net::{IpAddr, Ipv4Addr}; struct HardcodedHost; libnss_host_hooks!(gildor, HardcodedHost); impl HostHooks for HardcodedHost { fn get_all_entries() -\u0026gt; Response\u0026lt;Vec\u0026lt;Host\u0026gt;\u0026gt; { Response::Success(vec![Host { name: \u0026#34;host1.example\u0026#34;.to_string(), addresses: Addresses::V4(vec![Ipv4Addr::new(192, 168, 1, 1)]), aliases: vec![\u0026#34;super-host1.example\u0026#34;.to_string()], }]) } fn get_host_by_addr(addr: IpAddr) -\u0026gt; Response\u0026lt;Host\u0026gt; { match addr { IpAddr::V4(addr) =\u0026gt; { if addr.octets() == [192, 168, 1, 1] { Response::Success(Host { name: \u0026#34;host1.example\u0026#34;.to_string(), addresses: Addresses::V4(vec![Ipv4Addr::new(192, 168,1, 1)]), aliases: vec![\u0026#34;super-host1.example\u0026#34;.to_string()], }) } else { Response::NotFound } } _ =\u0026gt; Response::NotFound, } } fn get_host_by_name(name: \u0026amp;str, family: AddressFamily) -\u0026gt; Response\u0026lt;Host\u0026gt; { if name.ends_with(\u0026#34;.example\u0026#34;) \u0026amp;\u0026amp; family == AddressFamily::IPv4 { Response::Success(Host { name: name.to_string(), addresses: Addresses::V4(vec![Ipv4Addr::new(192, 168, 1, 1)]), aliases: vec![\u0026#34;host1.example\u0026#34;.to_string(), \u0026#34;super-host1.example\u0026#34;.to_string()], }) } else { Response::NotFound } } } We also need an install script to put our library in the correct place:\n$ cat install.sh #!/bin/sh cd target/release cp libnss_gildor.so libnss_gildor.so.2 sudo install -m 0644 libnss_gildor.so.2 /lib sudo /sbin/ldconfig -n /lib /usr/lib It’s time to enable our module in /etc/nsswitch.conf:\n$ cat /etc/nsswitch.conf | grep hosts: hosts: gildor dns files And query NSS with getent (man 1 getent) tool:\n$ getent ahosts host1.example 192.168.1.1 STREAM host1.example 192.168.1.1 DGRAM 192.168.1.1 RAW 4.3 Name service cache daemon NSCD # If we revisit our example from Chapter 3 and examine the output of strace more carefully, we can find the following lines:\n$ sudo strace -T -tt ./getaddrinfo microsoft.com 2\u0026gt;\u0026amp;1 | grep nscd 21:03:57.515650 connect(3, {sa_family=AF_UNIX, sun_path=\u0026#34;/var/run/nscd/socket\u0026#34;}, 110) = -1 ENOENT (No such file or directory) \u0026lt;0.000102\u0026gt; 21:03:57.516376 connect(3, {sa_family=AF_UNIX, sun_path=\u0026#34;/var/run/nscd/socket\u0026#34;}, 110) = -1 ENOENT (No such file or directory) \u0026lt;0.000053\u0026gt; 21:03:57.521218 connect(3, {sa_family=AF_UNIX, sun_path=\u0026#34;/var/run/nscd/socket\u0026#34;}, 110) = -1 ENOENT (No such file or directory) \u0026lt;0.000053\u0026gt; 21:03:57.521794 connect(3, {sa_family=AF_UNIX, sun_path=\u0026#34;/var/run/nscd/socket\u0026#34;}, 110) = -1 ENOENT (No such file or directory) \u0026lt;0.000050\u0026gt; There are multiple attempts to connect over the UNIX socket to the /var/run/nscd/socket path, which usually doesn’t exist by default. The output shows that, on average, each call took 0.000050 seconds.\nThe UNIX socket at/var/run/nscd/socket is part of glibc and the Name Service Cache Daemon (nscd), which is known for its instability and frequent bugs. As a result, it is often avoided. However, there are alternatives, and if you cannot afford to spend 0.000050 seconds on every getaddrinfo() call, you might consider installing them or replacing the stub resolver. Fortunately, everything is open source.\n4.4 Thread safety issues with getaddrinfo() # It’s worth noting that you should be cautious when calling getaddrinfo() in a multithreaded application, as getaddrinfo() invokes getenv(). This is detailed in the resolv.conf (man 5 resolv.conf).\nThe search keyword of a system\u0026rsquo;s resolv.conf file can be overridden on a per-process basis by setting the environment variable LOCALDOMAIN to a space-separated list of search domains.\nThe options keyword of a system\u0026rsquo;s resolv.conf file can be amended on a per-process basis by setting the environment variable RES_OPTIONS to a space-separated list of resolver options as explained above under options.\nSo there are two environment variables: LOCALDOMAIN and RES_OPTIONS. This means that getaddrinfo() calls getenv()internally, but setenv() is not thread safe according to its own man.\nsetenv(), unsetenv() │ Thread safety │ MT-Unsafe const:env So if you have two threads in an application where one is resolving a domain name and the other is setting environment variables using setenv(), you might encounter a segmentation fault.\nThis is particularly relevant for Golang applications, which are typically multithreaded by default due to the design of language and goroutines. An example of this issue can be seen in an open issue with the cgo stub resolver: https://github.com/golang/go/issues/63567.\n4.5. /etc/resolv.conf # Technically, /etc/resolv.conf is not part of glibc; it is a system configuration file used by all known resolvers. However, I’ve chosen to discuss it in the glibc section because it is often challenging to unwind the dependencies and relationships due to the interconnections and interoperability between components (for example, glibc resolver library man 3 resolver).\nThe man page for /etc/resolv.conf (man 5 resolv.conf) includes several important configuration parameters that could save you hours of debugging and troubleshooting:\nnameserver – parameter allows you to specify up to three nameservers, which will be queried in sequence. Note that in contrast, the musl libc implementation queries all specified nameservers in parallel (see the musl libc chapter below for details).\nThe algorithm used is to try a name server, and if the query times out, try the next, until out of name servers, then repeat trying all the name servers until a maximum number of retries are made.\nsearch – it’s a search list of domains to use for a hostname lookup.\nResolver queries having fewer than ndots dots (default is 1) in them will be attempted using each component of the search path in turn until a match is found.\nThis means that if the domain name used in getaddrinfo() contains fewer than the specified ndots (explained further below), the resolver will append the search domain from the list and attempt to resolve it first.\ndomain – is the obsolete version of search.\nThe options ndots:n – specifies the minimum number of dots a domain name must have to avoid automatically appending the search domain to the name being resolved. The default setting is 1, meaning the only way to trigger the addition of a search domain is to attempt to resolve a hostname that contains no dots. For example:\n$ cat /etc/resolv.conf | grep search search mydomain If we try now to resolve a hostname without a domain name:\n$ getent ahosts resolve-me In tcpdump you should see a number of queries:\n$ tcpdump -s0 -i any -n -A dst port 53 08:15:42.289500 lo In IP 127.0.0.1.43396 \u0026gt; 127.0.0.53.53: 4492+ [1au] A? resolve-me.mydomain. (48) 08:15:42.289517 lo In IP 127.0.0.1.43396 \u0026gt; 127.0.0.53.53: 51586+ [1au] AAAA? resolve-me.mydomain. (48) 08:15:42.421518 lo In IP 127.0.0.1.57952 \u0026gt; 127.0.0.53.53: 48456+ [1au] A? resolve-me. (39) 08:15:42.421526 lo In IP 127.0.0.1.57952 \u0026gt; 127.0.0.53.53: 40777+ [1au] AAAA? resolve-me. (39) The output shows that the first two requests for A and AAAA were sent with the search domain appended.\noptions edns0 – enables DNS extensions described in RFC 2671 which brings more efficient query and response handling. options timeout:n – how long to wait for an answer from a nameserver before sending a query to the next in-order. The default is 5 seconds. options attempts:n – how many times to retry a nameserver before switching to the next one. Default is 2 times. options rotate – don’t start from the first nameserver every time, instead apply a round-robin algorithm. As we wrap up this chapter, I’d like to highlight a potential resolver issue related to the ndots setting in Kubernetes environments, where /etc/resolv.conf might include multiple search domains, and ndots may be set higher than the default one dot.\nFor example:\n$ cat /etc/resolv.conf search namespace.svc.cluster.local svc.cluster.local cluster.local options ndots:5 Firstly, it’s important to understand what a Fully Qualified Domain Name (FQDN) is. By convention, an FQDN should end with a trailing dot, meaning \u0026ldquo;example.com.\u0026rdquo; is a FQDN, whereas \u0026ldquo;example.com\u0026rdquo; is not. However, many applications, such as web browsers, implicitly assume the trailing dot, which makes the distinction subtle.\nWith the above resolv.conf settings, attempting to resolve \u0026ldquo;example.com\u0026rdquo; will generate six additional, unnecessary DNS queries (three for A records and three for AAAA records) for three FQDNs:\nexample.com.namespace.svc.cluster.local. example.com.svc.cluster.local. example.com.cluster.local. These queries occur before resolving what we actually want: \u0026ldquo;example.com.\u0026rdquo;. This can add significant latency and increase DNS traffic. To mitigate this situation, there are at least two approaches:\nModify the ndots option if you are confident that your application in a pod will not use internal DNS names: \\ apiVersion: v1 kind: Pod spec: dnsConfig: options: - name: ndots value: \u0026#34;1\u0026#34; Change all your domain names to have a trailing dot at the end in configs and databases: Ensure all your domain names in configs and databases include a trailing dot at the end, such as \u0026ldquo;example.com.cluster.local.\u0026rdquo;. 4.6. /etc/hosts # \u0026ldquo;The well-known /etc/hosts (man 5 hosts) is a plain text file used by operating systems to map hostnames to IP addresses.\nThe only interesting thing I can probably share is the first hostname is the canonical_hostname (which is included in a getaddrinfo() output) and all others are aliases.\nIP_address canonical_hostname [aliases...] Read next chapter → "},{"id":16,"href":"/docs/fd-pipe-session-terminal/4-terminals-and-pseudoterminals/","title":"Terminals and pseudoterminals","section":"GNU/Linux shell related internals","content":" Terminals and pseudoterminals # Last updated: Oct 2025 Contents\nPseudoterminal (devpts) Terminal settings Handling Terminal Signals screen and tmux Pseudoterminal proxy Changing a process’s controlling terminal Terminals come to us from the history of UNIX systems. Basically, terminals provided an API for the console utils (physical ones!) to generalize interaction with users. It includes ways of reading input and writing to it in two modes:\nthe canonical mode (default) – input is buffered line by line and read into after a new line char \\n occurs; the noncanonical mode – an application can read terminal input a character at a time. For example vi, emacs and less use this mode. Nowadays, with the widespread use of rich graphical UIs, the significance of the terminals are lesser than it was, but still, we use this protocol implicitly every time we start an ssh connection.\nThe are a bunch of files under /dev/ directory that represents different types of terminals:\n/dev/tty* – physical consoles; /dev/ttyS* – serial connections; /dev/pts/* – pseudoterminals. Also, the /proc/tty/drivers file contains other supported drivers.\nSo, in order to determine what terminal file the current shell session is using, we have a tty cli tool (man 1 tty).\nOn my remote ssh connection:\n$ tty /dev/pts/0 For a physical console:\n$ tty /dev/tty1 We can also open a virtual device /dev/tty to get a fd of the controlling terminal if it exists for the current process.\nPseudoterminal (devpts) # In order to make it possible to use a terminal remotely, the Linux kernel provides a feature called pseudoterminal or devpts (https://www.kernel.org/doc/html/latest/filesystems/devpts.html).\nIt allows us to build terminal emulators and use them instead of a real terminal, where an application expects a terminal device. Using pseudoterminals we can build terminal proxies, record screen sessions and mock user input. You can think about pseudoterminal like as a special type of Inter-process communication (IPC). It\u0026rsquo;s a bidirectional communication channel. All operations that can be applied to a terminal device can also be applied to a pts device end, including something that doesn\u0026rsquo;t make sense. For example: changing the speed of connection transforms into a no-op internally.\nPseudoterminal consists of 2 parts:\nA ptmx part which is a leader for the pseudoterminal. This end is used to emulate the user input and read back the program output. A pts is a secondary end. This part is given to an application that needs a terminal. The following image shows how an ssh client uses pseudoterminals to establish remote access.\nFigure 4. – ssh client, sshd server and two pairs of pseudoterminals ❶ – We usually have a graphical UI on our local host and use some kind of xterm to run a local console. The UI subsystem receives all keyboard inputs, so it opens a pseudoterminal and redirects it into its ptmx device. The other side of the terminal is what an xterm emulator sees. ❷ – The local bash process creates a new foreground process group (job). It\u0026rsquo;s running in the foreground, so a ssh gets full control of the terminal. ssh client is a special terminal program that leverages the full power of terminals. It sets the terminal into the raw mode. Thus, the future CTRL-C and CTRL-Z combinations do not affect the local ssh process. Instead, all such commands will be sent to the remote side of the ssh connection and interpreted there. When the ssh client exits, it returns all settings back. ❸, ❼ – The communication between the ptmx and pts happens in the kernel and is hidden from our eyes. ❹ – sshd server is listening to new connections. It makes a fork for each connected user and checks their credentials. ❺ – The sshd process creates a new pseudoterminal pair. It basically connects the ptmx side and the client tcp socket. ❻ – Then the sshd process makes a fork(), opens a corresponding new pts, starts a new session (setsid()), opens our pts making it the controlling terminal of the session and duplicates standard file descriptors 0,1 and 2 with the pts descriptor. Now it’s ready to call execve() to start bash shell. Let’s emulate the above with a small example. We are creating a new session with a new pseudoterminal pair and write the stdin into a file on disk.\nimport os import time import sys print(f\u0026#34;parent: {os.getpid()}\u0026#34;) ptmx, secondary = os.openpty() pid = os.fork() if not pid: print(f\u0026#34;child: {os.getpid()}\u0026#34;) os.close(ptmx) os.setsid() name = os.ttyname(secondary) print(name) s = os.open(name, os.O_RDWR) os.dup2(s, 0) os.dup2(s, 1) os.dup2(s, 2) os.close(secondary) os.close(s) with open(\u0026#39;/tmp/file.txt\u0026#39;, \u0026#34;w\u0026#34;) as f: for l in sys.stdin: f.write(l) f.flush() time.sleep(999999) else: os.close(secondary) os.write(ptmx, b\u0026#34;text\\n\u0026#34;) os.waitpid(-1,0) Run it:\n$ python3 ./terminal.py parent: 8776 child: 8777 /dev/pts/3 Check the file:\n$ cat /tmp/file.txt text File descriptors show that all is good:\n$ ls -l /proc/8776/fd lrwx------ 1 vagrant vagrant 64 Jul 12 21:45 0 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 12 21:45 1 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 12 21:45 2 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 12 21:45 3 -\u0026gt; /dev/ptmx $ ls -l /proc/8777/fd lrwx------ 1 vagrant vagrant 64 Jul 12 21:45 0 -\u0026gt; /dev/pts/3 lrwx------ 1 vagrant vagrant 64 Jul 12 21:45 1 -\u0026gt; /dev/pts/3 lrwx------ 1 vagrant vagrant 64 Jul 12 21:45 2 -\u0026gt; /dev/pts/3 l-wx------ 1 vagrant vagrant 64 Jul 12 21:45 3 -\u0026gt; /tmp/file.txt Terminal settings # The ptmx and pts devices share terminal attributes (termios) and window size (winsize) structures.\nThe current setting of a terminal can be obtained and updated by the stty command:\n$ stty -a As we discussed earlier, the background jobs can print to the stdout by default. However, we can change it by setting TOSTOP flag for the terminal. If we do that, the background process group will receive a SIGTTOU signal from the kernel. The default handler for this signal is stop.\n$ stty tostop And run some background job:\n$ yes | grep y \u0026amp; [1] 10694 [vagrant@archlinux post2]$ jobs -l [1]+ 10693 Stopped (tty output)\tyes 10694 | grep y And return it back:\n$ stty -tostop We also can return back to the default setting by using the \u0026ldquo;sane\u0026rdquo; parameter:\n$ stty sane Handling Terminal Signals # As we discussed, the kernel can send some terminal signals to foreground and background processes. Some of them we already touched:\nSIGTTIN – a background process tried to read from a terminal. SIGTTOU – a background process tries to write to a terminal when the tostop flag is set or a background process asks to send it to the foreground. SIGTSTP – a default response to a CTRL-Z pressed combination. The noncanonical programs such as vi, emacs and less, need to handle all the above signals in order to reset terminal settings back and forth, redraw a terminal content and place the cursor in the right place.\nAnother interesting terminal signal we haven’t seen is the SIGWINCH signal. A foreground process receives it when size of a terminal window has changed. Usually, a program uses ioctl() with the TIOCGWINSZ operation to get the current size in its signal handler. For example:\nimport time import signal import termios import fcntl import struct def signal_handler(signum, frame): packed = fcntl.ioctl(0, termios.TIOCGWINSZ, struct.pack(\u0026#39;HHHH\u0026#39;, 0, 0, 0, 0)) rows, cols, h_pixels, v_pixels = struct.unpack(\u0026#39;HHHH\u0026#39;, packed) print(rows, cols, h_pixels, v_pixels) signal.signal(signal.SIGWINCH, signal_handler) time.sleep(9999) And if you start it and play with size of terminal window:\n$ python3 ./size.py 32 85 1360 1280 32 72 1152 1280 32 74 1184 1280 32 86 1376 1280 32 83 1328 1280 33 73 1168 1320 screen and tmux # screen (man 1 screen) and tmux (man 1 tmux) are usually used for protecting shell sessions between connections. It is also widely used for long-running jobs and better ssh user client experience. Both use pseudoterminals to multiplex a single physical terminal (or terminal window) between multiple processes (multiple shell sessions). In this section, we will talk about tmux, but the screen is almost the same in all discussed topics here.\nOn the first start, tmux starts a server with a set of ptmx corresponding to its panes. Clients of tmux (tmux attach) use a default unix socket to find and connect to the server:\n$ ls -lad /tmp/tmux-1000/default srwxrwx--- 1 vagrant vagrant 0 Jul 14 12:57 /tmp/tmux-1000/default where 1000 is a user id UID.\ntmux doesn\u0026rsquo;t do any tcsetpgrp() calls, because any panel or window creates a new pair of terminals.\nSo let’s demonstrate it. After we ssh to a box, we have our bash with PID 11761 :\n$ echo $$ 11761 $ ls -la /proc/$$/fd lrwx------ 1 vagrant vagrant 64 Jul 14 12:08 0 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 14 12:08 1 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 14 12:08 2 -\u0026gt; /dev/pts/0 Let assume, that we already have a working tmux session on the server. So if we check the ps command, we can see it with PID 11781.\n... ├─sshd(11619)───sshd(11749)───sshd(11760)───bash(11761) ... └─tmux: server(11780)───bash(11781) Now let’s attach to the tmux session:\n$ tmux attach We get a new bash PID from the above ps output and a new pseudoterminal:\n$ echo $$ 11781 $ ls -la /proc/$$/fd lrwx------ 1 vagrant vagrant 64 Jul 14 12:10 0 -\u0026gt; /dev/pts/1 lrwx------ 1 vagrant vagrant 64 Jul 14 12:10 1 -\u0026gt; /dev/pts/1 lrwx------ 1 vagrant vagrant 64 Jul 14 12:10 2 -\u0026gt; /dev/pts/1 If we check the pseudoterminal devpts folder /dev/pts/:\n$ ls -la /dev/pts/ crw--w---- 1 vagrant tty 136, 0 Jul 14 12:11 0 crw--w---- 1 vagrant tty 136, 1 Jul 14 12:11 1 c--------- 1 root\troot 5, 2 Jul 9 21:14 ptmx we can see that there are 2 pseudoterminals, one is our ssh client, and the other is our tmux.\nLet’s observe the tmux server’s file descriptors:\n$ ls -la /proc/11780/fd lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 0 -\u0026gt; /dev/null lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 1 -\u0026gt; /dev/null lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 2 -\u0026gt; /dev/null … lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 5 -\u0026gt; /dev/pts/0 … lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 8 -\u0026gt; /dev/ptmx We see it has an open /dev/ptmx to control the terminal on /dev/pts/1, and a /dev/pts/0 to read our input and write output back to our ssh connection.\nNow, if we detach, we can still see the bash process in the ps output:\n$ pstree -p tmux: server(11780)───bash(11781) It’s left there and is waiting for us. Talkinh about the tmux server, it closed /dev/pts/0 because we returned back control of the ssh terminal and it doesn\u0026rsquo;t need to read and write to it anymore:\n$ ls -la /proc/11780/fd lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 0 -\u0026gt; /dev/null lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 1 -\u0026gt; /dev/null lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 2 -\u0026gt; /dev/null … lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 8 -\u0026gt; /dev/ptmx Also, if we take a look the procfs, we will find out that tmux server has its own session and process group. It makes sense, it should not depend on any of the active terminal connections.\n$ cat /proc/11780/stat | cut -d \u0026#34; \u0026#34; -f 1,5,6,7,8,9 | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | paste \u0026lt;(echo -ne \u0026#34;pid\\nppid\\npgid\\nsid\\ntty\\ntpgid\\n\u0026#34;) - pid 11780 ppid 1 pgid 11780 sid 11780 tty 0 tpgid -1 If we open one more session, we will see one more shell process:\n└─tmux: server(11780)─┬─bash(11781) └─bash(11846) And open file descriptors of the server:\n$ ls -la /proc/11780/fd lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 0 -\u0026gt; /dev/null lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 1 -\u0026gt; /dev/null lrwx------ 1 vagrant vagrant 64 Jul 14 12:18 10 -\u0026gt; /dev/pts/2 lrwx------ 1 vagrant vagrant 64 Jul 14 12:20 11 -\u0026gt; /dev/ptmx ... lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 7 -\u0026gt; /dev/pts/0 lrwx------ 1 vagrant vagrant 64 Jul 14 12:09 8 -\u0026gt; /dev/ptmx The overall schema described above could be presented in the below figure 5:\nFigure 5. – tmux client-server architecture tmux server opens as many pseudoterminals as needed, but none of them is a controlling terminal. It is possible to do it in several ways, and the most simple one is to open a /dev/pts/ptmx with the O_NOCTTY open flag (man 2 open).\nIn order to set a demonize, the tmux server made a single fork() and setsid():\nint daemon(int nochdir, int noclose) { int fd; switch (fork()) { case -1: return (-1); case 0: break; default: _exit(0); } if (setsid() == -1) return (-1); if (!nochdir) (void)chdir(\u0026#34;/\u0026#34;); if (!noclose \u0026amp;\u0026amp; (fd = open(_PATH_DEVNULL, O_RDWR, 0)) != -1) { (void)dup2(fd, STDIN_FILENO); (void)dup2(fd, STDOUT_FILENO); (void)dup2(fd, STDERR_FILENO); if (fd \u0026gt; 2) (void)close (fd); } #ifdef __APPLE__ daemon_darwin(); #endif return (0); } It makes the tmux server immune to terminal terminations and signals logic I described earlier.\nPseudoterminal proxy # As I mentioned earlier, we could think about pseudoterminals as a proxy. The reasonable question is can we leverage them in our day-to-day scripting routines? The answer, as you can guess, is yes. There are two incredible tools: expect (man 1 expect) and script (man 1 script) that uses pseudoterminals in the proxy mode and are super helpful in writing basic automation.\nexpect # The expect program uses a pseudoterminal to allow an interactive terminal-oriented program to be driven from a script file. Let’s assume we need to automate an ssh connection in a shell script. We want to insert username and password when the ssh client asks for them. We can easily achieve this with expect:\n#!/usr/bin/expect set timeout 20 set host [lindex $argv 0] set username [lindex $argv 1] set password [lindex $argv 2] spawn ssh \u0026#34;$username\\@$host\u0026#34; expect \u0026#34;password:\u0026#34; send \u0026#34;$password\\r\u0026#34;; interact And test it:\n[local ~] $ ./ssh.exp 192.168.0.1 vagrant vagrant spawn ssh vagrant@192.168.0.1 vagrant@192.168.0.1\u0026#39;s password: [remote ~]$ where \u0026ldquo;vagrant\u0026rdquo; is our username and password.\nscript # Another task is to record a terminal session. The pseudoterminals are used in the script program, which records all of the input and output that occurs during a shell session.\nRecord to file:\n$ script --timing=time.txt script.log Replay:\n$ scriptreplay --timing=time.txt script.log Changing a process\u0026rsquo;s controlling terminal # And lastly, I want to show you one more fascinating tool reptyr https://github.com/nelhage/reptyr. Imagine, you forgot to start a screen or tmux session and have run a long-running script. Using reptyr you can move it under a screen or tmux session without a restart!\nIt uses ptrace systemcall to change the session id of the running process.\nWe use ptrace to attach to a target process and force it to execute code of our own choosing in order to open the new terminal, and dup2 it over stdout and stderr.\nMore info about it could be found in the detailed author’s blog post:\nhttps://blog.nelhage.com/2011/02/changing-ctty/\nHow it works tl;dr:\nWhile we have mutt captured with ptrace, we can make it fork a dummy child, and start tracing that child, too. We’ll make the child setpgid to make it into its own process group, and then get mutt to setpgid itself into the child’s process group. mutt can then setsid, moving into a new session, and now, as a session leader, we can finally ioctl(TIOCSCTTY) on the new terminal, and we win.\n"},{"id":17,"href":"/docs/resolver-dual-stack-application/5-getaddrinfo-from-musl-libc/","title":"getaddrinfo() from musl libc","section":"DNS resolvers and Dual-Stack applications","content":" 5. getaddrinfo() from musl libc # Last updated: Oct 2025 musl libc is a lightweight, fast, and simple implementation of the standard C library (libc) that aims for efficiency, standards compliance, and security.\nIt gained popularity following its extensive use in Alpine Linux, a security-oriented, lightweight Linux distribution often used as a base image for Docker containers.\nHowever, it is crucial for us to understand that musl libc incorporates a completely new resolver code that behaves differently in certain situations. The most significant differences include:\nInstead of querying nameservers in /etc/resolv.conf sequentially, one by one, musl libc queries all of them in parallel and returns the fastest response.\nmusl libc does not support the single-request and single-request-reopen in /etc/resolv.conf, which aim to wrap around borked and weirdly behaved network devices.\nThe handling of the domain and search directives may also differ depending on the ndots setting in /etc/resolv.conf.\nmusl\u0026rsquo;s resolver previously did not support the domain and search keywords in resolv.conf. This feature was added in version 1.1.13, but its behavior differs slightly from glibc\u0026rsquo;s: queries with fewer dots than the ndots configuration variable are processed with search first then tried literally (just like glibc), but those with at least as many dots as ndots are only tried in the global namespace (never falling back to search, which glibc would do if the name is not found in the global DNS namespace).\nDefault ai_flags differences which are important for dual stack applications which we will cover later in the series.\nIn glibc (we will cover these flags later in the dual stack section):\nai_flags = AI_ADDRCONFIG|AI_V4MAPPED In musl libc:\nai_flags = 0 It is also important to note that TCP support for DNS queries was introduced in version 1.2.4. Prior to this, there could be issues handling larger packets, such as those required for DNSSEC or when a large number of records are returned—situations that are not unusual in Kubernetes setups.\nAlso if we run our example from the Chapter 3 built with musl under strace: we can observe that it doesn\u0026rsquo;t open /etc/nsswitch.conf, the nscd socket, nor /etc/gai.conf (which we discuss later). The only files it reads are:\n$ strace -e openat ./getaddrinfo microsoft.com openat(AT_FDCWD, \u0026#34;/etc/services\u0026#34;, O_RDONLY|O_LARGEFILE|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026#34;/etc/hosts\u0026#34;, O_RDONLY|O_LARGEFILE|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026#34;/etc/resolv.conf\u0026#34;, O_RDONLY|O_LARGEFILE|O_CLOEXEC) = 3 Read next chapter → "},{"id":18,"href":"/docs/page-cache/3-page-cache-and-basic-file-operations/","title":"Page Cache and basic file operations","section":"Linux Page Cache series","content":" Page Cache and basic file operations # Last updated: Oct 2025 Contents\nFile reads Reading files with read() syscall Reading files with mmap() syscall File writes Writing to files with write() syscall File writes with mmap() syscall Dirty pages Synchronize file changes with fsync(), fdatasync() and msync() Checking file presence in Page Cache with mincore() Now it\u0026rsquo;s time to roll up our sleeves and get started with some practical examples. By the end of this chapter, you will know how to interact with Page Cache and which tools you can use.\nUtils needed for this section:\nsync (man 1 sync) – a tool to flush all dirty pages to persistent storage; /proc/sys/vm/drop_caches (man 5 proc) – the kernel procfs file to trigger Page Cache clearance; vmtouch – a tool for getting Page Cache info about a particular file by its path. NOTE For now, we ignore how vmtouch works. I\u0026rsquo;m showing how to write an alternative with almost all its features later. File reads # Reading files with read() syscall # I start with a simple program that reads the first 2 bytes from our test file /var/tmp/file1.db.\nwith open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;br\u0026#34;) as f: print(f.read(2)) Usually, these kinds of read requests are translated into the read() syscall. Let\u0026rsquo;s run the script with strace (man 1 strace) to make sure that f.read() uses read() syscall:\n$ strace -s0 python3 ./read_2_bytes.py The output should look something like this:\n... openat(AT_FDCWD, \u0026#34;./file1.db\u0026#34;, O_RDONLY|O_CLOEXEC) = 3 ... read(3, \u0026#34;%B\\353\\276\\0053\\356\\346Nfy2\\354[\u0026amp;\\357\\300\\260%D6$b?\u0026#39;\\31\\237_fXD\\234\u0026#34;..., 4096) = 4096 ... NOTE\nThe read() syscall returned 4096 bytes (one page) even though the script asked only for 2 bytes. It\u0026rsquo;s an example of python optimizations and internal buffered IO. Although this is beyond the scope of this post, but in some cases it is important to keep this in mind.\nNow let’s check how much data the kernel\u0026rsquo;s cached. In order to get this info, we use vmtouch:\n$ vmtouch /var/tmp/file1.db Files: 1 LOOK HERE Directories: 0 ⬇ Resident Pages: 20/32768 80K/128M 0.061% Elapsed: 0.001188 seconds From the output, we can see that instead of 2B of data that Python\u0026rsquo;s asked for, the kernel has cached 80KiB or 20 pages.\nBy design, the kernel can\u0026rsquo;t load anything less than 4KiB or one page into Page Cache, but what about the other 19 pages? It is a excellent example of the kernel\u0026rsquo;s read ahead logic and preference to perform sequential IO operations over random ones. The basic idea is to predict the subsequent reads and minimize the number of disks seeks. Syscalls can control this behavior: posix_fadvise() (man 2 posix_fadvise) and readahead() (man 2 readahead).\nNOTE\nUsually, it doesn\u0026rsquo;t make a big difference for database management systems and storages to tune the default read-ahead parameters in a production environment. If DBMS doesn\u0026rsquo;t need data that were cached by the read-ahead, the kernel memory reclaim policy should eventually evict these pages from Page Cache. And usually, the sequential IO is not expensive for kernel and hardware. Disabling read-ahead at all might even lead to some performance degradations due to increased number of disk IO operations in the kernel queues, more context switches and more time for kernel memory management subsystem to recognize the working set. We will talk about memory reclaiming policy, memory pressure, and cache writeback later in this series.\nLet’s now use posix_fadvise() to notify the kernel that we are reading the file randomly, and thus we don\u0026rsquo;t want to have any read ahead features:\nimport os with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;br\u0026#34;) as f: fd = f.fileno() os.posix_fadvise(fd, 0, os.fstat(fd).st_size, os.POSIX_FADV_RANDOM) print(f.read(2)) Before running the script, we need to drop all caches:\n$ echo 3 | sudo tee /proc/sys/vm/drop_caches \u0026amp;\u0026amp; python3 ./read_2_random.py And now, if you check the vmtouch output, you can see that there is only one page as expected:\n$ vmtouch /var/tmp/file1.db Files: 1 LOOK HERE Directories: 0 ⬇ Resident Pages: 1/32768 4K/128M 0.00305% Elapsed: 0.001034 seconds Reading files with mmap() syscall # For reading data from files we can also use mmap() syscall (man 2 mmap). mmap() is a \u0026ldquo;magic\u0026rdquo; tool and can be used to solve a wide range of tasks. But for our tests, we need only one of its features – an ability to map a file into a process memory in order to access the file as a flat array. I\u0026rsquo;m talking about mmap() in more detail later. But at the moment, if you are not familiar with it, mmap() API should be clear from the following example:\nimport mmap with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;r\u0026#34;) as f: with mmap.mmap(f.fileno(), 0, prot=mmap.PROT_READ) as mm: print(mm[:2]) The above code does the same as we\u0026rsquo;ve just done with read() syscall. It reads the first 2 bytes of the file.\nAlso, for test purposes, we need to flush all caches before the script should be executed:\n$ echo 3 | sudo tee /proc/sys/vm/drop_caches \u0026amp;\u0026amp; python3 ./read_2_mmap.py And checking the Page Cache content:\n$ vmtouch /var/tmp/file1.db Files: 1. LOOK HERE Directories: 0 ⬇ Resident Pages: 1024/32768 4M/128M 3.12% Elapsed: 0.000627 seconds As you can see, mmap() has performed an even more aggressive readahead.\nLet\u0026rsquo;s change the readahead with madvise() syscall like we did with fadvise().\nimport mmap with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;r\u0026#34;) as f: with mmap.mmap(f.fileno(), 0, prot=mmap.PROT_READ) as mm: mm.madvise(mmap.MADV_RANDOM) print(mm[:2]) Run it:\n$ echo 3 | sudo tee /proc/sys/vm/drop_caches \u0026amp;\u0026amp; python3 ./read_2_mmap_random.py and Page Cache content:\n$ vmtouch /var/tmp/file1.db Files: 1 LOOK HERE Directories: 0 ⬇ Resident Pages: 1/32768 4K/128M 0.00305% Elapsed: 0.001077 seconds As you can see from the above output, with the MADV_RANDOM flag, we managed to achieve exactly one page read from disk and thus one page in Page Cache.\nFile writes # Now let\u0026rsquo;s play with writes.\nWriting to files with write() syscall # Let’s continue working with our experimental file and try to update the first 2 bytes instead:\nwith open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;br+\u0026#34;) as f: print(f.write(b\u0026#34;ab\u0026#34;)) NOTE\nBe careful, and don\u0026rsquo;t open a file with w mode. It will rewrite your file with 2 bytes. We need r+ mode.\nDrop all caches and run the above script:\nsync; echo 3 | sudo tee /proc/sys/vm/drop_caches \u0026amp;\u0026amp; python3 ./write_2_bytes.py Now let\u0026rsquo;s check the content of the Page Cache.\n$ vmtouch /var/tmp/file1.db Files: 1 LOOK HERE Directories: 0 ⬇ Resident Pages: 1/32768 4K/128M 0.00305% Elapsed: 0.000674 seconds As you can see, we have 1 page cached after only 2B write. It\u0026rsquo;s an important observation because if your writes are smaller than a page size, you will have 4KiB reads before your writes in order to populate Page Cache.\nAlso, we can check dirty pages by reading the current cgroup memory stat file.\nGet a current terminal cgroup:\n$ cat /proc/self/cgroup 0::/user.slice/user-1000.slice/session-4.scope $ grep dirty /sys/fs/cgroup/user.slice/user-1000.slice/session-3.scope/memory.stat file_dirty 4096 If you see 0, run the script one more time, you apparently get lucky, and the dirty pages have already been written to disk.\nFile writes with mmap() syscall # Let\u0026rsquo;s now replicate the write with mmap():\nimport mmap with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;r+b\u0026#34;) as f: with mmap.mmap(f.fileno(), 0) as mm: mm[:2] = b\u0026#34;ab\u0026#34; You can repeat the above commands with vmtouch and cgroup grep to get dirty pages, and you should get the same output. The only exception is the read ahead policy. By default, mmap() loads much more data in Page Cache, even for write requests.\nDirty pages # As we saw earlier, a process generates dirty pages by writing to files through Page Cache.\nLinux provides several options to get the number of dirty pages. The first and oldest one is to read /proc/meminfo:\n$ cat /proc/meminfo | grep Dirty Dirty: 4 kB The full system information is often hard to interpret and use because we can\u0026rsquo;t determine which process and file has these dirty pages.\nThat\u0026rsquo;s why the best option in order to get dirty page info is to use cgroup:\n$ cat /sys/fs/cgroup/user.slice/user-1000.slice/session-3.scope/memory.stat | grep dirt file_dirty 4096 If your program uses mmap() to write to files, you have one more option to get dirty pages stats with a per-process granularity. procfs has the /proc/PID/smaps file. It contains memory counters for the process broken down by virtual memory areas (VMA). We can get dirty pages by finding:\nPrivate_Dirty – the amount of dirty data this process generated; Shared_Dirty – and the amount other processes wrote. This metric shows data only for referenced pages. It means the process should access pages and keep them in its page table (more details later). $ cat /proc/578097/smaps | grep file1.db -A 12 | grep Dirty Shared_Dirty: 0 kB Private_Dirty: 736 kB But what if we want to get the dirty page stats for a file? To answer this question linux kernel provides 2 files in procfs: /proc/PID/pagemap and /proc/kpageflags. I\u0026rsquo;m showing how to write our own tool with them later in the series, but for now we can use the debug tool from the linux kernel repo to get per file page info: page-types.\n$ sudo page-types -f /var/tmp/file1.db -b dirty flags page-count MB symbolic-flags long-symbolic-flags 0x0000000000000838 267 1 ___UDl_____M________________________________ uptodate,dirty,lru,mmap 0x000000000000083c 20 0 __RUDl_____M________________________________ referenced,uptodate,dirty,lru,mmap total 287 1 I filtered out all pages of our file /var/tmp/file1.db by the dirty flag. In the output, you can see that the file has 287 dirty pages or 1 MiB of dirty data, which will be persisted to storage eventually. page-type aggregates pages by flags, so that you can see 2 sets in the output. Both have the dirty flag D, and the difference between them is the presence of the referenced flag R (which I\u0026rsquo;m briefly touching on in the Page Cache eviction section later).\nSynchronize file changes with fsync(), fdatasync() and msync() # We already used sync (man 1 sync) to flush all dirty pages to disks before every test to get a fresh system without any interference. But what if we want to write a database management system, and we need to be sure that all writes will get to disks before a power outage or other hardware errors occur? For such cases, Linux provides several methods to force the kernel to run a sync of pages for the file in Page Cache:\nfsync() – blocks until all dirty pages of the target file and its metadata are synced; fdatasync() – the same as the above but excluding metadata; msync() – the same as the fsync() but for memory mapped file; open a file with O_SYNC or O_DSYNC flags to make all file writes synchronous by default and work as a corresponding fsync() and fdatasync() syscalls accordingly. NOTE\nYou still need to care about write barriers and understand how the underlying file system works because the kernel scheduler might reorder write operations. Usually, a file append operation is safe and can\u0026rsquo;t corrupt the previously written data. Other types of mutate operations may mess with your files (for instance, for ext4, even with the default journal). That\u0026rsquo;s why almost all database management systems like MongoDB, PostgreSQL, Etcd, Dgraph, etc, have write ahead logs (WAL) which are append-only. There are some exceptions though. If you\u0026rsquo;re curious more about this topic, this blog post from Dgraph is a good starting point.\nThere are some exceptions, though. For instance, lmdb (and its clones, bboltdb from etcd) uses a witty and clever idea of keeping two roots of its B+ tree and doing a copy-on-write.\nAnd here is an example of the file sync:\nimport os with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;br+\u0026#34;) as f: fd = f.fileno() os.fsync(fd) Checking file presence in Page Cache with mincore() # Before we go any further, let’s figure out how vmtouch manages to show us how many pages of a target file Page Cache contains.\nThe secret is a mincore() syscall (man 2 mincore). mincore() stands for \u0026ldquo;memory in the core\u0026rdquo;. Its parameters are a starting virtual memory address, a length of the address space and a resulting vector. mincore() works with memory (not files), so it can be used for checking if anonymous memory was swapped out.\nman 2 mincore\nmincore() returns a vector that indicates whether pages of the calling process\u0026rsquo;s virtual memory are resident in core (RAM), and so will not cause a disk access (pagefault) if referenced. The kernel returns residency information about the pages starting at the address addr, and continuing for length bytes.\nSo to replicate vmtouch we need to map a file into the virtual memory of the process, even though we are not going to make neither reads nor writes. We just want to have it in the process memory area (more about this later in mmap() section).\nNow we have all we need to write our own simple vmtouch in order to show cached pages by file path. I\u0026rsquo;m using go here because, unfortunately, Python doesn\u0026rsquo;t have an easy way to call mincore() syscall:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;syscall\u0026#34; \u0026#34;unsafe\u0026#34; ) var ( pageSize = int64(syscall.Getpagesize()) mode = os.FileMode(0600) ) func main() { path := \u0026#34;/var/tmp/file1.db\u0026#34; file, err := os.OpenFile(path, os.O_RDONLY|syscall.O_NOFOLLOW|syscall.O_NOATIME, mode) if err != nil { log.Fatal(err) } defer file.Close() stat, err := os.Lstat(path) if err != nil { log.Fatal(err) } size := stat.Size() pages := size / pageSize mm, err := syscall.Mmap(int(file.Fd()), 0, int(size), syscall.PROT_READ, syscall.MAP_SHARED) defer syscall.Munmap(mm) mmPtr := uintptr(unsafe.Pointer(\u0026amp;mm[0])) cached := make([]byte, pages) sizePtr := uintptr(size) cachedPtr := uintptr(unsafe.Pointer(\u0026amp;cached[0])) ret, _, err := syscall.Syscall(syscall.SYS_MINCORE, mmPtr, sizePtr, cachedPtr) if ret != 0 { log.Fatal(\u0026#34;syscall SYS_MINCORE failed: %v\u0026#34;, err) } n := 0 for _, p := range cached { // the least significant bit of each byte will be set if the corresponding page // is currently resident in memory, and be clear otherwise. if p%2 == 1 { n++ } } fmt.Printf(\u0026#34;Resident Pages: %d/%d %d/%d\\n\u0026#34;, n, pages, n*int(pageSize), size) } And if we run it:\n$ go run ./main.go Resident Pages: 1024/32768 4194304/134217728 And comparing it with vmtouch output:\n$ vmtouch /var/tmp/file1.db Files: 1 LOOK HERE Directories: 0 ⬇ Resident Pages: 1024/32768 4M/128M 3.12% Elapsed: 0.000804 seconds Read next chapter → "},{"id":19,"href":"/docs/resolver-dual-stack-application/6-dual-stack-applications/","title":"Dual-Stack applications","section":"DNS resolvers and Dual-Stack applications","content":" 6. Dual-Stack applications # Last updated: Oct 2025 Contents\n6.1 Dual stack server 6.1.1 IPV6_V6ONLY socket option 6.1.2 Multiple listening sockets with systemd 6.2 Dual stack client 6.2.1 Sorting destination addresses (RFC 6724) 6.2.2 Happy Eyeballs: Success with Dual-Stack Hosts Let’s now focus on dual-stack programs, which support both IPv4 and IPv6. Here are some critical questions to consider:\nFor server code:\nHow can we easily listen on all IPv4 and all IPv6 addresses? Do we need separate listeners for each? Are there any tools or helpers available to manage multiple listeners? For client code:\nWhich address family should our client program resolve and use: A, AAAA, or both? What to do if the resolver returns multiple addresses for each family? Does a machine have active IPv4 and IPv6 connectivity? Is the IPv6 routing configured correctly to the destination? In case of connection errors, which address should be used to reconnect? Ideally, we want to abstract away from these technical details to focus more on developing our core business logic, such as converting JSONs to protobufs and vice versa. 🙃\nTo address these questions effectively, we need to take a closer look at getaddrinfo(), its arguments, and its implementation details. Let’s dive in, starting with the server side.\n6.1 Dual stack server # Common sense suggests that to support both IPv4 and IPv6, we would need at least two listening sockets: one for each address family. However, just as we use the special IPv4 address 0.0.0.0 (also known as wildcard IPv4 or INADDR_ANY) to bind to all available IPv4 interfaces, there is a similar solution for IPv6. The IPv6 wildcard address, represented as :: or IN6ADDR_ANY_INIT, functions similarly by allowing binding to all interfaces and all IPv6 addresses. But it has one more feature that the IPv4 wildcard address is missing. The listener on :: address can handle IPv4 connections too, thanks to a special block of IPv6 addresses known as IPv4-mapped addresses.\nGood explanation of what are these IPv4-mapped addresses can be found in RFC 3493 Basic Socket Interface Extensions for IPv6:\n3.7 Compatibility with IPv4 Nodes\nThe API also provides a different type of compatibility: the ability for IPv6 applications to interoperate with IPv4 applications. This feature uses the IPv4-mapped IPv6 address format defined in the IPv6 addressing architecture specification [2]. This address format allows the IPv4 address of an IPv4 node to be represented as an IPv6 address. The IPv4 address is encoded into the low-order 32 bits of the IPv6 address, and the high-order 96 bits hold the fixed prefix 0:0:0:0:0:FFFF. IPv4-mapped addresses are written as follows:\n::FFFF:\u0026lt;IPv4-address\u0026gt; These addresses can be generated automatically by the getaddrinfo() function, as described in Section 6.1. Applications may use AF_INET6 sockets to open TCP connections to IPv4 nodes, or send UDP packets to IPv4 nodes, by simply encoding the destination\u0026rsquo;s IPv4 address as an IPv4-mapped IPv6 address, and passing that address, within a sockaddr_in6 structure, in the connect() or sendto() call. When applications use AF_INET6 sockets to accept TCP connections from IPv4 nodes, or receive UDP packets from IPv4 nodes, the system returns the peer\u0026rsquo;s address to the application in the accept(), recvfrom(), or getpeername() call using a sockaddr_in6 structure encoded this way.\ngetaddrinfo() in server code plays several roles:\nIt allows the use of hostnames to bind a socket if the node parameter of getaddrinfo() is set. It prepares the necessary data structures for use in socket API calls. It makes it possible to bind to all available and future addresses with minimal changes to the code. By leveraging these functionalities, getaddrinfo() simplifies the process of setting up network connections in server applications.\nBy leveraging these functionalities, getaddrinfo() simplifies the process of setting up network connections in server applications.\nServer code example:\n#define _POSIX_C_SOURCE 200112L #include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;netdb.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #define PORT \u0026#34;80\u0026#34; #define BACKLOG 10 void handle_client(int client_fd) { const char *response = \u0026#34;HTTP/1.0 200 OK\\r\\n\u0026#34; \u0026#34;Content-Type: text/html\\r\\n\u0026#34; \u0026#34;Connection: close\\r\\n\u0026#34; \u0026#34;\\r\\n\u0026#34; \u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;Hello, World!\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\\n\u0026#34;; send(client_fd, response, strlen(response), 0); shutdown(client_fd, SHUT_WR); close(client_fd); } int main() { struct addrinfo hints, *servinfo, *p; int sockfd; int yes = 1; int rv; memset(\u0026amp;hints, 0, sizeof hints); hints.ai_family = AF_INET6; // \u0026lt;--- ① hints.ai_socktype = SOCK_STREAM; hints.ai_flags = AI_PASSIVE; // \u0026lt;--- ② if ((rv = getaddrinfo(NULL /* ---\u0026gt; ③ \u0026lt;---*/, PORT, \u0026amp;hints, \u0026amp;servinfo)) != 0) { fprintf(stderr, \u0026#34;getaddrinfo: %s\\n\u0026#34;, gai_strerror(rv)); return 1; } // ---\u0026gt; ④ \u0026lt;--- for (p = servinfo; p != NULL; p = p-\u0026gt;ai_next) { if ((sockfd = socket(p-\u0026gt;ai_family, p-\u0026gt;ai_socktype, p-\u0026gt;ai_protocol)) == -1) { perror(\u0026#34;server: socket\u0026#34;); continue; } if (setsockopt(sockfd, SOL_SOCKET, SO_REUSEADDR, \u0026amp;yes, sizeof(int)) == -1) { perror(\u0026#34;setsockopt\u0026#34;); close(sockfd); return 1; } if (bind(sockfd, p-\u0026gt;ai_addr, p-\u0026gt;ai_addrlen) == -1) { close(sockfd); perror(\u0026#34;server: bind\u0026#34;); continue; } break; } if (p == NULL) { fprintf(stderr, \u0026#34;server: failed to bind\\n\u0026#34;); return 2; } freeaddrinfo(servinfo); // all done with this structure if (listen(sockfd, BACKLOG) == -1) { perror(\u0026#34;listen\u0026#34;); close(sockfd); return 1; } printf(\u0026#34;server: waiting for connections...\\n\u0026#34;); while (1) { struct sockaddr_storage client_addr; socklen_t addr_size = sizeof client_addr; int client_fd = accept(sockfd, (struct sockaddr *)\u0026amp;client_addr, \u0026amp;addr_size); if (client_fd == -1) { perror(\u0026#34;accept\u0026#34;); continue; } char name[INET6_ADDRSTRLEN]; char port[10]; getnameinfo((struct sockaddr *)\u0026amp;client_addr, addr_size, name, sizeof(name), // \u0026lt;--- ⑤ port, sizeof(port), NI_NUMERICHOST | NI_NUMERICSERV); printf(\u0026#34;client %s:%s\\n\u0026#34;, name, port); handle_client(client_fd); } close(sockfd); return 0; } ① – Set AF_INET6 family to allow handle IPv4 and IPv6 addresses.\n② – AI_PASSIVE flag asks getaddrinfo() to return a socket suitable for binding the socket that will accept connections.\n③ – NULL for node with the AI_PASSIVE flag uses either INADDR_ANY for IPv4 address, or IN6ADDR_ANY_INIT for IPv6 address.\n④ – The RFC 6724 (Default Address Selection for Internet Protocol Version 6 (IPv6)) clearly explains how to use the returned list of addresses in the applications:\nWell-behaved applications SHOULD NOT simply use the first address returned from an API such as getaddrinfo() and then give up if it fails. For many applications, it is appropriate to iterate through the list of addresses returned from getaddrinfo() until a working address is found. For other applications, it might be appropriate to try multiple addresses in parallel (e.g., with some small delay in between) and use the first one to succeed.\nThe last sentence concerns the client programs and will be discussed later.\nIn our example, we bind() and listen() only to the first successful address, but if you’re writing a real server, it may be necessary to listen to multiple addresses due to security concerns. Therefore, your code should create multiple listeners and accept connections on all of them simultaneously. ⑤ – using getnameinfo (man 3 getnameinfo) to convert the client address to human readable format.\nCompile and run:\n$ gcc ./server.c -o server \u0026amp;\u0026amp; ./server Check that the server listens on proper address family and port:\n$ sudo netstat -ntlp | grep 80 tcp6 0 0 :::80 :::* LISTEN 423492/./server Let’s run some requests in the new terminal:\n$ curl localhost # \u0026lt;--- ① \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;Hello, World!\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; $ curl 192.168.5.15 # \u0026lt;--- ② \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;Hello, World!\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; $ curl \u0026#34;[2001:db8:123:456:6af2:68fe:ff7c:e25c]\u0026#34; # \u0026lt;--- ③ \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;Hello, World!\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; $ curl --interface 127.0.0.1 localhost # \u0026lt;--- ④ \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;Hello, World!\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; $ curl \u0026#34;[fe80::5055:55ff:fe8e:3d07%eth0]\u0026#34; # \u0026lt;--- ⑤ \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;Hello, World!\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; and collect the output:\nserver: waiting for connections… client ::1:41646 # \u0026lt;--- ① client ::ffff:192.168.5.15:52236 # \u0026lt;--- ② client 2001:db8:123:456:6af2:68fe:ff7c:e25c:41098 # \u0026lt;--- ③ client ::ffff:127.0.0.1:35800 # \u0026lt;--- ④ client fe80::5055:55ff:fe8e:3d07%eth0:34328 # \u0026lt;--- ⑤ ① – connection to localhost transforms to ::1 client address.\n② – 192.168.5.15 is container local address, the log shows IPv4-mapped address.\n③ – 2001:db8:123:456:6af2:68fe:ff7c:e25 is a local container address with global scope.\n④ – using --interface we can force curl to use IPv4 source address for localhost.\nInternally curl uses bind() before connect() to set the source address:\n$ strace -e trace=network curl --interface 127.0.0.1 localhost … setsockopt(5, SOL_SOCKET, SO_BINDTODEVICE, \u0026#34;127.0.0.1\\0\u0026#34;, 10) = -1 ENODEV (No such device) setsockopt(5, SOL_IP, IP_BIND_ADDRESS_NO_PORT, [1], 4) = 0 bind(5, {sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr(\u0026#34;127.0.0.1\u0026#34;)}, 16) = 0 getsockname(5, {sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr(\u0026#34;127.0.0.1\u0026#34;)}, [128 =\u0026gt; 16]) = 0 connect(5, {sa_family=AF_INET, sin_port=htons(80), sin_addr=inet_addr(\u0026#34;127.0.0.1\u0026#34;)}, 16) = -1 EINPROGRESS (Operation now in progress) … ⑤ – if we’d like to use a link-local IPv6 address we need to use zone id and specify interface with % character.\n6.1.1 IPV6_V6ONLY socket option # You always can opt out of handling IPv4 addresses with IPv6. To do so you need to set the socket option IPV6_V6ONLY:\nIf this flag is set to true (nonzero), then the socket is restricted to sending and receiving IPv6 packets only. In this case, an IPv4 and an IPv6 application can bind to a single port at the same time.\nIf this flag is set to false (zero), then the socket can be used to send and receive packets to and from an IPv6 address or an IPv4-mapped IPv6 address. The argument is a pointer to a boolean value in an integer.\nThe default value for this flag is defined by the contents of the file /proc/sys/net/ipv6/bindv6only. The default value for that file is 0 (false).\n6.1.1.1 Nginx and Envoy (Envoyproxy) and IPV6_V6ONLY # For example, Nginx and Envoy (Envoyproxy) set this option by default on all IPv6 listeners. The justification is to be explicit and to avoid hiding the IPv4-mapped addresses. Additionally, creating a socket with IPV6_V6ONLY is the default for Windows.\nTherefore, for Nginx, if you need to listen on all interfaces, the default suggestion is to use two wildcard addresses:\nlisten [::]:80; listen 80; Request in logs:\n127.0.0.1 - - [07/Jul/2024:08:41:34 +0100] \u0026#34;GET / HTTP/1.1\u0026#34; 200 615 \u0026#34;-\u0026#34; \u0026#34;curl/8.5.0\u0026#34; ::1 - - [07/Jul/2024:08:41:45 +0100] \u0026#34;GET / HTTP/1.1\u0026#34; 200 615 \u0026#34;-\u0026#34; \u0026#34;curl/8.5.0\u0026#34; However you can change it back to Linux defaults:\nFor nginx by setting ipv6only=off for listen directive:\nlisten [::]:80 ipv6only=off; In logs now:\n::1 - - [07/Jul/2024:08:40:40 +0100] \u0026#34;GET / HTTP/1.1\u0026#34; 200 615 \u0026#34;-\u0026#34; \u0026#34;curl/8.5.0\u0026#34; ::ffff:127.0.0.1 - - [07/Jul/2024:08:40:48 +0100] \u0026#34;GET / HTTP/1.1\u0026#34; 200 615 \u0026#34;-\u0026#34; \u0026#34;curl/8.5.0\u0026#34; For envoy set the ipv4_compat for a listener:\nlisteners: - name: listener_0 address: socket_address: address: \u0026#34;::\u0026#34; port_value: 8080 ipv4_compat: true 6.1.1.2 Go (Golang) and IPV6_V6ONLY # Go (golang), on the other hand, decided to choose simplicity (as usual) and provide sane defaults. It doesn\u0026rsquo;t reset the Linux default (IPV6_V6ONLY is not set) and it silently transforms IPv4-mapped addresses to IPv4 addresses and treats them as equal.\nServer example:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { connStateHandler := func(conn net.Conn, state http.ConnState) { if state != http.StateNew { return } if addr, ok := conn.RemoteAddr().(*net.TCPAddr); ok { log.Printf(\u0026#34;connected: %#v, %s:%d\u0026#34;, addr.IP, addr.IP, addr.Port) } } server := \u0026amp;http.Server{ Addr: \u0026#34;:80\u0026#34;, ConnState: connStateHandler, Handler: http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { fmt.Fprintln(w, \u0026#34;Hello, this is the hardcoded response!\u0026#34;) }), } fmt.Println(\u0026#34;Starting server at :80\u0026#34;) if err := server.ListenAndServe(); err != nil { log.Fatalf(\u0026#34;Error starting server: %v\u0026#34;, err) } } Run it:\n$ go run ./main.go Check listening sockets, the server listens to IPv6 wildcard only:\n$ sudo netstat -ntlp | grep 80 tcp6 0 0 :::80 :::* LISTEN 426610/main Repeat the same exercise with curl and addresses from both families:\n$ curl localhost Hello, this is the hardcoded response! $ curl --interface 127.0.0.1 localhost Hello, this is the hardcoded response! $ curl 192.168.5.15 Hello, this is the hardcoded response! $ curl \u0026#34;[2001:db8:123:456:6af2:68fe:ff7c:e25c]\u0026#34; Hello, this is the hardcoded response! The result contains IPv6 and IPv4 address without IPv4-mapped addresses in string representation:\nStarting server at :80 connected: net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x1}, ::1:55540 connected: net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0x7f, 0x0, 0x0, 0x1}, 127.0.0.1:58108 connected: net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xc0, 0xa8, 0x5, 0xf}, 192.168.5.15:41442 connected: net.IP{0x20, 0x1, 0xd, 0xb8, 0x1, 0x23, 0x4, 0x56, 0x6a, 0xf2, 0x68, 0xfe, 0xff, 0x7c, 0xe2, 0x5c}, 2001:db8:123:456:6af2:68fe:ff7c:e25c:38752 But as you can see the binary representation of IP addresses are IPv4-mapped.\nInternally the To4() converts IPv4-mapped and real IPv4 to the same net.IP:\nhttps://cs.opensource.google/go/go/+/refs/tags/go1.22.5:src/net/ip.go;l=210-223\n// To4 converts the IPv4 address ip to a 4-byte representation. // If ip is not an IPv4 address, To4 returns nil. func (ip IP) To4() IP { if len(ip) == IPv4len { return ip } if len(ip) == IPv6len \u0026amp;\u0026amp; isZeros(ip[0:10]) \u0026amp;\u0026amp; ip[10] == 0xff \u0026amp;\u0026amp; ip[11] == 0xff { return ip[12:16] } return nil } Also String() method of net.IP does this transformation for us implicitly:\nhttps://cs.opensource.google/go/go/+/refs/tags/go1.22.5:src/net/ip.go;l=289-308\n// String returns the string form of the IP address ip. // It returns one of 4 forms: // - \u0026#34;\u0026lt;nil\u0026gt;\u0026#34;, if ip has length 0 // - dotted decimal (\u0026#34;192.0.2.1\u0026#34;), if ip is an IPv4 or IP4-mapped IPv6 address // - IPv6 conforming to RFC 5952 (\u0026#34;2001:db8::1\u0026#34;), if ip is a valid IPv6 address // - the hexadecimal form of ip, without punctuation, if no other cases apply func (ip IP) String() string { if len(ip) == 0 { return \u0026#34;\u0026lt;nil\u0026gt;\u0026#34; } if len(ip) != IPv4len \u0026amp;\u0026amp; len(ip) != IPv6len { return \u0026#34;?\u0026#34; + hexString(ip) } // If IPv4, use dotted notation. if p4 := ip.To4(); len(p4) == IPv4len { return netip.AddrFrom4([4]byte(p4)).String() } return netip.AddrFrom16([16]byte(ip)).String() } 6.1.1.3 Rust # If we make the same experiment with Rust, hyper and tokio:\nDependencies:\n[dependencies] hyper = { version = \u0026#34;0.14\u0026#34;, features = [\u0026#34;full\u0026#34;] } tokio = { version = \u0026#34;1\u0026#34;, features = [\u0026#34;full\u0026#34;] } Code:\nuse hyper::service::{make_service_fn, service_fn}; use hyper::{Body, Request, Response, Server}; use std::convert::Infallible; use std::net::SocketAddr; use std::sync::Arc; use tokio::sync::Mutex; // Shared state for logging struct Logger { // Here you can add additional shared state if needed } impl Logger { fn log(\u0026amp;self, addr: \u0026amp;SocketAddr) { println!(\u0026#34;Received request from {}:{}, is ipv4: {}\u0026#34;, addr.ip(), addr.port(), addr.is_ipv4()); } } async fn handle_request( req: Request\u0026lt;Body\u0026gt;, logger: Arc\u0026lt;Mutex\u0026lt;Logger\u0026gt;\u0026gt;, remote_addr: SocketAddr, ) -\u0026gt; Result\u0026lt;Response\u0026lt;Body\u0026gt;, Infallible\u0026gt; { // Log the remote address { let logger = logger.lock().await; logger.log(\u0026amp;remote_addr); } Ok(Response::new(Body::from(\u0026#34;Hello, world!\u0026#34;))) } #[tokio::main] async fn main() { // Define the address to listen on (IPv6 address :: and port 3000) let addr = ([0, 0, 0, 0, 0, 0, 0, 0], 3000).into(); // Create a logger instance let logger = Arc::new(Mutex::new(Logger {})); // Create a service that handles requests let make_svc = make_service_fn(move |conn: \u0026amp;hyper::server::conn::AddrStream| { let logger = logger.clone(); let remote_addr = conn.remote_addr(); async move { Ok::\u0026lt;_, Infallible\u0026gt;(service_fn(move |req| { handle_request(req, logger.clone(), remote_addr) })) } }); // Create the server with the specified address and service let server = Server::bind(\u0026amp;addr).serve(make_svc); // Run the server println!(\u0026#34;Listening on http://[::]:3000\u0026#34;); if let Err(e) = server.await { eprintln!(\u0026#34;Server error: {}\u0026#34;, e); } } And run it:\n$ cargo run Send some requests with curl:\n$ curl \u0026#34;[fe80::5055:55ff:fe8e:3d07%eth0]:3000\u0026#34; $ curl 192.168.5.15:3000 $ curl 127.0.0.1:3000 We get the following logs:\nReceived request from fe80::5055:55ff:fe8e:3d07:44388, is ipv4: false Received request from ::ffff:192.168.5.15:49918, is ipv4: false Received request from ::ffff:127.0.0.1:42254, is ipv4: false where you can see that IPv4-mapped addresses are shown as is, and the is_ipv4() method returns false for them.\n6.1.2 Multiple listening sockets with systemd # One more side note about multiple listener sockets.\nThere are many cases when an application can’t use a wildcard address due to security concerns and should bind to a number of specific addresses, regardless of the address family. In such cases, you don’t have many options other than creating and managing multiple accept sockets. If you have such a task, consider looking into the systemd socket activation feature, which can simplify socket creation and provide all the benefits of systemd.\nGo server example which gets listeners directly from systemd:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;net\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/coreos/go-systemd/v22/activation\u0026#34; ) func handleConnection(conn net.Conn) { defer conn.Close() buf := make([]byte, 1024) for { n, err := conn.Read(buf) if err != nil { fmt.Println(\u0026#34;Error reading from connection:\u0026#34;, err) return } fmt.Printf(\u0026#34;Received: %s\\n\u0026#34;, string(buf[:n])) conn.Write([]byte(\u0026#34;Hello from Go!\\n\u0026#34;)) } } func main() { // Retrieve sockets from systemd listeners, err := activation.Listeners() if err != nil { fmt.Fprintf(os.Stderr, \u0026#34;Error retrieving listeners: %v\\n\u0026#34;, err) os.Exit(1) } if len(listeners) == 0 { fmt.Fprintln(os.Stderr, \u0026#34;No sockets to listen on.\u0026#34;) os.Exit(1) } // Listen for connections on each socket for _, listener := range listeners { go func(l net.Listener) { for { conn, err := l.Accept() if err != nil { fmt.Println(\u0026#34;Error accepting connection:\u0026#34;, err) continue } go handleConnection(conn) } }(listener) } // Block the main goroutine to keep the program running select {} } Socket file /etc/systemd/system/go-service.socket:\n[Unit] Description=Sockets for Go service [Socket] ListenStream=127:0.0.1:12345 ListenStream=192.168.1.1:23456 ListenStream=[fe80::5055:55ff:fe8e:3d07]:34567 [Install] WantedBy=sockets.target systemd service unit file /etc/systemd/system/go-service.service:\n[Unit] Description=Your Go service After=network.target [Service] ExecStart=/path/to/your/go/program NonBlocking=true [Install] WantedBy=multi-user.target Both above files should have the same name before the dot! Reload and start the service and the socket:\n$ sudo systemctl daemon-reload $ sudo systemctl enable go-service.socket $ sudo systemctl start go-service.socket 6.2 Dual stack client # Dual stack client applications are usually more complex than the server side in terms of address agnosticism. In server code getaddrinfo() calls can be reasonably slow or even omitted, with sockets created manually (which is not a good practice). But, client code often needs to re-resolve hostnames periodically. For instance, a load balancer needs to understand if there are new backend updates from a service discovery. One of the issues arising from this is the overhead of performing an additional DNS request for an unconfigured network stack (unfortunately, this is often still IPv6), which might add unnecessary delays and increase DNS traffic without any benefits.\nAnother complexity lies behind getaddrinfo(). Its calls are blocking (we will look at asynchronous alternatives in the next chapter). The blocking nature leads to the following issues:\nIf you set the AF_UNSPEC family, which you probably should in order to be dual stack and IPv6 ready, you could wait longer if your resolver is periodically slow. This issue arises from the design of getaddrinfo(), which always waits for two answers for A and AAAA queries in this case. Even if one answer has already arrived, it will wait until the arrival of the second one or until a timeout form /etc/resolv.conf. Avoid setting AF_UNSPEC and instead make two asynchronous calls with some logic and timeouts on top, though this can be cumbersome and difficult to do properly. The AI_ADDRCONFIG hints flag is intended to help mitigate such issues and reduce traffic and latency. However, there is an important caveat with its heuristics. First, let’s read the section of man page for getaddrinfo():\nIf hints.ai_flags includes the AI_ADDRCONFIG flag, then IPv4 addresses are returned in the list pointed to by res only if the local system has at least one IPv4 address configured, and IPv6 addresses are returned only if the local system has at least one IPv6 address configured. The loopback address is not considered for this case as valid as a configured address.\nThe exception is made only for loopback addresses. Any other IPv6 addresses are considered valid, even for link-local scoped addresses, which are configured automatically when a link is up.\nThe conclusion is not favorable; the AI_ADDRCONFIG flag could potentially be useful only with IPv6-only hosts with a fully disabled IPv4 stack (because autoconfigured IPv4 addresses are also valid addresses) or IPv4 only.\nBut thanks to RFC 6724’s sorting algorithm (discussed in the next chapter) – which includes Rule 2: Prefer matching scope in Section 6 – the return order will prefer IPv4 over IPv6 if there are no global scope IPv6 source addresses on the device. Which we already saw with our getaddrinfo() experiments in Chapter 3.\nThe AI_ADDRCONFIG is enabled by default (man 3 getaddrinfo):\nSpecifying hints as NULL is equivalent to setting ai_socktype and ai_protocol to 0; ai_family to AF_UNSPEC; and ai_flags to (AI_V4MAPPED | AI_ADDRCONFIG).\nwhich violates POSIX, but helps to improve user experience:\nAccording to POSIX.1, specifying hints as NULL should cause ai_flags to be assumed as 0. The GNU C library instead assumes a value of (AI_V4MAPPED | AI_ADDRCONFIG) for this case, since this value is considered an improvement on the specification.\nIf we look under the hood, how getaddrinfo() manages to figure out existing addresses, we will see the__check_pf() function. It has a cache for the responses, and in case of a cache miss or a stale entry, it connects to NETLINK socket and queries routing information, which is parsed in the make_request() function afterwords to find an answer:\nvoid attribute_hidden __check_pf (bool *seen_ipv4, bool *seen_ipv6, struct in6addrinfo **in6ai, size_t *in6ailen) { … if (cache_valid_p ()) { data = cache; … } else { int fd = __socket (PF_NETLINK, SOCK_RAW | SOCK_CLOEXEC, NETLINK_ROUTE); … data = make_request (fd, nladdr.nl_pid); … } … if (data != NULL) { /* It worked. */ *seen_ipv4 = data-\u0026gt;seen_ipv4; *seen_ipv6 = data-\u0026gt;seen_ipv6; *in6ailen = data-\u0026gt;in6ailen; *in6ai = data-\u0026gt;in6ai; … return; } /* We cannot determine what interfaces are available. Be pessimistic. */ *seen_ipv4 = true; *seen_ipv6 = true; } Another somewhat confusing getaddrinfo() hints flags is AI_V4MAPPED. RFC 3493 Basic Socket Interface Extensions for IPv6 where getaddrinfo() was introduced explains it a bit:\nIf the AI_V4MAPPED flag is specified along with an ai_family of AF_INET6, then getaddrinfo() shall return IPv4-mapped IPv6 addresses on finding no matching IPv6 addresses (ai_addrlen shall be 16).\nFor example, when using the DNS, if no AAAA records are found then a query is made for A records and any found are returned as IPv4-mapped IPv6 addresses.\nThe AI_V4MAPPED flag shall be ignored unless ai_family equals AF_INET6.\nIf the AI_ALL flag is used with the AI_V4MAPPED flag, then getaddrinfo() shall return all matching IPv6 and IPv4 addresses.\nFor example, when using the DNS, queries are made for both AAAA records and A records, and getaddrinfo() returns the combined results of both queries. Any IPv4 addresses found are returned as IPv4-mapped IPv6 addresses.\nThus the AI_V4MAPPED could help write a tool which wants to deal only with addresses in IPv6 format.\nFor IPv6-only hosts with no IPv4 connectivity, there is no routing for IPv4-mapped addresses because they are a special case in the IPv6 address space. If you need to route such addresses, you can, for instance, set up NAT64. So, we have addresses now, but in what order should we try to connect to them? As usual, there is no silver bullet (we will take a look at Nginx and Envoy, which address this issue from different angles). However, there are two main views on this problem:\nConsider all addresses returned for a hostname as different ways to communicate with the same entity (it could be a multi-address host, service, website, etc.). Sometimes it’s also called a multihomed host:\nA multihomed host is a computer or device that is connected to two or more networks and can be identified by multiple IP addresses, typically associated with different network interfaces. This configuration is common in scenarios requiring redundancy, load balancing, or enhanced security. Connecting to any of the returned addresses ends up at the same entity. For example, if a host has both an IPv4 and an IPv6 address and our client is dual-stack, it doesn’t matter via which protocol a connection is established from the user’s point of view.\nConsider addresses returned as different entities under one domain name. This is now a Service Discovery via DNS rather than exploring different paths. For example StatefulSet pod names in Kubernetes in DNS.\nThe second option is kind of straightforward, and we will not talk about it much in this series. The client side code should implement a load balancer algorithm, use all the records as its backends, periodically refresh the records, and filter out some suspicious addresses (for example, link-local scope addresses).\nBut the first option has more subtle details that we need to discuss. Let’s start with the sorting problem.\n6.2.1 Sorting destination addresses (RFC 6724) # As we already saw and know, a hostname could be resolved into multiple A and/or AAAA addresses.\nThe issue is in which order to iterate through all of them (basically in which order getaddrinfo() or its alternatives return the result).\nLet’s not forget that the original goal of introducing IPv6 addresses was to migrate to them as soon as possible. Over time, this has evolved to a goal of eventually migrating. With this in mind, it makes sense to prioritize returning IPv6 addresses first, if they exist, and IPv4 addresses next. This approach was standardized in RFC 3484 and later updated in RFC 6724: Default Address Selection for Internet Protocol Version 6 (IPv6). These RFCs provide two algorithms for address selection: one for source address selection (which occurs inside the Linux kernel and won’t be covered in detail here) and the destination address selection algorithm described in Section 6 of the document.\nThus, in order to seamlessly and smoothly migrate a dual-stack application from IPv4 to IPv6, it should follow RFC 6724 and behave well, where:\nWell-behaved applications SHOULD NOT simply use the first address returned from an API such as getaddrinfo() and then give up if it fails. For many applications, it is appropriate to iterate through the list of addresses returned from getaddrinfo() until a working address is found. For other applications, it might be appropriate to try multiple addresses in parallel (e.g., with some small delay in between) and use the first one to succeed.\nSo the algorithm is the following:\nflowchart TD Rule1[Rule 1: Avoid unusable destinations] Rule2[Rule 2: Prefer matching scope] Rule3[Rule 3: Avoid deprecated addresses] Rule4[Rule 4: Prefer home addresses] Rule5[Rule 5: Prefer matching label] Rule6[Rule 6: Prefer higher precedence] Rule7[Rule 7: Prefer native transport] Rule8[Rule 8: Prefer smaller scope] Rule9[Rule 9: Use longest matching prefix] Rule10[Rule 10: Otherwise, leave the order unchanged] Rule1 --\u003e Rule2 --\u003e Rule3 --\u003e Rule4 --\u003e Rule5 Rule5 --\u003e Rule6 --\u003e Rule7 --\u003e Rule8 --\u003e Rule9 --\u003e Rule10 Figure 3. – Destination address selection algorithm RFC 6724\nLet me provide a quick explanation with examples. To sort addresses, a comparator function is used. This function takes two addresses and determines which one wins based on a series of rules. If the first rule results in a tie, the next rule is applied, and so on.\nThe algorithm employs terms introduced in various RFCs, including:\naddress scopes; IPv6 address types; policy table with a configuration file. But before we start, let’s take a closer look at each of these components.\nThe IPv6 addressing architecture [RFC4291] allows multiple unicast addresses to be assigned to interfaces. These addresses might have different reachability scopes (link-local, site-local, or global).\nHere we need to quickly remind ourselves what scopes are there:\nhost scope – ::1 and 127.0.0.1 link-local scope – fe80::/10 and 169.254.0.0/16 unique Local Address (ULA) scope – fc00::/7 global scope – all other addresses (for IPv6 usually starts with 2000::/3), including IPv4 private networks such as 192.168.0.0/16, 172.16.0.0/12 and 10.0.0.0/8. These addresses might also be \u0026ldquo;preferred\u0026rdquo; or \u0026ldquo;deprecated\u0026rdquo; [RFC4862]. Privacy considerations have introduced the concepts of \u0026ldquo;public addresses\u0026rdquo; and \u0026ldquo;temporary addresses\u0026rdquo; [RFC4941]. The mobility architecture introduces \u0026ldquo;home addresses\u0026rdquo; and \u0026ldquo;care-of addresses\u0026rdquo; [RFC6275].\nPolicy table is an auxiliary data structure to help sort source and destination addresses. It has a default value:\nPrefix Precedence Label ::1/128 50 0 ::/0 40 1 ::ffff:0:0/96 35 4 2002::/16 30 2 2001::/32 5 5 fc00::/7 3 13 ::/96 1 3 fec0::/10 1 11 3ffe::/16 1 12 and could be tuned by changing /etc/gai.conf (man 5 gai.conf).\n2.1. Policy Table\nThe policy table is a longest-matching-prefix lookup table, much like a routing table. Given an address A, a lookup in the policy table produces two values: a precedence value denoted Precedence(A) and a classification or label denoted Label(A).\nThe precedence value Precedence(A) is used for sorting destination addresses. If Precedence(A) \u0026gt; Precedence(B), we say that address A has higher precedence than address B, meaning that our algorithm will prefer to sort destination address A before destination address B.\nThe label value Label(A) allows for policies that prefer a particular source address prefix for use with a destination address prefix. The algorithms prefer to use a source address S with a destination address D if Label(S) = Label(D).\nAlso the destination address selection algorithm needs to know a source address for the destination address. We will review how a stub resolver can obtain thin information later in this chapter.\nSource Address Selection The source address selection algorithm produces as output a single source address for use with a given destination address. This algorithm only applies to IPv6 destination addresses, not IPv4 addresses.\nNow we can review the rules with examples.\nRule 1 Avoid unusable destinations.\nIf a stub resolver knows that a destination address is unreachable, it should deprioritize (pessimize) it.\nExample: if a stub resolver has a cache and/or health checks mechanism, or history of connections, it can immediately pessimize an address.\nRule 2: Prefer matching scope.\nRetrieve source addresses for the destination addresses under consideration and compare the scopes of each pair. If the scopes match for a pair, prioritize that destination address.\nExample: DNS returned 203.0.113.1 and 2001:0db8:0:1::1. The system has 192.168.0.2 and a link-local IPv6 address. The result will be [203.0.113.1, 2001:0db8:0:1::1] because 192.168.0.2 and 203.0.113.1 are both global scoped addresses.\nRule 3: Avoid deprecated addresses.\nRetrieve source addresses for the destination addresses under consideration. If a source address is marked as deprecated for a pair, deprioritize (pessimize) the corresponding destination address.\nRule 4: Prefer home addresses.\nIt’s related to mobile home and care-of addresses. A home address should win.\nRule 5: Prefer matching label.\nRetrieve source addresses for the destination addresses under consideration. If a pair has equal labels in the policy table, prioritize (bump) the related destination address.\nExample: DNS returned 2002:c633:6401::1 or 2001:db8:1::1. The source address for the first is 2002:c633:6401::2 and for the second is fe80::2. The result is [2002:c633:6401::1, 2001:db8:1::1] because label is the same for a pair 2002:c633:6401::1 and 2002:c633:6401::2 and is 1.\nRule 6: Prefer higher precedence.\nPerform the same steps as above, but compare the Precedence values of the two destination addresses only.\nExample: DNS returned 203.0.113.1 and 2001:0db8:0:1::1. The result is [ 2001:0db8:0:1::1, 203.0.113.1]. The precedence for 2001:0db8:0:1::1 is 40, and for 203.0.113.1 is 35.\nRule 7: Prefer native transport.\nAlways prefer a non-encapsulated destination address.\nRule 8: Prefer smaller scope.\nCompare the scopes of the destination addresses under consideration and select the one with the smallest scope.\nExample: DNS returned 2001:0db8:0:1::1 and fe80::2. The result is [fe80::2, 2001:0db8:0:1::1]. The link local scope is smaller.\nRule 9: Use longest matching prefix.\nRetrieve source addresses for the destination addresses under consideration and compare the longest matching prefixes between each pair.\nExample: DNS returned 2001:db8:1::1 and 2001:db8:3ffe::1. Sources are 2001:db8:1::2 and 2001:db8:3f44::2. The result is [2001:db8:1::1, 2001:db8:3ffe::1]. The longest matching prefix wins for the pair of 2001:db8:1::1 and 2001:db8:1::2.\nRule 10: Otherwise, leave the order unchanged.\nPreserve the order provided by the DNS server, allowing for possible Round-robin DNS.\nOne important implication from the above is that IPv6 usually takes precedence over IPv4 due to the higher default precedence in the policy table (man 5 gai.conf). If you do not want this behavior, you can change it by adding the following line to /etc/gai.conf:\nprecedence ::ffff:0:0/96 100 This line sets a precedence of 100 for the IPv4-mapped range. This adjustment is likely the only practical use of /etc/gai.conf since distributing changes across multiple machines is difficult and unreliable. However, be aware that not all stub resolvers read this configuration file.\n6.2.1.1 Retrieve Source address for Destination address # Let’s now explore the details of how to obtain a source address for a given destination.\nThe algorithm is using a feature of connect (man 2 connect) syscall for SOCK_DGRAM (UDP) sockets:\nIf the socket sockfd is of type SOCK_DGRAM, then addr is the address to which datagrams are sent by default, and the only address from which datagrams are received.\nBut what is more interesting for us is that we can run getsockname (man 2 getsockname) afterwards for the file descriptor and get the source address for the destination:\ngetsockname() returns the current address to which the socket sockfd is bound, in the buffer pointed to by addr.\nAll code lives for getaddrinfo() of glibc here:\n/* We overwrite the type with SOCK_DGRAM since we do not want connect() to connect to the other side. If we cannot determine the source address remember this fact. */ if (fd == -1 || (af == AF_INET \u0026amp;\u0026amp; q-\u0026gt;ai_family == AF_INET6)) { if (fd != -1) __close_nocancel_nostatus (fd); af = q-\u0026gt;ai_family; fd = __socket (af, SOCK_DGRAM | SOCK_CLOEXEC, IPPROTO_IP); } else { /* Reset the connection. */ struct sockaddr sa = { .sa_family = AF_UNSPEC }; __connect (fd, \u0026amp;sa, sizeof (sa)); } if (try_connect (\u0026amp;fd, \u0026amp;af, \u0026amp;results[i].source_addr, q-\u0026gt;ai_addr, q-\u0026gt;ai_addrlen, q-\u0026gt;ai_family)) { results[i].source_addr_len = sizeof (results[i].source_addr); results[i].got_source_addr = true; The results array is used next to sort:\n/* We got all the source addresses we can get, now sort using the information. */ struct sort_result_combo src = { .results = results, .nresults = nresults }; if (__glibc_unlikely (gaiconf_reload_flag_ever_set)) { __libc_lock_define_initialized (static, lock); __libc_lock_lock (lock); if (__libc_once_get (old_once) \u0026amp;\u0026amp; gaiconf_reload_flag) gaiconf_reload (); __qsort_r (order, nresults, sizeof (order[0]), rfc3484_sort, \u0026amp;src); __libc_lock_unlock (lock); } else __qsort_r (order, nresults, sizeof (order[0]), rfc3484_sort, \u0026amp;src); Where rfc3484_sort contains all 10 rules from the RFC.\nstatic int rfc3484_sort (const void *p1, const void *p2, void *arg) The retrieval of source addresses involves at least two syscalls for each destination address. This overhead should be considered if performance is crucial. For example, the alternative stub resolver c-ares (which we will review later) allows the option to disable sorting by RFC 6724.\nIf we run the above program under strace we will see all the calls:\n$ strace -f -s0 -e trace=network ./getaddrinfo microsoft.com … ① socket(AF_INET6, SOCK_DGRAM|SOCK_CLOEXEC, IPPROTO_IP) = 3 ② connect(3, {sa_family=AF_INET6, sin6_port=htons(53), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \u0026#34;2603:1030:b:3::152\u0026#34;, \u0026amp;sin6_addr), sin6_scope_id=0}, 28) = 0 ③ getsockname(3, {sa_family=AF_INET6, sin6_port=htons(47754), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \u0026#34;2001:db8:123:456:6af2:68fe:ff7c:e25c\u0026#34;, \u0026amp;sin6_addr), sin6_scope_id=0}, [28]) = 0 ④ connect(3, {sa_family=AF_UNSPEC, sa_data=\u0026#34;\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\u0026#34;}, 16) = 0 … ① – create a UDP socket to connect to the destination;\n② – connect to it;\n③ – obtaining the source address;\n④ – reset socket by using connect() to AF_UNSPEC.\n6.2.2 Happy Eyeballs: Success with Dual-Stack Hosts # We have yet to address a very crucial issue: the reliability of IPv6 network routing in the real world. This problem is so significant that it has been a major obstacle to migration, leading to the development of new RFCs to promote IPv6 adoption.\nFrom the RFC 6555 Happy Eyeballs: Success with Dual-Stack Hosts:\nIn order to use applications over IPv6, it is necessary that users enjoy nearly identical performance as compared to IPv4. A combination of today\u0026rsquo;s applications, IPv6 tunneling, IPv6 service providers, and some of today\u0026rsquo;s content providers all cause the user experience to suffer. For IPv6, a content provider may ensure a positive user experience by using a DNS white list of IPv6 service providers who peer directly with them. However, this does not scale well (to the number of DNS servers worldwide or the number of content providers worldwide) and does react to intermittent network path outages.\nTo addresses this issue, let’s revisit the following quote from RFC 6724: Default Address Selection for Internet Protocol Version 6 (IPv6) first:\nWell-behaved applications SHOULD NOT simply use the first address returned from an API such as getaddrinfo() and then give up if it fails. For many applications, it is appropriate to iterate through the list of addresses returned from getaddrinfo() until a working address is found. For other applications, it might be appropriate to try multiple addresses in parallel (e.g., with some small delay in between) and use the first one to succeed.\nThe last sentence essentially captures the gist of the Happy Eyeballs algorithm: when it is impossible to determine the state of the network in advance, but the system is configured correctly with both global scope IP families, the only option is to use both families concurrently, with some preference given to IPv6.\nThere are two RFCs about Happy Eyeballs algorithm:\nRFC 6555 Happy Eyeballs: Success with Dual-Stack Hosts (obsoleted by RFC 8305 ↓). RFC 8305 Happy Eyeballs Version 2: Better Connectivity Using Concurrency The motivation for the emergence of the above RFCs was the desire to create a standardized algorithm that prioritizes IPv6 addresses. This was necessary to combat the variety of existing algorithms that did not prioritize IPv6, thereby failing to encourage infrastructure upgrades and reduce reliance on outdated IPv4 hardware. Additionally, making numerous concurrent connections simultaneously can harm the network by overloading network equipment, routers, and servers. As stated in RFC 6555:\nInstead, applications reduce connection setup delays themselves, by more aggressively making connections on IPv6 and IPv4. There are a variety of algorithms that can be envisioned. This document specifies requirements for any such algorithm, with the goals that the network and servers not be inordinately harmed with a simple doubling of traffic on IPv6 and IPv4 and the host\u0026rsquo;s address preference be honored.\nAlgorithm Requirements A \u0026ldquo;Happy Eyeballs\u0026rdquo; algorithm has two primary goals:\nProvides fast connection for users, by quickly attempting to connect using IPv6 and (if that connection attempt is not quickly successful) to connect using IPv4.\nAvoids thrashing the network, by not (always) making simultaneous connection attempts on both IPv6 and IPv4\nBy following these not-always-simple rules, you can create a client application that is reliable, flexible, and predictable.\nIt’s time to look at alternative stub resolvers and examples of real high load software.\nRead next chapter → "},{"id":20,"href":"/docs/resolver-dual-stack-application/7-async-non-blocking-resolvers-in-c/","title":"Async non-blocking resolvers in C","section":"DNS resolvers and Dual-Stack applications","content":" 7. Async non-blocking resolvers in C # Last updated: Oct 2025 Contents\n7.1 getaddrinfo_a() 7.2 c-ares library 7.2.1 Essentials 7.2.1 Dual stack application Now that we’ve covered the essential theory, let’s explore alternative stub resolver libraries and frameworks for the C language. Other languages will be discussed next, but don’t skip this chapter, as it contains foundational information that will be referenced later.\n7.1 getaddrinfo_a() # getaddrinfo_a (man 7 getaddrinfo_a) is an asynchronous version of getaddrinfo() but with some limitations: results can be collected by polling or notified by a signal.\nInternally it creates a thread for each request using a thread pool:\n/* Now try to start a thread. If we fail, no big deal, because we know that there is at least one thread (us) that is working on lookup operations. */ if (__pthread_create (\u0026amp;thid, \u0026amp;attr, handle_requests, NULL) == 0) ++nthreads; and running original getaddrinfo() inside:\nreq-\u0026gt;__return = getaddrinfo (req-\u0026gt;ar_name, req-\u0026gt;ar_service, req-\u0026gt;ar_request, \u0026amp;req-\u0026gt;ar_result); The notification mechanism is not perfectly suitable for the present-day high performance application development. Its callbacks are implemented either via POSIX signal handler, or by creating a new waiting thread and running a caller’s callback function on it.\nThe only user I could find on github is pgbouncer (lightweight connection pooler for PostgreSQL):\n7.2 c-ares library # It should now be clear that neither getaddrinfo() nor getaddrinfo_a() can meet all the needs of modern software development, where asynchronous and non-blocking code is an essential building block. Such code enables great performance by wisely utilizing system resources. This is why the curl developers forked the ares library some time ago and continue to maintain an advanced stub resolver framework called c-ares.\nc-ares is a modern DNS (stub) resolver library, written in C. It provides interfaces for asynchronous queries while trying to abstract the intricacies of the underlying DNS protocol. It was originally intended for applications which need to perform DNS queries without blocking, or need to perform multiple DNS queries in parallel.\nc-ares likely has the most features of any alternative stub resolver library I’ve encountered. The list of supported RFCs is massive and comprehensive.\nUnlike other resolvers, c-ares doesn’t use signals for notifications nor does it spawn threads to resolve domain names. Instead, it leverages non-blocking sockets and the epoll (man 7 epoll) syscall on Linux. This makes it an ideal candidate for integration into other epoll-based modern programs.\nSome of the most significant users of c-ares include curl, NodeJS, and Envoy.\nc-ares, like getaddrinfo(), uses getenv(), which could potentially lead to a segmentation fault in a multithreaded environment if there is a concurrent setenv() call.\u0026quot; 7.2.1 Essentials # The configuration step happening during the initialization process of a communications channel (ares_init_options) allows to change the basic resolver behavior. Here are some important options that could divergence from the glibc getaddrinfo():\nARES_FLAG_NOSEARCH – don’t use the search domain from /etc/resolv.conf and always ask a nameserver as is. ARES_OPT_TIMEOUTMS, ARES_OPT_TRIES, ARES_OPT_DOMAINS which overwrite the settings from /etc/resolv.conf. ARES_OPT_RESOLVCONF, ARES_OPT_HOSTS_FILE – alternative paths for /etc/resolv.conf and /etc/hosts. ARES_OPT_SERVER_FAILOVER – an equivalent of rotate option from /etc/resolv.conf. This is just a small compatibility gist with /etc/resolv.conf and libc getaddrinfo(), but the full list of options contains much more settings.\nThe resolve functions are:\nares_gethostbyname() – an alternative for gethostbyname()(it’s not deprecated); ares_getaddrinfo() – an equivalent for getaddrinfo(). Version 1.28.0 of c-ares made a significant change in how it reads system configuration files. This is important to mention because Ubuntu 24.04 LTS includes c-ares version 1.27 in its repository. Therefore, in this chapter, we will be using version 1.27. The default c-ares initialization process for resolving order is a bit different from the glibc getaddrinfo().\nFigure 4. – c-ares 1.27 init process of reading default sources. ① – at first it reads /etc/nsswitch.conf to determine the order of sources. It only supports files, dns and resolve (systemd-resolved, we will touch it later in the series).\n② – if /etc/host.conf exists, overwrite the order with its content. This is important to know because if your distribution of GNU/Linux still has the /etc/host.conf (man 5 host.conf) file it could cost you some time to troubleshoot this behavior.\n③ – if /etc/svc.conf exists, overwrite the order with its content.\nThis behavior changed in version 1.28, where the old deprecated system files /etc/host.conf and /etc/svc.conf were dropped.\u0026quot; Let’s finally write an example with c-ares:\n/* Example from https://c-ares.org/docs.html */ #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;ares.h\u0026gt; /* Callback that is called when DNS query is finished */ static void addrinfo_cb(void *arg, int status, int timeouts, struct ares_addrinfo *result) { (void)arg; /* Example does not use user context */ printf(\u0026#34;Result: %s, timeouts: %d\\n\u0026#34;, ares_strerror(status), timeouts); if (result) { struct ares_addrinfo_node *node; for (node = result-\u0026gt;nodes; node != NULL; node = node-\u0026gt;ai_next) { char addr_buf[64] = \u0026#34;\u0026#34;; const void *ptr = NULL; if (node-\u0026gt;ai_family == AF_INET) { const struct sockaddr_in *in_addr = (const struct sockaddr_in *)((void *)node-\u0026gt;ai_addr); ptr = \u0026amp;in_addr-\u0026gt;sin_addr; } else if (node-\u0026gt;ai_family == AF_INET6) { const struct sockaddr_in6 *in_addr = (const struct sockaddr_in6 *)((void *)node-\u0026gt;ai_addr); ptr = \u0026amp;in_addr-\u0026gt;sin6_addr; } else { continue; } ares_inet_ntop(node-\u0026gt;ai_family, ptr, addr_buf, sizeof(addr_buf)); printf(\u0026#34;Addr: %s\\n\u0026#34;, addr_buf); } } ares_freeaddrinfo(result); } int main(int argc, char **argv) { ares_channel_t *channel = NULL; struct ares_options options; int optmask = 0; struct ares_addrinfo_hints hints; if (argc != 2) { printf(\u0026#34;Usage: %s domain\\n\u0026#34;, argv[0]); return 1; } /* Initialize library */ ares_library_init(ARES_LIB_INIT_ALL); if (!ares_threadsafety()) { printf(\u0026#34;c-ares not compiled with thread support\\n\u0026#34;); return 1; } /* Enable event thread so we don\u0026#39;t have to monitor file descriptors */ memset(\u0026amp;options, 0, sizeof(options)); optmask |= ARES_OPT_EVENT_THREAD; options.evsys = ARES_EVSYS_DEFAULT; /* Initialize channel to run queries, a single channel can accept unlimited * queries */ if (ares_init_options(\u0026amp;channel, \u0026amp;options, optmask) != ARES_SUCCESS) { printf(\u0026#34;c-ares initialization issue\\n\u0026#34;); return 1; } /* Perform an IPv4 and IPv6 request for the provided domain name */ memset(\u0026amp;hints, 0, sizeof(hints)); hints.ai_family = AF_UNSPEC; hints.ai_flags = ARES_AI_CANONNAME; ares_getaddrinfo(channel, argv[1], NULL, \u0026amp;hints, addrinfo_cb, NULL /* user context not specified */); /* Wait until no more requests are left to be processed */ ares_queue_wait_empty(channel, -1); /* Cleanup */ ares_destroy(channel); ares_library_cleanup(); return 0; } Install dependencies:\n$ sudo apt install libc-ares-dev $ # I use aarch64 if you\u0026#39;re not please change it $ gcc -L/usr/lib/aarch64-linux-gnu/ -I/usr/lib ./resolver.c -o resolver -lcares If we run it with strace, we can see all the above config files in the output:\n$ strace -e openat ./resolver micrisoft.com openat(AT_FDCWD, \u0026#34;/etc/ld.so.cache\u0026#34;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026#34;/lib/aarch64-linux-gnu/libcares.so.2\u0026#34;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026#34;/lib/aarch64-linux-gnu/libc.so.6\u0026#34;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026#34;/etc/resolv.conf\u0026#34;, O_RDONLY) = 3 openat(AT_FDCWD, \u0026#34;/etc/nsswitch.conf\u0026#34;, O_RDONLY) = 3 openat(AT_FDCWD, \u0026#34;/etc/host.conf\u0026#34;, O_RDONLY) = 3 openat(AT_FDCWD, \u0026#34;/etc/svc.conf\u0026#34;, O_RDONLY) = -1 ENOENT (No such file or directory) As you can see my cloud image of Ubuntu 24.04 has /etc/host.conf file:\n$ cat /etc/host.conf # The \u0026#34;order\u0026#34; line is only used by old versions of the C library. order files bind multi on Another important observation from the strace output is there is no /etc/gai.conf. So there is no way to change the sorting rules by providing a custom policy table.\n7.2.1 Dual stack application # Due to the subtle logic of AI_ADDRCONFIG heuristics,the c-ares developers decided not to bring it.\nThus if you set AF_UNSPEC flag, two DNS requests for A and AAAA will be sended and two address families will be returned (if of course there is anything to return).\nIt is worth mentioning, though, that thanks to RFC 6724’s sorting algorithm—which includes Rule 2: Prefer matching scope in Section 6—the return order will prefer IPv4 over IPv6 if there are no global scope IPv6 source addresses on the device.\nWith IPv6 global scope address on a machine:\n$ ./resolver microsoft.com Result: Successful completion, timeouts: 0 Addr: 2603:1030:b:3::152 Addr: 2603:1030:20e:3::23c Addr: 2603:1030:c02:8::14 Addr: 2603:1020:201:10::10f Addr: 2603:1010:3:3::5b Addr: 20.76.201.171 Addr: 20.231.239.246 Addr: 20.70.246.20 Addr: 20.112.250.133 Addr: 20.236.44.162 And without:\n$ ./resolver microsoft.com Result: Successful completion, timeouts: 0 Addr: 20.70.246.20 Addr: 20.236.44.162 Addr: 20.231.239.246 Addr: 20.76.201.171 Addr: 20.112.250.133 Addr: 2603:1030:c02:8::14 Addr: 2603:1010:3:3::5b Addr: 2603:1030:b:3::152 Addr: 2603:1020:201:10::10f Addr: 2603:1030:20e:3::23c Sorting code:\n/* Rule 2: Prefer matching scope. */ scope_src1 = ARES_IPV6_ADDR_SCOPE_NODELOCAL; if (a1-\u0026gt;has_src_addr) { scope_src1 = get_scope(\u0026amp;a1-\u0026gt;src_addr.sa); } scope_dst1 = get_scope(a1-\u0026gt;ai-\u0026gt;ai_addr); scope_match1 = (scope_src1 == scope_dst1); scope_src2 = ARES_IPV6_ADDR_SCOPE_NODELOCAL; if (a2-\u0026gt;has_src_addr) { scope_src2 = get_scope(\u0026amp;a2-\u0026gt;src_addr.sa); } scope_dst2 = get_scope(a2-\u0026gt;ai-\u0026gt;ai_addr); scope_match2 = (scope_src2 == scope_dst2); if (scope_match1 != scope_match2) { return scope_match2 - scope_match1; } If we run the code under strace with network trace, we see how it performs the same socket calls as getaddrinfo() does in order to retrieve a source address for the destination:\n[pid 209162] socket(AF_INET6, SOCK_DGRAM, IPPROTO_UDP) = 7 [pid 209162] connect(7, {sa_family=AF_INET6, sin6_port=htons(0), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \u0026#34;2603:1030:b:3::152\u0026#34;, \u0026amp;sin6_addr), sin6_scope_id=0}, 28) = 0 [pid 209162] getsockname(7, {sa_family=AF_INET6, sin6_port=htons(54532), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \u0026#34;fec0::5055:55ff:fe8e:3d07\u0026#34;, \u0026amp;sin6_addr), sin6_scope_id=0}, [28]) = 0 [pid 209162] close(7) But notice, if you disable sorting by setting ARES_AI_NOSORT flag in our above code by change hints.ai_flags to:\nhints.ai_flags = ARES_AI_NOSORT; it always returns IPv4 addresses first. However, I’m not sure if this order is guaranteed or if it’s due to the order of DNS requests, with the A request being sent before the AAAA. Typically, without network reordering, the answers also arrive in this order.\n$ ./resolver microsoft.com Result: Successful completion, timeouts: 0 Addr: 20.231.239.246 Addr: 20.70.246.20 Addr: 20.76.201.171 Addr: 20.112.250.133 Addr: 20.236.44.162 Addr: 2603:1030:c02:8::14 Addr: 2603:1020:201:10::10f Addr: 2603:1010:3:3::5b Addr: 2603:1030:b:3::152 Addr: 2603:1030:20e:3::23c Without a fully functional alternative to the AI_ADDRCONFIG flag, using the AF_UNSPEC family can result in unnecessary work if there is no IPv6 routing on the system, as it will still issue AAAA DNS requests. However, these calls could be amortized by the built-in c-ares’ cache. If this isn’t sufficient, you have at least two options, though they add complexity:\nWrite your own IPv4/IPv6 routable addresses/stack detection.\nResolve both families just in case but make per address family DNS calls asynchronously. You can add deadline logic and use the fastest answer with the Happy Eyeballs algorithm. This approach is suggested in RFC 8305 Happy Eyeballs Version 2: Better Connectivity Using Concurrency:\nImplementations SHOULD NOT wait for both families of answers to return before attempting connection establishment. If one query fails to return or takes significantly longer to return, waiting for the second address family can significantly delay the connection establishment of the first one. Therefore, the client SHOULD treat DNS resolution as asynchronous. Note that if the platform does not offer an asynchronous DNS API, this behavior can be simulated by making two separate synchronous queries on different threads, one per address family.\nRead next chapter → "},{"id":21,"href":"/docs/page-cache/4-page-cache-eviction-and-page-reclaim/","title":"Page Cache eviction and page reclaim","section":"Linux Page Cache series","content":" Page Cache eviction and page reclaim # Last updated: Oct 2025 Contents\nEssesntial theory Manual pages eviction with POSIX_FADV_DONTNEED Make your memory unevictable Page Cache, vm.swappiness and modern kernels Understanding memory reclaim process with /proc/pid/pagemap page-types kernel page tool Writing Page Cache LRU monitor tool So far, we have talked about adding data to Page Cache by reading and writing files, checking the existence of files in the cache, and flushing the cache content manually. But the most crucial part of any cache system is its eviction policy, or regarding Linux Page Cache, it\u0026rsquo;s also the memory page reclaim policy. Like any other cache, Linux Page Cache continuously monitors the last used pages and makes decisions about which pages should be deleted and which should be kept in the cache.\nThe primary approach to control and tune Page Cache is the cgroup subsystem. You can divide the server\u0026rsquo;s memory into several smaller caches (cgroups) and thus control and protect applications and services. In addition, the cgroup memory and IO controllers provide a lot of statistics that are useful for tuning your software and understanding the internals of the cache.\nTheory # Linux Page Cache is closely tightened with Linux Memory Management, cgroup and virtual file system (VFS). So, in order to understand how eviction works, we need to start with some basic internals of the memory reclaim policy. Its core building block is a per cgroup pair of active and inactive lists:\nthe first pair for anonymous memory (for instance, allocated with malloc() or not file backended mmap()); the second pair for Page Cache file memory (all file operations including read(), write, filemmap() accesses, etc.). The former is exactly what we are interested in. This pair is what linux uses for Page Cache evection process. The least recently used algorithm LRU is the core of each list. In turn, these 2 lists form a double clock data structure. In general, Linux should choose pages that have not been used recently (inactive) based on the fact that the pages that have not been used recently will not be used frequently in a short period of time. It is the basic idea of ​​the LRU algorithm. Both the active and the inactive lists adopt the form of FIFO (First In First Out) for their entries. New elements are added to the head of the linked list, and the elements in between gradually move toward the end. When memory reclamation is needed, the kernel always selects the page at the end of the inactive list for releasing. The following figure shows the simplified concept of the idea:\nFor example, the system starts with the following content of the lists. A user process has just read some data from disks. This action triggered the kernel to load data to the cache. It was the first time when the kernel had to access the file. Hence it added a page h to the head of the inactive list of the process\u0026rsquo;s cgroup:\nSome time has passed, and the system loads 2 more pages: i and j to the inactive list and accordingly has to evict pages a and b from it. This action also shifts all pages toward the tail of the inactive LRU list, including our page h:\nNow, a new file operation to the page h promotes the page to the active LRU list by putting it at the head. This action also ousts the page 1 to the head of the inactive LRU list and shifts all other members.\nAs time flies, page h looses its head position in the active LRU list.\nBut a new file access to the h\u0026rsquo;s position in the file returns h back to the head of the active LRU list.\nThe above figures show the simplified version of the algorithm.\nBut it\u0026rsquo;s worth mentioning that the real process of pages promotion and demotion is much more complicated and sophisticated.\nFirst of all, if a system has NUMA hardware nodes (man 8 numastat), it has twice more LRU lists. The reason is that the kernel tries to store memory information in the NUMA nodes in order to have fewer lock contentions.\nIn addition, Linux Page Cache also has special shadow and referenced flag logic for promotion, demotion and re-promotion pages.\nShadow entries help to mitigate the memory thrashing problem. This issue happens when the programs\u0026rsquo; working set size is close to or greater than the real memory size (maybe cgroup limit or the system RAM limitation). In such a situation, the reading pattern may evict pages from the inactive list before the subsequent second read request has appeared. The full idea is described in the mm/workingset.c and includes calculating refault distance. This distance is used to judge whether to put the page from the shadow entries immediately to the active LRU list.\nAnother simplification I made was about PG_referenced page flag. In reality, the page promotion and demotion use this flag as an additional input parameter in the decision algorithm. A more correct flow of the page promotion:\nflowchart LR A[Inactive LRU,\\nunreferenced] --\u003e B[Inactive LRU,\\nreferenced] B --\u003e C[Active LRU,\\nunreferenced] C --\u003e D[Active LRU,\\nreferenced] Manual pages eviction with POSIX_FADV_DONTNEED # I\u0026rsquo;ve already shown how to drop all Page Cache entries using /proc/sys/vm/drop_caches file. But what if we want for some reason to clear the cache for a file?\nEXAMPLE\nIt could sometimes be useful to evict a file from the cache in a real life situation. Assume we want to test how fast our MongoDB gets back to optimal condition after a system reboot. You can stop a replica, clean all its files from Page Cache and start it back.\nvmtouch already can do that. Its -e flag commands the kernel to evict all pages of the requested file from Page Cache:\nFor example:\n$ vmtouch /var/tmp/file1.db -e Files: 1 Directories: 0 Evicted Pages: 32768 (128M) Elapsed: 0.000704 seconds $ vmtouch /var/tmp/file1.db Files: 1. LOOK HERE Directories: 0 ⬇ Resident Pages: 0/32768 0/128M 0% Elapsed: 0.000566 seconds Let\u0026rsquo;s look under the hood and figure out how it works. In order to write our own tool we need to use the already seen posix_fadvise syscall with the POSIX_FADV_DONTNEED option.\nCode:\nimport os with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;br\u0026#34;) as f: fd = f.fileno() os.posix_fadvise(fd, 0, os.fstat(fd).st_size, os.POSIX_FADV_DONTNEED) For testing, I read the entire test file into Page Cache with dd:\n$ dd if=/var/tmp/file1.db of=/dev/null 262144+0 records in 262144+0 records out 134217728 bytes (134 MB, 128 MiB) copied, 0.652248 s, 206 MB/s $ vmtouch /var/tmp/file1.db Files: 1 LOOK HERE Directories: 0 ⬇ Resident Pages: 32768/32768 128M/128M 100% Elapsed: 0.002719 seconds And now, after running our script, we should see 0 pages in Page Cache:\n$ python3 ./evict_full_file.py $ vmtouch /var/tmp/file1.db Files: 1 LOOK HERE Directories: 0 ⬇ Resident Pages: 0/32768 0/128M 0% Elapsed: 0.000818 seconds Make your memory unevictable # But what if you want to force the kernel to keep your file memory in Page Cache, no matter what. It is called making the file memory unevictable .\nEXAMPLE\nSometimes you have to force the kernel to give you a 100% guarantee that your files will not be evicted from the memory. You can want it even with modern linux kernels and properly configured cgroup limits, which should keep the working data set in Page Cache. For example, due to issues with other processes on the system where you share disk and network IO. Or, for instance, because of an outage of a network attached storage.\nKernel provides a bunch of syscalls for doing that: mlock(), mlock2() and mlockall(). As with the mincore(), you must map the file first.\nmlock2() is a preferable syscall for Page Cache routines because it has the handy flag MLOCK_ONFAULT:\nLock pages that are currently resident and mark the entire range so that the remaining nonresident pages are locked when they are populated by a page fault.\nAnd don\u0026rsquo;t forget about limits (man 5 limits.conf). You likely need to increased it:\n$ ulimit -l 64 And finally, to get the amount of unevictable memory, please, check the cgroup memory controller stats for your cgroup:\n$ grep unevictable /sys/fs/cgroup/user.slice/user-1000.slice/session-3.scope/memory.stat unevictable 0 Page Cache, vm.swappiness and modern kernels # Now that we understand the basic reclaiming theory with 4 LRU lists (for anon and file memory) and evictable/unevictable types of memory, we can talk about the sources to refill the system free memory. The kernel constantly maintains lists of free pages for itself and user-space needs. If such lists get below the threshold, the linux kernel starts to scan LRU lists in order to find pages to reclaim. It allows the kernel to keep memory in some equilibrium state.\nThe Page Cache memory is usually evictable memory (with some rare mlock() exceptions). And thus, it maybe look obvious, that Page Cache should be the first and the only option for the memory eviction and reclaiming. Since disks already have all that data, right? But fortunately or unfortunately, in real production situations, this is not always the best choice.\nIf the system has swap (and it should have it with modern kernels), the kernel has one more option. It can swap out the anonymous (not file-backed) pages. It may seem counterintuitive, but the reality is that sometimes user-space daemons can load tons of initialization code and never use it afterward. Some programs, especially statically built, for example, could have a lot of functionality in their binaries that may be used only several times in some edge cases. In all such situations, there is not much sense in keeping them in valuable memory.\nSo, in order to control which inactive LRU list to prefer for scans, the kernel has the sysctl vm.swappiness knob.\n$ sudo sysctl -a | grep swap vm.swappiness = 60 There are a lot of blog posts, stories and forum threads about this magic setting. On top of that, the legacy cgroup v1 memory subsystem has its own per cgroup swappiness knob. All this makes information about the current vm.swappiness meaning hard to understand and change. But let me try to explain some recent changes and give you fresh links.\nFirst of all, by default vm.swappiness is set 60, the min is 0 and the max is 200:\n/* * From 0 .. 200. Higher means more swappy. */ int vm_swappiness = 60; The 100 value means that the kernel considers anonymous and Page Cache pages equally in terms of reclamation.\nSecondly, the cgroup v2 memory controller doesn\u0026rsquo;t have the swappiness knob at all:\n#ifdef CONFIG_MEMCG static inline int mem_cgroup_swappiness(struct mem_cgroup *memcg) { /* Cgroup2 doesn\u0026#39;t have per-cgroup swappiness */ if (cgroup_subsys_on_dfl(memory_cgrp_subsys)) return vm_swappiness; /* root ? */ if (mem_cgroup_disabled() || mem_cgroup_is_root(memcg)) return vm_swappiness; return memcg-\u0026gt;swappiness; Instead, the kernel developers decided to change the swappiness logic significantly. You can check it if you run git blame on mm/vmscan.c and search for the get_scan_count() function.\nFor example, at the time of writing, the anonymous memory will not be touched regardless of vm.swappiness if the inactive LRU Page Cache list has a decent amount of pages:\n/* * If there is enough inactive page cache, we do not reclaim * anything from the anonymous working right now. */ if (sc-\u0026gt;cache_trim_mode) { scan_balance = SCAN_FILE; goto out; } The full logic of making decisions about what and from which LRU to reclaim, you can find in the get_scan_count()function in mm/vmscan.c.\nAlso, please take a look at the memory.swap.high and the memory.swap.max cgroup v2 settings. You can control them if you want to correct the vm.swappiness logic for your cgroup and load pattern.\nAnother interesting topic, which you should keep in mind when dealing with the swap and Page Cache, is the IO load during the swapping in/out processes. If you have IO pressure, you can easily hit your IO limits and, for example, degrade your Page Cache writeback performance.\nUnderstanding memory reclaim process with /proc/pid/pagemap # Now it\u0026rsquo;s time for low level troubleshooting technics.\nThere is a /proc/PID/pagemap file that contains the page table information of the PID. The page table, basically speaking, is an internal kernel map between page frames (real physical memory pages stored in RAM) and virtual pages of the process. Each process in the linux system has its own virtual memory address space which is completely independent form other processes and physical memory addresses.\nThe full /proc/PID/pagemap and related file documentation, including data formats and ways to read, it can be found in the kernel documentation folder. I strongly recommend that you read it before proceeding to the sections below.\npage-types kernel page tool # page-types is the Swiss knife of every kernel memory hacker. Its source code comes with the Linux kernel sources tools/vm/page-types.c.\nIf you didn\u0026rsquo;t install it in the first chapter:\n$ wget https://github.com/torvalds/linux/archive/refs/tags/v5.13.tar.gz $ tar -xzf ./v5.13.tar.gz $ cd v5.13/vm/tools $ make Now let\u0026rsquo;s use it in order to understand how many pages of our test file /var/tmp/file1.db the kernel has placed in Active and Inactive LRU lists:\n$ sudo ./page-types --raw -Cl -f /var/tmp/file1.db foffset cgroup offset len flags /var/tmp/file1.db Inode: 133367 Size: 134217728 (32768 pages) Modify: Mon Aug 30 13:14:19 2021 (13892 seconds ago) Access: Mon Aug 30 13:15:47 2021 (13804 seconds ago) 10689 @1749 21fa 1 ___U_lA_______________________P____f_____F_1 ... 18965 @1749 24d37 1 ___U_l________________________P____f_____F_1 18966 @1749 28874 1 ___U_l________________________P____f_____F_1 18967 @1749 10273 1 ___U_l________________________P____f_____F_1 18968 @1749 1f6ad 1 ___U_l________________________P____f_____F_1 flags page-count MB symbolic-flags long-symbolic-flags 0xa000010800000028 105 0 ___U_l________________________P____f_____F_1 uptodate,lru,private,softdirty,file,mmap_exclusive 0xa00001080000002c 16 0 __RU_l________________________P____f_____F_1 referenced,uptodate,lru,private,softdirty,file,mmap_exclusive 0xa000010800000068 820 3 ___U_lA_______________________P____f_____F_1 uptodate,lru,active,private,softdirty,file,mmap_exclusive 0xa001010800000068 1 0 ___U_lA_______________________P____f_I___F_1 uptodate,lru,active,private,softdirty,readahead,file,mmap_exclusive 0xa00001080000006c 16 0 __RU_lA_______________________P____f_____F_1 referenced,uptodate,lru,active,private,softdirty,file,mmap_exclusive total 958 3 The output contains 2 sections: the first one provides per-page information, and the second aggregates all pages with the same flags and counts the summary. What we need from the output in order to answer to the LRU question are A and l flags, which, as you can guess, stand for \u0026ldquo;active\u0026rdquo; and \u0026ldquo;inactive\u0026rdquo; lists.\nAs you can see, we have:\n105 + 16 = 121 pages or 121 * 4096 = 484 KiB in inactive LRU list. 820 + 1 + 16 = 837 pages or 837 * 4096 = 3.2 MiB in active LRU list. Writing Page Cache LRU monitor tool # page-types is a really useful tool for low-level debugging and investigations, but its output format is hard to read and aggregate. I promised earlier that we would write our own vmtouch, so now we\u0026rsquo;re creating it. Our alternative version will provide even more information about pages. It will show not only how many pages are in Page Cache, but also how many of them are in the active and the inactive LRU lists.\nTo do this, we need 2 kernel files: /proc/PID/pagemap and /proc/kpageflags.\nThe full code you can find in the github repo, however here, I would like to focus on a few important moments:\n... ① err = syscall.Madvise(mm, syscall.MADV_RANDOM) ... ② ret, _, err := syscall.Syscall(syscall.SYS_MINCORE, mmPtr, sizePtr, cachedPtr) for i, p := range cached { ③ if p%2 == 1 { ④ _ = *(*int)(unsafe.Pointer(mmPtr + uintptr(pageSize*int64(i)))) } } ... ⑤ err = syscall.Madvise(mm, syscall.MADV_SEQUENTIAL) ... ① – Here, we need to disable the read ahead logic for the target file in order to protect ourselves from loading (charging) unneeded data to Page Cache by our tool; ② – Use mincore() syscall to get a vector of the pages in Page Cache; ③ – Here, we check whether a page is in Page Cache or not; ④ – If Page Cache contains a page, we need to update the corresponding process\u0026rsquo;s page table entry by referencing this page. Our tool has to do this in order to use the /proc/pid/pagemap. Otherwise the /proc/pid/pagemap file will not contain the target file pages and thus their flags. ⑤ – Here, we are turning off the harvesting of reference bits. This is required due to kernel reclaim logic. Our tool read memory and hence influences the kerne LRU lists. By using madvise() with MADV_SEQUENTIAL we notify linux kernel to ignore our operations. Let\u0026rsquo;s test our tool. We need 2 terminals. In the first one, start our tool with watch (man 1 watch) to run our tool every 100ms in an infinitive loop:\nwatch -n 0.1 \u0026#39;sudo go run ./lru.go\u0026#39; And in the second terminal, we will read the file with the dd (man 1 dd):\ndd if=/var/tmp/file1.db of=/dev/null Demo of what you should see:\nUsing the above approach, you can now perform low-level Page Cache investigations.\nRead next chapter → "},{"id":22,"href":"/docs/resolver-dual-stack-application/8-stub-resolvers-in-languages/","title":"Stub resolvers in languages","section":"DNS resolvers and Dual-Stack applications","content":" 8. Stub resolvers in languages # Last updated: Oct 2025 Contents\n8.1 Python 8.1.1 Stub resolvers 8.1.2 Happy Eyeballs 8.2 Go (golang) 8.2.1 Resolver 8.2.2 Happy Eyeballs 8.3 Rust 8.3.1 Resolver 8.3.2 Happy Eyeballs 8.4 Java and Netty 8.4.1 Resolver 8.4.2 Java SecurityManager 8.4.3 Happy eyeballs 8.5 Node.js 8.5.1 Resoler 8.5.2 Happy Eyeballs Let’s now take a look at other popular languages and understand the capabilities, features and options they provide in the context of resolvers.\n8.1 Python # We are going to talk about cpython 3.12.\n8.1.1 Stub resolvers # The Python standard library provides socket.getaddrinfo():\nsocket.getaddrinfo(host, port, family=0, type=0, proto=0, flags=0) which internally calls libc getaddrinfo():\n/* Python interface to getaddrinfo(host, port). */ /*ARGSUSED*/ static PyObject * socket_getaddrinfo(PyObject *self, PyObject *args, PyObject* kwargs) { ... memset(\u0026amp;hints, 0, sizeof(hints)); hints.ai_family = family; hints.ai_socktype = socktype; hints.ai_protocol = protocol; hints.ai_flags = flags; Py_BEGIN_ALLOW_THREADS error = getaddrinfo(hptr, pptr, \u0026amp;hints, \u0026amp;res0); ... } There are also socket.gethostbyname() and socket.gethostbyname_ex(), but they both don\u0026rsquo;t support IPv6.\nThe latest significant paradigm shift in Python was the adoption of async functions. The async standard framework includes loop.getaddrinfo(), which provides asynchronous DNS resolution capabilities:\nimport asyncio import socket async def resolve_hostname(hostname): loop = asyncio.get_running_loop() addresses = await loop.getaddrinfo(hostname, None, proto=socket.IPPROTO_TCP) return addresses addresses = asyncio.run(resolve_hostname(\u0026#39;microsoft.com\u0026#39;)) which internally shows the same behavior as getaddrinfo():\nstrace -f -s0 python3 ./main.py Open config files:\n[pid 49937] openat(AT_FDCWD, \u0026#34;/etc/nsswitch.conf\u0026#34;, O_RDONLY|O_CLOEXEC) = 6 [pid 49937] openat(AT_FDCWD, \u0026#34;/etc/host.conf\u0026#34;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) [pid 49937] openat(AT_FDCWD, \u0026#34;/etc/resolv.conf\u0026#34;, O_RDONLY|O_CLOEXEC) = 6 [pid 49937] openat(AT_FDCWD, \u0026#34;/etc/hosts\u0026#34;, O_RDONLY|O_CLOEXEC) = 6 [pid 49937] openat(AT_FDCWD, \u0026#34;/etc/gai.conf\u0026#34;, O_RDONLY|O_CLOEXEC) = 6 Obtaining source addresses:\n[pid 49955] connect(6, {sa_family=AF_UNSPEC, sa_data=\u0026#34;\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\u0026#34;}, 16) = 0 [pid 49955] connect(6, {sa_family=AF_INET6, sin6_port=htons(0), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \u0026#34;2603:1030:20e:3::23c\u0026#34;, \u0026amp;sin6_addr), sin6_scope_id=0}, 28) = 0 [pid 49955] getsockname(6, {sa_family=AF_INET6, sin6_port=htons(33582), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \u0026#34;2001:db8:123:456:6af2:68fe:ff7c:e25c\u0026#34;, \u0026amp;sin6_addr), sin6_scope_id=0}, [ 28]) = 0 So we can guess that under the hood it still runs socket.getaddrinfo():\nasync def getaddrinfo(self, host, port, *, family=0, type=0, proto=0, flags=0): if self._debug: getaddr_func = self._getaddrinfo_debug else: getaddr_func = socket.getaddrinfo return await self.run_in_executor( None, getaddr_func, host, port, family, type, proto, flags) The socket.getaddrinfo() is running in a thread execution pool.\nSuch a solution could show performance issues under high load, that’s why there are plenty of third party libraries. But I’d like to show the aiodns. Internally it uses pycares which is a Python interface for c-ares.\nimport aiodns import asyncio import socket async def resolve_hostname(hostname): resolver = aiodns.DNSResolver() result = await resolver.gethostbyname(hostname, socket.AF_UNSPEC) return result addresses = asyncio.run(resolve_hostname(\u0026#39;microsoft.com\u0026#39;)) print(addresses.addresses) Opening system files:\n$ strace -e openat python3 ./dns.py … openat(AT_FDCWD, \u0026#34;/etc/resolv.conf\u0026#34;, O_RDONLY) = 6 openat(AT_FDCWD, \u0026#34;/etc/nsswitch.conf\u0026#34;, O_RDONLY) = 6 openat(AT_FDCWD, \u0026#34;/etc/host.conf\u0026#34;, O_RDONLY) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026#34;/etc/svc.conf\u0026#34;, O_RDONLY) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026#34;/etc/hosts\u0026#34;, O_RDONLY) = 6 … Gather source addresses:\n$ strace -f -e trace=network python3 ./dns.py socket(AF_INET6, SOCK_DGRAM, IPPROTO_UDP) = 7 connect(7, {sa_family=AF_INET6, sin6_port=htons(0), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \u0026#34;2603:1030:b:3::152\u0026#34;, \u0026amp;sin6_addr), sin6_scope_id=0}, 28) = 0 getsockname(7, {sa_family=AF_INET6, sin6_port=htons(58701), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \u0026#34;2001:db8:123:456:6af2:68fe:ff7c:e25c\u0026#34;, \u0026amp;sin6_addr), sin6_scope_id=0}, [28]) = 0 socket(AF_INET6, SOCK_DGRAM, IPPROTO_UDP) = 7 connect(7, {sa_family=AF_INET6, sin6_port=htons(0), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \u0026#34;2603:1030:20e:3::23c\u0026#34;, \u0026amp;sin6_addr), sin6_scope_id=0}, 28) = 0 getsockname(7, {sa_family=AF_INET6, sin6_port=htons(43168), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \u0026#34;2001:db8:123:456:6af2:68fe:ff7c:e25c\u0026#34;, \u0026amp;sin6_addr), sin6_scope_id=0}, [28]) = 0 socket(AF_INET6, SOCK_DGRAM, IPPROTO_UDP) = 7 connect(7, {sa_family=AF_INET6, sin6_port=htons(0), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \u0026#34;2603:1010:3:3::5b\u0026#34;, \u0026amp;sin6_addr), sin6_scope_id=0}, 28) = 0 getsockname(7, {sa_family=AF_INET6, sin6_port=htons(36319), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \u0026#34;2001:db8:123:456:6af2:68fe:ff7c:e25c\u0026#34;, \u0026amp;sin6_addr), sin6_scope_id=0}, [28]) = 0 socket(AF_INET6, SOCK_DGRAM, IPPROTO_UDP) = 7 connect(7, {sa_family=AF_INET6, sin6_port=htons(0), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \u0026#34;2603:1020:201:10::10f\u0026#34;, \u0026amp;sin6_addr), sin6_scope_id=0}, 28) = 0 getsockname(7, {sa_family=AF_INET6, sin6_port=htons(42078), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \u0026#34;2001:db8:123:456:6af2:68fe:ff7c:e25c\u0026#34;, \u0026amp;sin6_addr), sin6_scope_id=0}, [28]) = 0 socket(AF_INET6, SOCK_DGRAM, IPPROTO_UDP) = 7 connect(7, {sa_family=AF_INET6, sin6_port=htons(0), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \u0026#34;2603:1030:c02:8::14\u0026#34;, \u0026amp;sin6_addr), sin6_scope_id=0}, 28) = 0 getsockname(7, {sa_family=AF_INET6, sin6_port=htons(43403), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \u0026#34;2001:db8:123:456:6af2:68fe:ff7c:e25c\u0026#34;, \u0026amp;sin6_addr), sin6_scope_id=0}, [28]) = 0 8.1.2 Happy Eyeballs # The async loop.create_connection() allows to use the Happy Eyeballs algorithm from RFC 8305. The code is in in Lib/asyncio/base_events.py:\nif happy_eyeballs_delay is None: # not using happy eyeballs … else: # using happy eyeballs sock, _, _ = await staggered.staggered_race( (functools.partial(self._connect_sock, exceptions, addrinfo, laddr_infos) for addrinfo in infos), happy_eyeballs_delay, loop=self) The params of loop.create_connection()are:\nhappy_eyeballs_delay, if given, enables Happy Eyeballs for this connection. It should be a floating-point number representing the amount of time in seconds to wait for a connection attempt to complete, before starting the next attempt in parallel. This is the \u0026ldquo;Connection Attempt Delay\u0026rdquo; as defined in RFC 8305. A sensible default value recommended by the RFC is 0.25 (250 milliseconds).\ninterleave – controls address reordering when a host name resolves to multiple IP addresses. If 0 or unspecified, no reordering is done, and addresses are tried in the order returned by getaddrinfo(). If a positive integer is specified, the addresses are interleaved by address family, and the given integer is interpreted as \u0026ldquo;First Address Family Count\u0026rdquo; as defined in RFC 8305. The default is 0 if happy_eyeballs_delay is not specified, and 1 if it is.\n8.2 Go (golang) # 8.2.1 Resolver # According to the documentation golang has two components to resolve domain names:\nOn Unix systems, the resolver has two options for resolving names.\nIt can use a pure Go resolver that sends DNS requests directly to the servers listed in /etc/resolv.conf, or it can use a cgo-based resolver that calls C library routines such as getaddrinfo and getnameinfo.\nOn Unix the pure Go resolver is preferred over the cgo resolver, because a blocked DNS request consumes only a goroutine, while a blocked C call consumes an operating system thread.\nWhen cgo is available, the cgo-based resolver is used instead under a variety of conditions: on systems that do not let programs make direct DNS requests (OS X), when the LOCALDOMAIN environment variable is present (even if empty), when the RES_OPTIONS or HOSTALIASES environment variable is non-empty, when the ASR_CONFIG environment variable is non-empty (OpenBSD only), when /etc/resolv.conf or /etc/nsswitch.conf specify the use of features that the Go resolver does not implement.\nOn all systems (except Plan 9), when the cgo resolver is being used this package applies a concurrent cgo lookup limit to prevent the system from running out of system threads. Currently, it is limited to 500 concurrent lookups.\nThe resolver decision can be overridden by setting the netdns value of the GODEBUG environment variable (see package runtime) to go or cgo, as in:\nexport GODEBUG=netdns=go # force pure Go resolver export GODEBUG=netdns=cgo # force native resolver (cgo, win32) Let’s create an example client resolver:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;net\u0026#34; \u0026#34;os\u0026#34; ) func main() { if len(os.Args) \u0026lt; 2 { fmt.Println(\u0026#34;Usage: go run main.go \u0026lt;domain\u0026gt;\u0026#34;) return } domain := os.Args[1] ips, err := net.LookupIP(domain) if err != nil { fmt.Printf(\u0026#34;Could not get IPs: %v\\n\u0026#34;, err) return } for _, ip := range ips { fmt.Println(ip.String()) } } Build it with debug info:\n$ go build -gcflags \u0026#34;all=-N -l\u0026#34; -o resolver ./main.go Run:\n$ ./resolver.go microsoft.com 2603:1020:201:10::10f 2603:1030:b:3::152 2603:1030:20e:3::23c 2603:1030:c02:8::14 2603:1010:3:3::5b 20.231.239.246 20.76.201.171 20.70.246.20 20.236.44.162 20.112.250.133 Run with strace:\n$ sudo strace -e openat ./resolver microsoft.com … [pid 48568] openat(AT_FDCWD, \u0026#34;/etc/resolv.conf\u0026#34;, O_RDONLY|O_CLOEXEC) = 3 [pid 48568] openat(AT_FDCWD, \u0026#34;/etc/nsswitch.conf\u0026#34;, O_RDONLY|O_CLOEXEC) = 3 [pid 48568] openat(AT_FDCWD, \u0026#34;/etc/hosts\u0026#34;, O_RDONLY|O_CLOEXEC) = 3 … It only reads the basic system configuration files. It also does source address retrieval:\n$ sudo strace -f -e trace=network ./resolver microsoft.com 2\u0026gt;\u0026amp;1 [pid 48674] socket(AF_INET6, SOCK_DGRAM|SOCK_CLOEXEC|SOCK_NONBLOCK, IPPROTO_IP) = 3 [pid 48674] setsockopt(3, SOL_IPV6, IPV6_V6ONLY, [0], 4) = 0 [pid 48674] setsockopt(3, SOL_SOCKET, SO_BROADCAST, [1], 4) = 0 [pid 48674] connect(3, {sa_family=AF_INET6, sin6_port=htons(9), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \u0026#34;2603:1030:c02:8::14\u0026#34;, \u0026amp;sin6_addr), sin6_scope_id=0}, 28) = 0 [pid 48674] getsockname(3, {sa_family=AF_INET6, sin6_port=htons(58438), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \u0026#34;2001:db8:123:456:6af2:68fe:ff7c:e25c\u0026#34;, \u0026amp;sin6_addr), sin6_scope_id=0}, [112 =\u0026gt; 28]) = 0 [pid 48674] getpeername(3, {sa_family=AF_INET6, sin6_port=htons(9), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \u0026#34;2603:1030:c02:8::14\u0026#34;, \u0026amp;sin6_addr), sin6_scope_id=0}, [112 =\u0026gt; 28]) = 0 Let make sure we don’t use libc getaddrinfo(). We use gdb debugger for that:\n$ gdb ./resolver (gdb) set args microsoft.com (gdb) break getaddrinfo Breakpoint 1 at 0x11aa90 (gdb) run 2603:1010:3:3::5b 2603:1030:b:3::152 2603:1020:201:10::10f 2603:1030:20e:3::23c 2603:1030:c02:8::14 20.112.250.133 20.231.239.246 20.76.201.171 20.236.44.162 20.70.246.20 [Inferior 1 (process 48729) exited normally] (gdb) Quit The break point was not hit, so there were no getaddrinfo() calls.\nAnd if we recompile with cgo resolver:\n$ export GODEBUG=netdns=cgo $ go build -gcflags \u0026#34;all=-N -l\u0026#34; -o resolver ./main.go $ gdb ./resolver (gdb) set args microsoft.com (gdb) break getaddrinfo Breakpoint 1 at 0x11aa90 (gdb) run Thread 1 \u0026#34;resolver\u0026#34; hit Breakpoint 1, __GI_getaddrinfo (name=0x4000012120 \u0026#34;microsoft.com\u0026#34;, service=0x0, hints=0x4000100540, pai=0x4000040048) at ./nss/getaddrinfo.c:2297 warning: 2297 ./nss/getaddrinfo.c: No such file or directory The decision which resolver to use happens here and has a lot of logic to make a decision which stub resolver would be beneficial.\nGo supports the destination address sorting (as we see earlier with obtaining source adresses) in its go resolver but doesn’t provide a way to change the sorting rules (doesn’t read /etc/gai.conf). The logic lives in https://github.com/golang/go/blob/master/src/net/addrselect.go.\n8.2.2 Happy Eyeballs # The standard golang library transparently supports RFC 6555 Happy Eyeballs: Success with Dual-Stack Hosts in net.Dialer:\ntype Dialer struct { … // FallbackDelay specifies the length of time to wait before // spawning a RFC 6555 Fast Fallback connection. That is, this // is the amount of time to wait for IPv6 to succeed before // assuming that IPv6 is misconfigured and falling back to // IPv4. // // If zero, a default delay of 300ms is used. // A negative value disables Fast Fallback support. FallbackDelay time.Duration … } Internally net.Dialer in the DialContext() method resolves hostname into adresses, splits them into two groups by address families, and runs the sysDialer.dialParallel() function:\nfunc (d *Dialer) DialContext(ctx context.Context, network, address string) (Conn, error) { ... addrs, err := d.resolver().resolveAddrList(resolveCtx, \u0026#34;dial\u0026#34;, network, address, d.LocalAddr) if err != nil { return nil, \u0026amp;OpError{Op: \u0026#34;dial\u0026#34;, Net: network, Source: nil, Addr: nil, Err: err} } sd := \u0026amp;sysDialer{ Dialer: *d, network: network, address: address, } var primaries, fallbacks addrList if d.dualStack() \u0026amp;\u0026amp; network == \u0026#34;tcp\u0026#34; { primaries, fallbacks = addrs.partition(isIPv4) } else { primaries = addrs } return sd.dialParallel(ctx, primaries, fallbacks) } where dialParallel() issues an IPv6 connection request first, waits for FallbackDelay or 300 ms by default in fallbackDelay().\n// dialParallel races two copies of dialSerial, giving the first a // head start. It returns the first established connection and // closes the others. Otherwise it returns an error from the first // primary address. func (sd *sysDialer) dialParallel(ctx context.Context, primaries, fallbacks addrList) (Conn, error) { if len(fallbacks) == 0 { return sd.dialSerial(ctx, primaries) } returned := make(chan struct{}) defer close(returned) type dialResult struct { Conn error primary bool done bool } results := make(chan dialResult) // unbuffered startRacer := func(ctx context.Context, primary bool) { ras := primaries if !primary { ras = fallbacks } c, err := sd.dialSerial(ctx, ras) select { case results \u0026lt;- dialResult{Conn: c, error: err, primary: primary, done: true}: case \u0026lt;-returned: if c != nil { c.Close() } } } var primary, fallback dialResult // Start the main racer. primaryCtx, primaryCancel := context.WithCancel(ctx) defer primaryCancel() go startRacer(primaryCtx, true) // Start the timer for the fallback racer. fallbackTimer := time.NewTimer(sd.fallbackDelay()) defer fallbackTimer.Stop() for { select { case \u0026lt;-fallbackTimer.C: fallbackCtx, fallbackCancel := context.WithCancel(ctx) defer fallbackCancel() go startRacer(fallbackCtx, false) case res := \u0026lt;-results: if res.error == nil { return res.Conn, nil } if res.primary { primary = res } else { fallback = res } if primary.done \u0026amp;\u0026amp; fallback.done { return nil, primary.error } if res.primary \u0026amp;\u0026amp; fallbackTimer.Stop() { // If we were able to stop the timer, that means it // was running (hadn\u0026#39;t yet started the fallback), but // we just got an error on the primary path, so start // the fallback immediately (in 0 nanoseconds). fallbackTimer.Reset(0) } } } } func (d *Dialer) fallbackDelay() time.Duration { if d.FallbackDelay \u0026gt; 0 { return d.FallbackDelay } else { return 300 * time.Millisecond } } 8.3 Rust # We are talking about Rust 1.80.0.\n8.3.1 Resolver # Rust has a ToSocketAddrs trait:\npub trait ToSocketAddrs { type Iter: Iterator\u0026lt;Item = SocketAddr\u0026gt;; // Required method fn to_socket_addrs(\u0026amp;self) -\u0026gt; Result\u0026lt;Self::Iter\u0026gt;; } where SocketAddr:\npub enum SocketAddr { V4(SocketAddrV4), V6(SocketAddrV6), } The stdlib comes with the trait implementation for the(\u0026amp;str, u16):\n#[stable(feature = \u0026#34;rust1\u0026#34;, since = \u0026#34;1.0.0\u0026#34;)] impl ToSocketAddrs for (\u0026amp;str, u16) { type Iter = vec::IntoIter\u0026lt;SocketAddr\u0026gt;; fn to_socket_addrs(\u0026amp;self) -\u0026gt; io::Result\u0026lt;vec::IntoIter\u0026lt;SocketAddr\u0026gt;\u0026gt; { let (host, port) = *self; // try to parse the host as a regular IP address first if let Ok(addr) = host.parse::\u0026lt;Ipv4Addr\u0026gt;() { let addr = SocketAddrV4::new(addr, port); return Ok(vec![SocketAddr::V4(addr)].into_iter()); } if let Ok(addr) = host.parse::\u0026lt;Ipv6Addr\u0026gt;() { let addr = SocketAddrV6::new(addr, port, 0, 0); return Ok(vec![SocketAddr::V6(addr)].into_iter()); } resolve_socket_addr((host, port).try_into()?) } } where (host, port).try_into() uses the TryFrom trait which is reciprocal for TryInto:\nimpl\u0026lt;\u0026#39;a\u0026gt; TryFrom\u0026lt;(\u0026amp;\u0026#39;a str, u16)\u0026gt; for LookupHost { type Error = io::Error; fn try_from((host, port): (\u0026amp;\u0026#39;a str, u16)) -\u0026gt; io::Result\u0026lt;LookupHost\u0026gt; { init(); run_with_cstr(host.as_bytes(), \u0026amp;|c_host| { let mut hints: c::addrinfo = unsafe { mem::zeroed() }; hints.ai_socktype = c::SOCK_STREAM; let mut res = ptr::null_mut(); unsafe { cvt_gai(c::getaddrinfo(c_host.as_ptr(), ptr::null(), \u0026amp;hints, \u0026amp;mut res)) .map(|_| LookupHost { original: res, cur: res, port }) } }) } } And here we can see here, it uses getaddrinfo() from libc.\nIf you need a non-boking resolver to run it with async rust feature, you can, for example, use trust-dns-resolver and tokio:\nDependency:\n[dependencies] tokio = { version = \u0026#34;1\u0026#34;, features = [\u0026#34;full\u0026#34;] } trust-dns-resolver = \u0026#34;0.20\u0026#34; Code:\nuse trust_dns_resolver::TokioAsyncResolver; use trust_dns_resolver::config::*; use trust_dns_resolver::system_conf::read_system_conf; #[tokio::main] async fn main() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { // Create the resolver from system configuration let (resolver_config, mut resolver_opts) =read_system_conf()?; // Specify the LookupIpStrategy you want resolver_opts.ip_strategy = LookupIpStrategy::Ipv4AndIpv6; // Create the resolver with the system configuration and modified options let resolver = TokioAsyncResolver::tokio(resolver_config, resolver_opts)?; let response = resolver.lookup_ip(\u0026#34;microsoft.com\u0026#34;).await?; // Print the IP addresses for ip in response.iter() { println!(\u0026#34;IP address: {}\u0026#34;, ip); } Ok(()) } It doesn’t follow the RFC 6724 Default Address Selection for Internet Protocol Version 6 (IPv6) but it has a lot of other interesting features.\nRun it:\n$ cargo build --release \u0026amp;\u0026amp; ./target/release/resolver Finished `release` profile [optimized] target(s) in 0.02s IP address: 20.76.201.171 IP address: 20.236.44.162 IP address: 20.231.239.246 IP address: 20.70.246.20 IP address: 20.112.250.133 IP address: 2603:1010:3:3::5b IP address: 2603:1030:c02:8::14 IP address: 2603:1030:b:3::152 IP address: 2603:1020:201:10::10f IP address: 2603:1030:20e:3::23c The strace() output regarding reading system config files:\nopenat(AT_FDCWD, \u0026#34;/etc/resolv.conf\u0026#34;, O_RDONLY|O_CLOEXEC) = 9 openat(AT_FDCWD, \u0026#34;/etc/hosts\u0026#34;, O_RDONLY|O_CLOEXEC) = 9 8.3.2 Happy Eyeballs # Although there is no support in the standard library, several third-party libraries provide such capabilities.\n8.4 Java and Netty # We will be talking about OpenJDK version 21.\n8.4.1 Resolver # The usual way to resolve a hostname is to run java.net.getAllByName(String host).\nIn order to change preferences of address families there are two system properties:\njava.net.preferIPv4Stack: systemProperty java.net.preferIPv4Stack (default: false) If IPv6 is available on the operating system the underlying native socket will be, by default, an IPv6 socket which lets applications connect to, and accept connections from, both IPv4 and IPv6 hosts. However, in the case an application would rather use IPv4 only sockets, then this property can be set to true. The implication is that it will not be possible for the application to communicate with IPv6 only hosts.\njava.net.preferIPv6Addresse: systemProperty java.net.preferIPv6Addresses (default: false) When dealing with a host which has both IPv4 and IPv6 addresses, and if IPv6 is available on the operating system, the default behavior is to prefer using IPv4 addresses over IPv6 ones. This is to ensure backward compatibility: for example, for applications that depend on the representation of an IPv4 address (e.g. 192.168.1.1). This property can be set to true to change that preference and use IPv6 addresses over IPv4 ones where possible, or system to preserve the order of the addresses as returned by the system-wide java.net.spi.InetAddressResolver resolver.\nInternally the following static variables are initialized in java.net.InetAdress:\nstatic { // create the impl impl = isIPv6Supported() ? new Inet6AddressImpl() : new Inet4AddressImpl(); // impl must be initialized before calling this method PLATFORM_LOOKUP_POLICY = initializePlatformLookupPolicy(); // create built-in resolver BUILTIN_RESOLVER = createBuiltinInetAddressResolver(); } which ends up in the native code and calls our old friend getaddrinfo():\nJNIEXPORT jobjectArray JNICALL Java_java_net_Inet6AddressImpl_lookupAllHostAddr(JNIEnv *env, jobject this, jstring host, jintcharacteristics) { ... memset(\u0026amp;hints, 0, sizeof(hints)); hints.ai_flags = AI_CANONNAME; hints.ai_family = lookupCharacteristicsToAddressFamily(characteristics); error = getaddrinfo(hostname, NULL, \u0026amp;hints, \u0026amp;res); ... } The lookupCharacteristicsToAddressFamily() function is controlling preferences from the above system properties:\nint lookupCharacteristicsToAddressFamily(int characteristics) { int ipv4 = characteristics \u0026amp; java_net_spi_InetAddressResolver_LookupPolicy_IPV4; int ipv6 = characteristics \u0026amp; java_net_spi_InetAddressResolver_LookupPolicy_IPV6; if (ipv4 != 0 \u0026amp;\u0026amp; ipv6 == 0) { return AF_INET; } if (ipv4 == 0 \u0026amp;\u0026amp; ipv6 != 0) { return AF_INET6; } return AF_UNSPEC; } We can make some conclusions now:\nBy default preference is given to IPv4 addresses, they are returned first. You can reverse the logic by setting preferIPv6Addresses to true. If you know that you are not going to use IPv6 at all, you can set preferIPv4Stack to true and the resolver stops querying for IPv6 AAAA records. If you want to be dual stack ready and fully address agnostic with some drawback of the lack of Happy Eyeballs in Java, set preferIPv6Addresses to system to delegate all responsibility to getaddrinfo(). However, networking services are typically written using the Netty asynchronous event-driven network framework. Netty provides its own asynchronous non-blocking resolver to work natively with its event loop-based model.\nLet’s review its features in version 4.1.\nThe resolver’s documentation is here and to resolve an address you need to call resolveAll(String inetHost) of SimpleNameResolver.\nFor example:\npackage com.example; import io.netty.bootstrap.Bootstrap; import io.netty.channel.ChannelFactory; import io.netty.channel.ChannelInitializer; import io.netty.channel.nio.NioEventLoopGroup; import io.netty.channel.socket.DatagramChannel; import io.netty.channel.socket.nio.NioDatagramChannel; import io.netty.resolver.ResolvedAddressTypes; import io.netty.resolver.dns.DnsNameResolver; import io.netty.resolver.dns.DnsNameResolverBuilder; import io.netty.resolver.dns.DnsServerAddresses; import io.netty.util.concurrent.Future; import java.net.InetAddress; import java.util.List; public class App { public static void main( String[] args ) throws java.lang.InterruptedException { // Create an event loop group for handling DNS resolution NioEventLoopGroup group = new NioEventLoopGroup(); try { // Create a DnsNameResolver DnsNameResolver resolver = new DnsNameResolverBuilder(group.next()) .channelFactory(new ChannelFactory\u0026lt;DatagramChannel\u0026gt;() { @Override public DatagramChannel newChannel() { return new NioDatagramChannel(); } }) .build(); // Resolve the domain name asynchronously String domainName = \u0026#34;microsoft.com\u0026#34;; Future\u0026lt;List\u0026lt;InetAddress\u0026gt;\u0026gt; resolveFuture = resolver.resolveAll(domainName); // Wait for the resolution to complete List\u0026lt;InetAddress\u0026gt; inetAddresses = resolveFuture.sync().getNow(); // Print the resolved IP addresses for (InetAddress inetAddress : inetAddresses) { System.out.println(\u0026#34;Resolved IP address: \u0026#34; + inetAddress); } } finally { // Shut down the event loop group to release resources group.shutdownGracefully(); } } } The output:\nResolved IP address: microsoft.com/20.70.246.20 Resolved IP address: microsoft.com/20.231.239.246 Resolved IP address: microsoft.com/20.236.44.162 Resolved IP address: microsoft.com/20.112.250.133 Resolved IP address: microsoft.com/20.76.201.171 Resolved IP address: microsoft.com/2603:1010:3:3:0:0:0:5b Resolved IP address: microsoft.com/2603:1030:20e:3:0:0:0:23c Resolved IP address: microsoft.com/2603:1030:c02:8:0:0:0:14 Resolved IP address: microsoft.com/2603:1020:201:10:0:0:0:10f Resolved IP address: microsoft.com/2603:1030:b:3:0:0:0:152 We can check that the resolver reads /etc/hosts in resolveHostsFileEntries():\nprivate List\u0026lt;InetAddress\u0026gt; resolveHostsFileEntries(String hostname) { if (hostsFileEntriesResolver == null) { return null; } List\u0026lt;InetAddress\u0026gt; addresses; if (hostsFileEntriesResolver instanceof DefaultHostsFileEntriesResolver) { addresses = ((DefaultHostsFileEntriesResolver) hostsFileEntriesResolver) .addresses(hostname, resolvedAddressTypes); } else { InetAddress address = hostsFileEntriesResolver.address(hostname, resolvedAddressTypes); addresses = address != null ? Collections.singletonList(address) : null; } return addresses == null \u0026amp;\u0026amp; isLocalWindowsHost(hostname) ? Collections.singletonList(LOCALHOST_ADDRESS) : addresses; } We can also specify the preference to resolve by settings DnsNameResolverBuilder():\nDnsNameResolver resolver = new DnsNameResolverBuilder(group.next()) .channelFactory(new ChannelFactory\u0026lt;DatagramChannel\u0026gt;() { @Override public DatagramChannel newChannel() { return new NioDatagramChannel(); } }).resolvedAddressTypes(ResolvedAddressTypes.IPV6_PREFERRED) .build(); Where ResolvedAddressTypes:\n/** * Defined resolved address types. */ public enum ResolvedAddressTypes { /** * Only resolve IPv4 addresses */ IPV4_ONLY, /** * Only resolve IPv6 addresses */ IPV6_ONLY, /** * Prefer IPv4 addresses over IPv6 ones */ IPV4_PREFERRED, /** * Prefer IPv6 addresses over IPv4 ones */ IPV6_PREFERRED } Unfortunately, Netty does not support the system options for IPv6 preference like the standard socket resolver does. The decision what to prefer happens in NetUtil:\nprivate static final boolean IPV4_PREFERRED = SystemPropertyUtil.getBoolean(\u0026#34;java.net.preferIPv4Stack\u0026#34;, false); ... static { String prefer = SystemPropertyUtil.get(\u0026#34;java.net.preferIPv6Addresses\u0026#34;, \u0026#34;false\u0026#34;); if (\u0026#34;true\u0026#34;.equalsIgnoreCase(prefer.trim())) { IPV6_ADDRESSES_PREFERRED = true; } else { // Let\u0026#39;s just use false in this case as only true is \u0026#34;forcing\u0026#34; ipv6. IPV6_ADDRESSES_PREFERRED = false; } logger.debug(\u0026#34;-Djava.net.preferIPv4Stack: {}\u0026#34;, IPV4_PREFERRED); logger.debug(\u0026#34;-Djava.net.preferIPv6Addresses: {}\u0026#34;, prefer); ... } If we set the properties:\n-Djava.net.preferIPv6Addresses=true And get the return:\nResolved IP address: microsoft.com/2603:1030:20e:3:0:0:0:23c Resolved IP address: microsoft.com/2603:1030:b:3:0:0:0:152 Resolved IP address: microsoft.com/2603:1030:c02:8:0:0:0:14 Resolved IP address: microsoft.com/2603:1010:3:3:0:0:0:5b Resolved IP address: microsoft.com/2603:1020:201:10:0:0:0:10f Resolved IP address: microsoft.com/20.231.239.246 Resolved IP address: microsoft.com/20.236.44.162 Resolved IP address: microsoft.com/20.112.250.133 Resolved IP address: microsoft.com/20.70.246.20 Resolved IP address: microsoft.com/20.76.201.171 If we force to return only IPv4 with preferIPv4Stack:\n-Djava.net.preferIPv4Stack=true Resolved IP address: microsoft.com/20.70.246.20 Resolved IP address: microsoft.com/20.231.239.246 Resolved IP address: microsoft.com/20.112.250.133 Resolved IP address: microsoft.com/20.76.201.171 Resolved IP address: microsoft.com/20.236.44.162 One more time there is no support of the system argument for -Djava.net.preferIPv6Addresses=system nor RFC 6724 with its section 6 Destination selection algorithm and Happy Eyeballs, so it’s completely on you to control you address preferences and the order in which they will appear in the result list from the resolver. This unfortunately could lead to a complexity with application configuration in the process of migration to IPv6 infrastructure.\nThe open issue https://github.com/netty/netty/issues/13400 has more info about the situation.\n8.4.2 Java SecurityManager # There is a well-known issue with Java regarding the caching of DNS resolutions.\nIf you have SecurityManager enabled, the default caching TTL is infinite. To change it, edit the $JAVA_HOME/jre/lib/security/java.security.\nThe defaults:\n# The Java-level namelookup cache policy for successful lookups: # # any negative value: caching forever # any positive value: the number of seconds to cache an address for # zero: do not cache # # default value is forever (FOREVER). For security reasons, this # caching is made forever when a security manager is set. When a security # manager is not set, the default behavior in this implementation # is to cache for 30 seconds. # # NOTE: setting this to anything other than the default value can have # serious security implications. Do not set it unless # you are sure you are not exposed to DNS spoofing attack. # #networkaddress.cache.ttl=-1 # # The Java-level namelookup cache stale policy: # # any positive value: the number of seconds to use the stale names # zero: do not use stale names # negative values are ignored # # default value is 0 (NEVER). # #networkaddress.cache.stale.ttl=0 # The Java-level namelookup cache policy for failed lookups: # # any negative value: cache forever # any positive value: the number of seconds to cache negative lookup results # zero: do not cache # # In some Microsoft Windows networking environments that employ # the WINS name service in addition to DNS, name service lookups # that fail may take a noticeably long time to return (approx. 5 seconds). # For this reason the default caching policy is to maintain these # results for 10 seconds. # networkaddress.cache.negative.ttl=10 8.4.3 Happy eyeballs # There is no support in standard libraries.\nThere is no support in 4.1 netty: https://github.com/netty/netty/issues/9540\nThe okhttp (doesn\u0026rsquo;t use Netty internally and implemented its own event loop) supports it since the 5.0 version.\n8.5 Node.js # 8.5.1 Resoler # The libuv is a C library originally written for Node.js to abstract non-blocking I/O operations.\nIt uses getaddrinfo() internally.\nlibuv provides asynchronous DNS resolution. For this it provides its own getaddrinfo replacement [3]. In the callback you can perform normal socket operations on the retrieved addresses. Let\u0026rsquo;s connect to Libera.chat to see an example of DNS resolution.\n[3] - libuv use the system getaddrinfo in the libuv threadpool.\nCode:\nstatic void uv__getaddrinfo_work(struct uv__work* w) { uv_getaddrinfo_t* req; int err; req = container_of(w, uv_getaddrinfo_t, work_req); err = getaddrinfo(req-\u0026gt;hostname, req-\u0026gt;service, req-\u0026gt;hints, \u0026amp;req-\u0026gt;addrinfo); req-\u0026gt;retcode = uv__getaddrinfo_translate_error(err); } 8.5.2 Happy Eyeballs # There is ongoing discussion about adding a basic support in https://github.com/nodejs/node/issues/48145.\nRead next chapter → "},{"id":23,"href":"/docs/resolver-dual-stack-application/9-dual-stack-software-examples/","title":"Dual-stack software examples","section":"DNS resolvers and Dual-Stack applications","content":" 9. Dual-stack software examples # Last updated: Oct 2025 Contents\n9.1 Nginx 9.2 Envoy proxy 9.3 HAProxy 9.1 Nginx # Nginx treats the hostname as a set of distinct entries rather than multiple paths to the same host. From the upstream module doc:\nA domain name that resolves to several IP addresses defines multiple servers at once.\nOn start Nginx resolves all hostnames using its static resolver with getaddrinfo() and the AF_UNSPEC and the AI_ADDRCONFIG flags.\nngx_int_t ngx_inet_resolve_host(ngx_pool_t *pool, ngx_url_t *u) { ... hints.ai_family = AF_UNSPEC; hints.ai_socktype = SOCK_STREAM; #ifdef AI_ADDRCONFIG hints.ai_flags = AI_ADDRCONFIG; #endif if (getaddrinfo((char *) host, NULL, \u0026amp;hints, \u0026amp;res) != 0) { u-\u0026gt;err = \u0026#34;host not found\u0026#34;; ngx_free(host); return NGX_ERROR; } ... If you have a paid Plus version of Nginx or use a workaround with a variable in proxy_pass, Nginx can periodically update in memory resolution cache. In order to enable it, you need to specify the global resolver first:\nresolver address ... [valid=time] [ipv4=on|off] [ipv6=on|off] [status_zone=zone]; The name resolving happens in ngx_resolver.c, which support A, AAAA, CNAME and SRV records (only in paid version):\nstatic ngx_int_t ngx_resolve_name_locked(ngx_resolver_t *r, ngx_resolver_ctx_t *ctx, ngx_str_t *name) { … addrs = ngx_resolver_export(r, rn, 1); … The resolver randomly rotate the addresses artificially implementing round-robin DNS:\nstatic ngx_resolver_addr_t * ngx_resolver_export(ngx_resolver_t *r, ngx_resolver_node_t *rn, ngx_uint_t rotate) { … d = rotate ? ngx_random() % n : 0; … Because Nginx handles each address of a hostname as multiple servers, the returned IPv6 addresses are treated as completely independent servers rather than alternative ways to reach the same host. As a result, Nginx does not support the Happy Eyeballs algorithm or sorting by destination address selection rules.\nThe addresses in upstreams are selected in a round-robin fashion.\nBy default, requests are distributed between the servers using a weighted round-robin balancing method.\nThe round-robin pear is created in ngx_http_upstream_create_round_robin_peer() function from the resolver answer:\nngx_int_t ngx_http_upstream_create_round_robin_peer(ngx_http_request_t *r, ngx_http_upstream_resolved_t *ur) { … The playground to play with the local Nginx and dynamic resolver via a prorxy_pass variable.\nIn order to control resolver we need to install our own DNS server. I suggest using CoreDNS, as it’s simple and powerful for our needs:\n$ wget https://github.com/coredns/coredns/releases/download/v1.11.1/coredns_1.11.1_linux_arm64.tgz $ tar -zxf coredns_1.11.1_linux_arm64.tgz Its config file:\n$ cat Corefile test.example { bind 127.0.0.153 loadbalance round_robin file test.example.db } Zone file for test.example domain:\n$ cat test.example.db $ORIGIN test.example. @ 3600 IN SOA sns.dns.icann.org. noc.dns.icann.org. 2017042745 7200 3600 1209600 3600 site IN A 192.168.5.15 IN AAAA ::1 IN AAAA fec0::5055:55ff:fe8e:3d07 where:\n192.168.5.15 – local container IPv4 address fec0::5055:55ff:fe8e:3d07 – local container IPv6 unique local address (ULA). Nginx doesn\u0026rsquo;t have link-local IPv6 address support for backends, so please don’t use them. Run it:\n$ sudo ./coredns Nginx test config:\nworker_processes auto; events { worker_connections 1024; } pid /tmp/nginx.pid; http { # Use a DNS resolver to resolve the backend host resolver 127.0.0.153 valid=5s; access_log /tmp/access.log; error_log /tmp/error.log debug; server { listen 80; server_name site.test.example; location / { set $backend_service \u0026#34;site.test.example:8080\u0026#34;; # Proxy to the dynamically resolved backend service proxy_pass http://$backend_service; # Pass host headers proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; # Handle timeouts and retries proxy_connect_timeout 10s; proxy_send_timeout 10s; proxy_read_timeout 10s; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; # Optional: Additional settings to improve proxy performance and security proxy_buffering on; proxy_buffer_size 16k; proxy_buffers 32 16k; proxy_busy_buffers_size 64k; proxy_max_temp_file_size 64k; } } default_type application/octet-stream; # Gzip settings gzip on; gzip_vary on; gzip_proxied any; gzip_comp_level 6; gzip_buffers 16 8k; gzip_http_version 1.1; gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript; } Run a dummy backend with python:\n$ python3 -m http.server 8080 --bind :: And make some requests in another terminal window:\n$ while true; do curl 127.0.0.1; done In the console with the python simple server you should see something similar, which shows the usage of round-robin algorithm:\nfec0::5055:55ff:fe8e:3d07 - - [01/Aug/2024 09:49:11] \u0026#34;GET / HTTP/1.0\u0026#34; 200 - ::ffff:192.168.5.15 - - [01/Aug/2024 09:49:11] \u0026#34;GET / HTTP/1.0\u0026#34; 200 - ::1 - - [01/Aug/2024 09:49:11] \u0026#34;GET / HTTP/1.0\u0026#34; 200 - fec0::5055:55ff:fe8e:3d07 - - [01/Aug/2024 09:49:11] \u0026#34;GET / HTTP/1.0\u0026#34; 200 - ::1 - - [01/Aug/2024 09:49:11] \u0026#34;GET / HTTP/1.0\u0026#34; 200 - fec0::5055:55ff:fe8e:3d07 - - [01/Aug/2024 09:49:11] \u0026#34;GET / HTTP/1.0\u0026#34; 200 - ::1 - - [01/Aug/2024 09:49:11] \u0026#34;GET / HTTP/1.0\u0026#34; 200 - ::1 - - [01/Aug/2024 09:49:11] \u0026#34;GET / HTTP/1.0\u0026#34; 200 - ::ffff:192.168.5.15 - - [01/Aug/2024 09:49:11] \u0026#34;GET / HTTP/1.0\u0026#34; 200 - ::1 - - [01/Aug/2024 09:49:11] \u0026#34;GET / HTTP/1.0\u0026#34; 200 - ::1 - - [01/Aug/2024 09:49:11] \u0026#34;GET / HTTP/1.0\u0026#34; 200 - ::ffff:192.168.5.15 - - [01/Aug/2024 09:49:11] \u0026#34;GET / HTTP/1.0\u0026#34; 200 - ::1 - - [01/Aug/2024 09:49:11] \u0026#34;GET / HTTP/1.0\u0026#34; 200 - In the tcpdump you can see periodic refresh of addresses:\n$ sudo tcpdump -s0 -i any -n -A host 127.0.0.153 09:53:16.002276 lo In IP 127.0.0.1.48587 \u0026gt; 127.0.0.153.53: 21998+ A? site.test.example. (35) 09:53:16.002288 lo In IP 127.0.0.1.48587 \u0026gt; 127.0.0.153.53: 19029+ AAAA? site.test.example. (35) 09:53:16.002394 lo In IP 127.0.0.153.53 \u0026gt; 127.0.0.1.48587: 19029*- 2/0/0 AAAA ::1, AAAA fec0::5055:55ff:fe8e:3d07 (125) 09:53:16.002422 lo In IP 127.0.0.153.53 \u0026gt; 127.0.0.1.48587: 21998*- 1/0/0 A 192.168.5.15 (68) 9.2 Envoy proxy # The default resolver in Envoy for GNU/Linux is c-ares. Envoy is built statically, so the simplest way to determine the version of c-ares, is to check the bazel repository_locations.bzl file.\nWe will be reviewing Envoy with the Logical DNS option set for its cluster config.cluster.v3.Cluster.DiscoveryType, because it is the general case and the most used one. The best place to start learning and configuring the stub resolve is config.cluster.v3.Cluster.DnsLookupFamily option:\nEnum config.cluster.v3.Cluster.DnsLookupFamily When V4_ONLY is selected, the DNS resolver will only perform a lookup for addresses in the IPv4 family. If V6_ONLY is selected, the DNS resolver will only perform a lookup for addresses in the IPv6 family. If AUTO is specified, the DNS resolver will first perform a lookup for addresses in the IPv6 family and fallback to a lookup for addresses in the IPv4 family. This is semantically equivalent to a non-existent V6_PREFERRED option. AUTO is a legacy name that is more opaque than necessary and will be deprecated in favor of V6_PREFERRED in a future major version of the API. If V4_PREFERRED is specified, the DNS resolver will first perform a lookup for addresses in the IPv4 family and fallback to a lookup for addresses in the IPv6 family. i.e., the callback target will only get v6 addresses if there were NO v4 addresses to return. If ALL is specified, the DNS resolver will perform a lookup for both IPv4 and IPv6 families, and return all resolved addresses. When this is used, Happy Eyeballs will be enabled for upstream connections.\nThe translation into the C socket address family is happening in cares/dns_impl.cc:\nswitch (dns_lookup_family_) { case DnsLookupFamily::V4Only: case DnsLookupFamily::V4Preferred: family_ = AF_INET; break; case DnsLookupFamily::V6Only: case DnsLookupFamily::Auto: family_ = AF_INET6; break; case DnsLookupFamily::All: family_ = AF_UNSPEC; break; } An important clarification regarding the DnsLookupFamily configuration is that the second fallback resolver call occurs only if there is a resolver-related error or a ‘not found’ response. This means that if you have the default DnsLookupFamily setting (which prefers IPv6), and the resolver returns an AAAA record, but your network routing is broken, there will be no retry for an A address. The same behavior applies when DnsLookupFamily is set to V4_PREFERRED.\nThe retry code is in dns_impl.cc:\n// Perform a second lookup for DnsLookupFamily::Auto and DnsLookupFamily::V4Preferred, given // that the first lookup failed to return any addresses. Note that DnsLookupFamily::All issues // both lookups concurrently so there is no need to fire a second lookup here. if (dns_lookup_family_ == DnsLookupFamily::Auto) { family_ = AF_INET; startResolutionImpl(AF_INET); } else if (dns_lookup_family_ == DnsLookupFamily::V4Preferred) { family_ = AF_INET6; startResolutionImpl(AF_INET6); } To help dealing with such errors you can think of using the filter_unroutable_families option. But there is a trick.\nfilter_unroutable_families\n(bool) The resolver will query available network interfaces and determine if there are no available interfaces for a given IP family. It will then filter these addresses from the results it presents. e.g., if there are no available IPv4 network interfaces, the resolver will not provide IPv4 addresses.\nThe trick is, it suffers from the same issue we identified earlier with the AI_ADDRCONFIG flag in getaddrinfo(): it considers any IPv6 address – except for localhost – as a cue to greenlight AAAA stub resolver queries. This means all link-local and unique local addresses are included and treated as votes for the IPv6 stack. Therefore, this option is really useful for IPv6-only hosts with a completely disabled IPv4 stack or IPv4-only with completely disabled IPv6.\nThe whole filtering is happening in DnsResolverImpl::AddrInfoPendingResolution::onAresGetAddrInfoCallback:\nif (addrinfo != nullptr \u0026amp;\u0026amp; addrinfo-\u0026gt;nodes != nullptr) { bool can_process_v4 = (!parent_.filter_unroutable_families_ || available_interfaces_.v4_available_); bool can_process_v6 = (!parent_.filter_unroutable_families_ || available_interfaces_.v6_available_); The v4_available_ and v6_available_ are set in dns_impl.cc:\nfor (const auto\u0026amp; interface_address : interface_addresses) { if (!interface_address.interface_addr_-\u0026gt;ip()) { continue; } if (Network::Utility::isLoopbackAddress(*interface_address.interface_addr_)) { continue; } switch (interface_address.interface_addr_-\u0026gt;ip()-\u0026gt;version()) { case Network::Address::IpVersion::v4: available_interfaces.v4_available_ = true; if (available_interfaces.v6_available_) { return available_interfaces; } break; case Network::Address::IpVersion::v6: available_interfaces.v6_available_ = true; if (available_interfaces.v4_available_) { return available_interfaces; } break; } } return available_interfaces; } where the only filtering happens for localhosts in common/network/utility.cc:\nbool Utility::isLoopbackAddress(const Address::Instance\u0026amp; address) { if (address.type() != Address::Type::Ip) { return false; } if (address.ip()-\u0026gt;version() == Address::IpVersion::v4) { // Compare to the canonical v4 loopback address: 127.0.0.1. return address.ip()-\u0026gt;ipv4()-\u0026gt;address() == htonl(INADDR_LOOPBACK); } else if (address.ip()-\u0026gt;version() == Address::IpVersion::v6) { static_assert(sizeof(absl::uint128) == sizeof(in6addr_loopback), \u0026#34;sizeof(absl::uint128) != sizeof(in6addr_loopback)\u0026#34;); absl::uint128 addr = address.ip()-\u0026gt;ipv6()-\u0026gt;address(); return 0 == memcmp(\u0026amp;addr, \u0026amp;in6addr_loopback, sizeof(in6addr_loopback)); } IS_ENVOY_BUG(\u0026#34;unexpected address type\u0026#34;); return false; } So the real solution for the dual stack upstreams or seamlessly migrating to IPv6 only backends is to enable the Happy Eyeballs algorithm which is supported by setting DnsLookupFamily to ALL.\nThe code is in source/common/network/happy_eyeballs_connection_impl.cc.\nOne interesting observation is that Envoy violates section 4 of the Happy Eyeballs RFC by not sorting addresses before initiating connections.\nFirst, the client MUST sort the addresses received up to this point using Destination Address Selection ([RFC6724], Section 6).\nBut in the source/extensions/network/dns_resolver/cares/dns_impl.cc we can how c-ares is called with disabled sorting:\n/** * ARES_AI_NOSORT result addresses will not be sorted and no connections to resolved addresses * will be attempted */ hints.ai_flags = ARES_AI_NOSORT; The justification is to increase performance due to fewer system calls.\nHowever Envoy has its own settings to control its own sorting for Happy Eyeballs:\nfirst_address_family_version\n(config.cluster.v3.UpstreamConnectionOptions.FirstAddressFamilyVersion) Specify the IP address family to attempt connection first in happy eyeballs algorithm according to RFC8305#section-4.\nfirst_address_family_count\n(UInt32Value) Specify the number of addresses of the first_address_family_version being attempted for connection before the other address family.\nThe full config to play with Envoy:\nstatic_resources: listeners: - name: listener_0 address: socket_address: address: \u0026#34;::\u0026#34; port_value: 10000 ipv4_compat: true filter_chains: - filters: - name: envoy.filters.network.http_connection_manager typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix: ingress_http access_log: - name: envoy.access_loggers.stdout typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog log_format: text_format: \u0026#34;%START_TIME% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% %REQ(:METHOD)% %REQ(X-FORWARDED-FOR?:REMOTE_ADDRESS)% %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %UPSTREAM_HOST%\\n\u0026#34; http_filters: - name: envoy.filters.http.router typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.router.v3.Router route_config: name: local_route virtual_hosts: - name: local_service domains: [\u0026#34;*\u0026#34;] routes: - match: prefix: \u0026#34;/\u0026#34; route: host_rewrite_literal: www.envoyproxy.io cluster: service_envoyproxy_io clusters: - name: service_envoyproxy_io type: LOGICAL_DNS # Change to play with other resolver rules dns_lookup_family: AUTO typed_dns_resolver_config: name: envoy.network.dns_resolver.cares typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.network.dns_resolver.cares.v3.CaresDnsResolverConfig resolvers: - socket_address: address: \u0026#34;8.8.8.8\u0026#34; port_value: 53 filter_unroutable_families: true dns_resolver_options: no_default_search_domain: true load_assignment: cluster_name: service_envoyproxy_io endpoints: - lb_endpoints: - endpoint: address: socket_address: address: www.envoyproxy.io port_value: 443 transport_socket: name: envoy.transport_sockets.tls typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext sni: www.envoyproxy.io And run it with debug:\n$ envoy -l debug -c ./envoy.yaml 9.3 HAProxy # HAProxy uses getaddrinfo() only for the initial name resolution when binding to listening addresses, similar to the approach we discussed in the dual-stack server section.\nTo connect to backend (upstream) servers, HAProxy employs its own resolver. It is possible to configure HAProxy with multiple name servers. If you set more than one, HAProxy will send queries to all of them in parallel:\nWhen multiple name servers are configured in a resolvers section, then HAProxy uses the first valid response. In case of invalid responses, only the last one is treated. Purpose is to give the chance to a slow server to deliver a valid answer after a fast faulty or outdated server.\nThe code is under src/resolvers.c:\nstatic int resolv_send_query(struct resolv_resolution *resolution) { … list_for_each_entry(ns, \u0026amp;resolvers-\u0026gt;nameservers, list) { if (dns_send_nameserver(ns, trash.area, len) \u0026gt;= 0) resolution-\u0026gt;nb_queries++; } … } You can also control the preference of address families:\nresolve-prefer \u0026lt;family\u0026gt;\nWhen DNS resolution is enabled for a server and multiple IP addresses from different families are returned, HAProxy will prefer using an IP address from the family mentioned in the \u0026ldquo;resolve-prefer\u0026rdquo; parameter. Available families: \u0026ldquo;ipv4\u0026rdquo; and \u0026ldquo;ipv6\u0026rdquo;\nDefault value: ipv6\nAccording to the documentation, the parameter mentioned above is only a preference. In the event of a resolve timeout, error, or \u0026ldquo;not found\u0026rdquo; address, a retry will be issued with another resource record family type.\nHAProxy also does not follow the Default Address Selection for Internet Protocol Version 6 (IPv6). The logic is similar to what we discussed with the Java JDK, but without the system value for the IPv6 preference property, which can make migrating from IPv4 to IPv6 challenging.\nAnother resolver-related setting controls how the initial resolution of backends occurs at startup. Interestingly, you can use getaddrinfo() for this purpose.\nAdditionally, there is no support for the Happy Eyeballs algorithm, which could make migration to or using a dual-stack application more difficult.\nRead next chapter → "},{"id":24,"href":"/docs/page-cache/5-more-about-mmap-file-access/","title":"More about mmap() file access","section":"Linux Page Cache series","content":" More about mmap() file access # Last updated: Oct 2025 Contents\nmmap() overview What is a page fault? Subtle MADV_DONT_NEED mmap() feature Before we start the cgroup chapter, where I\u0026rsquo;m showing how to leverage memory and IO limits in order to control Page Cache eviction and improve the reliability of services, I want to delve a bit deeper into mmap() syscall. We need to understand what is happening under the hood and shed more light on the reading and writing process with mmap().\nmmap() overview # Memory mapping is one of the most interesting features of Linux systems. One of its features is the ability for software developers to work transparently with files whose size exceeds the actual physical memory of the system. In the image below, you can see what the Virtual Memory of a process looks like. Each process has its own region where mmap() maps files.\nFigure 1. – AMD x86_64 process virtual memory What I\u0026rsquo;m not touching here is whether to use mmap() or file syscalls like read() and write() in your software. What is better, faster, or safer to use is beyond the scope of this post. But you definitely need to understand how to get the mmap() stats because almost every Page Cache user-space tool uses it.\nLet’s write one more script with mmap(). It prints a PID of the process, maps the test file and sleeps. The sleep time should be enough to play with the process.\nimport mmap import os from time import sleep print(\u0026#34;pid:\u0026#34;, os.getpid()) with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;rb\u0026#34;) as f: with mmap.mmap(f.fileno(), 0, prot=mmap.PROT_READ) as mm:f sleep(10000) Run it in one terminal window, and in another one, run pmap -x PID with the PID of the script.\npmap -x 369029 | less where 369029 is my PID.\nThe output of the pmap shows us all contiguous virtual memory areas (VMA or struct vm_area_struct) of the process. We can determine the virtual addresses of the mmaped test file file1.db. In my case:\nAddress Kbytes RSS Dirty Mode Mapping ... 00007f705cc12000 131072 0 0 r--s- file1.db We can see that we have 0 dirty pages for the file (it only shows the dirty pages of this process). The RSS column equals 0, which tells us how much memory in KiB our process has already referenced. This 0, by the way, doesn\u0026rsquo;t mean that there are no pages of the file in Page Cache. It means that our process hasn\u0026rsquo;t accessed any pages yet. NOTE\npmap can display even more detailed output with -XX. Without -XX, it uses /proc/pid/maps, but for the extended mode it shows stats from /proc/pid/smaps. More info can be found in man 5 proc and kernel documentation filesystems/proc.rst.\nSo, the most exciting part about mmap() for SRE is how it transparently loads data on accessing and writing. And I\u0026rsquo;m showing all this in the following chapters.\nWhat is a page fault? # Before we start talking about file tools, we need to understand the concept of page faults. Generally speaking, the page fault is the CPU mechanism for communicating with the Linux Kernel and its memory subsystem. The page fault is a building block of the Virtual Memory concept and demand paging. Briefly speaking, the kernel usually doesn\u0026rsquo;t allocate physical memory immediately after a memory request is done by mmap() or malloc(). Instead, the kernel creates some records it the process\u0026rsquo;s page table structure and uses it as a storage for its memory promises. In addition, the page table contains extra info for each page, such as memory permissions and page flags (we\u0026rsquo;ve already seen some of them: LRUs flags, dirty flag, etc.).\nFrom the examples in chapter 2, you can see that in order to read mmaped file at any position, the code doesn\u0026rsquo;t need to perform any seeks (man 2 lseek), unlike with file operations. We can just start reading from or writing to any point of the mapped area. For this reason, when an application wants to access a page, the page fault can occur if the target page has not been loaded to Page Cache or there are no connections between the page in the Page Cache and the process\u0026rsquo; page table.\nThere are 2 useful for us types of page faults: minor and major. A minor basically means that there will be no disk access in order to fulfill a process\u0026rsquo;s memory access. And on the other hand, a major page fault means that there will be a disk IO operation.\nFor example, if we load half of a file with dd in Page Cache and afterward access this first half from a program with mmap(), we will trigger minor page faults. The kernel doesn\u0026rsquo;t need to go to disks because these pages were already loaded to Page Cache. The kernel only needs to reference these already loaded pages with the page table entries of the process. But if the process tries to read within the same mmaped area the second half of the file, the kernel will have to go to the disk in order to load the pages, and the system will generate major page faults.\nIf you want to get more information about demand paging, Linux kernel and system internals, please watch \u0026ldquo;Introduction to Memory Management in Linux\u0026rdquo; video from Embedded Linux Conf.\nLet’s do an experiment and write a script with an infinitive random read of the file:\nimport mmap import os from random import randint from time import sleep with open(\u0026#34;/var/tmp/file1.db\u0026#34;, \u0026#34;r\u0026#34;) as f: fd = f.fileno() size = os.stat(fd).st_size with mmap.mmap(fd, 0, prot=mmap.PROT_READ) as mm: try: while True: pos = randint(0, size-4) print(mm[pos:pos+4]) sleep(0.05) except KeyboardInterrupt: pass Now we need 3 terminal windows. In the first one:\n$ sar -B 1 which shows the system memory statistics per second including page faults.\nAnd in the second one, perf trace:\n$ sudo perf trace -F maj --no-syscalls which shows major page faults and corresponding file paths.\nFinally, in the 3rd terminal window, start the above python script:\n$ python3 ./mmap_random_read.py The output should be something closer to the following:\n$ sar -B 1 .... LOOK HERE ⬇ ⬇ 05:45:55 PM pgpgin/s pgpgout/s fault/s majflt/s pgfree/s pgscank/s pgscand/s pgsteal/s %vmeff 05:45:56 PM 8164.00 0.00 39.00 4.00 5.00 0.00 0.00 0.00 0.00 05:45:57 PM 2604.00 0.00 20.00 1.00 1.00 0.00 0.00 0.00 0.00 05:45:59 PM 5600.00 0.00 22.00 3.00 2.00 0.00 0.00 0.00 0.00 ... Take a look at the fault/s and majflt/s fields. They show what I\u0026rsquo;ve just explained.\nAnd from the perf trace, we can get insides about the file where we have major page faults:\n$ sudo perf trace -F maj --no-syscalls ... SCROLL ➡ LOOK HERE ⬇ 5278.737 ( 0.000 ms): python3/64915 majfault [__memmove_avx_unaligned_erms+0xab] =\u0026gt; /var/tmp/file1.db@0x2aeffb6 (d.) 5329.946 ( 0.000 ms): python3/64915 majfault [__memmove_avx_unaligned_erms+0xab] =\u0026gt; /var/tmp/file1.db@0x539b6d9 (d.) 5383.701 ( 0.000 ms): python3/64915 majfault [__memmove_avx_unaligned_erms+0xab] =\u0026gt; /var/tmp/file1.db@0xb3dbc7 (d.) 5434.786 ( 0.000 ms): python3/64915 majfault [__memmove_avx_unaligned_erms+0xab] =\u0026gt; /var/tmp/file1.db@0x18f7c4f (d.) ... The cgroup also has per cgroup stats regarding page faults:\n$ grep fault /sys/fs/cgroup/user.slice/user-1000.slice/session-3.scope/memory.stat ... pgfault 53358 pgmajfault 13 ... Subtle MADV_DONT_NEED mmap() feature # Now let\u0026rsquo;s perform another experiment. Stop all scripts and drop all caches:\n$ sync; echo 3 | sudo tee /proc/sys/vm/drop_caches Restart the script with the infinite read and start monitoring per memory area usage of the process:\nwatch -n 0.1 \u0026#34;grep \u0026#39;file1\u0026#39; /proc/$pid/smaps -A 24\u0026#34; You can now see the mmaped area of the file and its info. The reference field should be growing.\nIn the other window, try to evict pages with vmtouch:\nvmtouch -e /var/tmp/file1.db And notice that the stats from the smaps output don\u0026rsquo;t drop entirely. When you run the vmtouch -e command, the smaps should show you some decrease in memory usage. The question is, what happens? Why when we explicitly ask the kernel to evict the file pages by setting the FADVISE_DONT_NEED flag, some of them are still present in Page Cache?\nThe answer is a little confusing, but very important to understand. If the Linux kernel has no memory pressure issues, why should it drop pages from Page Cache? There is a high probability that the program will need them in the future. But if you, as a software developer, are sure that these pages are useless, there is a madvise() and MADV_DONT_NEED flag for that. It informs the kernel that it can remove these pages from the corresponding page table, and the following vmtouch -e call will successfully be able to expel the file data from Page Cache.\nIn case of the memory pressure situation, the kernel will start reclaiming memory from inactive LRU lists. Which means eventually it can drop these pages if they are good candidates for reclaiming.\nRead next chapter → "},{"id":25,"href":"/docs/resolver-dual-stack-application/10-systemd-resolved/","title":"systemd-resolved","section":"DNS resolvers and Dual-Stack applications","content":" 10. systemd-resolved # Last updated: Oct 2025 Contents\n10.1 Managing /etc/resolv.conf content 10.2 Integrating systemd-resolved into system 10.3 Per link name servers and search domains 10.4 Useful commands 10.5 Querying systemd-resolved 10.5.1 Varlink 10.5.2 D-Bus 10.1 Managing /etc/resolv.conf content # The main issue with /etc/resolv.conf is managing it in modern distributions, which can have multiple sources of nameserver and search domain information due to multiple interfaces (both real and virtual, such as VPN tunnels) with concurrent DHCP clients.\nThe legacy method to handle this complexity was the Resolvconf project. It set up a number of hooks and updated /etc/resolv.conf appropriately by following some defined rules. However, its main issue is its lack of flexibility and inability to manage several nameservers and search domains on a per-link basis, which is sometimes needed in complex setups.\nTo address this, the emergence of systemd-resolved was intended to resolve (^_^) these issues.\n10.2 Integrating systemd-resolved into system # Slowly but surely, systemd is taking on more features and responsibilities in modern GNU/Linux distributions. One of these responsibilities is serving as a local source of truth for hostname resolution with systemd-resolved.\nThere are several ways you can start using it system-wide or directly in your application.\nFigure 5. – The ways systemd-resolved can integrate into the system. For the system wide setup there are currently two ways:\nUsing nss module resolve (man libnss_resolve.so.2). Delegate the management of /etc/resolv.conf to systemd-resolved by symlinking /run/systemd/resolve/resolv.conf. The file sets the nameserver to 127.0.0.53 where systemd-resolved is listening and sets search domains. The second option is preferred because it also addresses issues with the search domain.\nBut systemd-resolved is not just a management tool for /etc/resolv.conf; it also acts as a local cache. It supports mDNS, /etc/hosts, and other name sources, and can mix them all using predefined and configurable rules, some of which we will discuss below.\n10.3 Per link name servers and search domains # By default the systemd uses a link name server if you have only one interface with DHCP:\n$ resolvectl Global Protocols: -LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported resolv.conf mode: stub Link 2 (eth0) Current Scopes: DNS Protocols: +DefaultRoute -LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported Current DNS Server: 192.168.5.3 DNS Servers: 192.168.5.3 $ resolvectl show-server-state Server: 192.168.5.3 Type: link Interface: eth0 Interface Index: 2 Verified feature level: UDP Possible feature level: UDP DNSSEC Mode: no DNSSEC Supported: no Maximum UDP fragment size received: 512 Failed UDP attempts: 0 Failed TCP attempts: 0 Seen truncated packet: no Seen OPT RR getting lost: yes Seen RRSIG RR missing: no Seen invalid packet: no Server dropped DO flag: no You can change this behavior by setting a DNS recursor in the systemd-resolved config file /etc/systemd/resolved.conf.\n# cat /etc/systemd/resolved.conf [Resolve] DNS=1.1.1.1 $ sudo systemctl restart systemd-resolved $ sudo resolvectl Global Protocols: -LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported resolv.conf mode: stub DNS Servers: 1.1.1.1 Link 2 (eth0) Current Scopes: DNS Protocols: +DefaultRoute -LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported DNS Servers: 192.168.5.3 As you can see now we have a global and a link DNS server.\n$ resolvectl show-server-state Server: 1.1.1.1 Type: system Verified feature level: UDP+EDNS0 Possible feature level: UDP+EDNS0 DNSSEC Mode: no DNSSEC Supported: no Maximum UDP fragment size received: 512 Failed UDP attempts: 0 Failed TCP attempts: 0 Seen truncated packet: no Seen OPT RR getting lost: no Seen RRSIG RR missing: no Seen invalid packet: no Server dropped DO flag: no Server: 192.168.5.3 Type: link Interface: eth0 Interface Index: 2 Verified feature level: n/a Possible feature level: UDP+EDNS0 DNSSEC Mode: no DNSSEC Supported: no Maximum UDP fragment size received: 512 Failed UDP attempts: 0 Failed TCP attempts: 0 Seen truncated packet: no Seen OPT RR getting lost: no Seen RRSIG RR missing: no Seen invalid packet: no Server dropped DO flag: no $ resolvectl dns Global: 1.1.1.1 Link 2 (eth0): 192.168.5.3 Run a DNS query:\n$ resolvectl query facebook.com facebook.com: 2a03:2880:f158:181:face:b00c:0:25de -- link: eth0 157.240.214.35 -- link: eth0 -- Information acquired via protocol DNS in 20.4ms. -- Data is authenticated: no; Data was acquired via local or encrypted transport: no -- Data from: network $ resolvectl query facebook.com facebook.com: 157.240.214.35 -- link: eth0 2a03:2880:f189:80:face:b00c:0:25de -- link: eth0 -- Information acquired via protocol DNS in 15.2ms. -- Data is authenticated: no; Data was acquired via local or encrypted transport: no -- Data from: cache network In order to test a link name resolver we need to install our own DNS server. I suggest using CoreDNS, as it’s simple and powerful for our needs:\n$ wget https://github.com/coredns/coredns/releases/download/v1.11.1/coredns_1.11.1_linux_arm64.tgz $ tar -zxf coredns_1.11.1_linux_arm64.tgz Configure it with the following config files where we create a zone test.example:\n$ cat Corefile test.example { bind 127.0.0.153 loadbalance round_robin file test.example.db } And zone file:\n$ cat test.example.db $ORIGIN test.example. @ 3600 IN SOA sns.dns.icann.org. noc.dns.icann.org. 2017042745 7200 3600 1209600 3600 site IN A 127.0.0.1 IN A 127.0.0.2 IN A 127.0.0.3 Now check the search domains:\n$ resolvectl domain Global: Link 2 (eth0): Let’s set it to test.example:\n$ resolvectl domain eth0 test.example $ sudo resolvectl domain Global: Link 2 (eth0): test.example The /etc/resolv.conf was updated accordingly:\n$ cat /etc/resolv.conf … nameserver 127.0.0.53 options edns0 trust-ad search test.example \u0026lt;-------------------------- Setting a name server for eth0:\n$ resolvectl dns eth0 127.0.0.153 $ resolvectl Global Protocols: -LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported resolv.conf mode: stub Current DNS Server: 1.1.1.1 DNS Servers: 1.1.1.1 127.0.0.153 Link 2 (eth0) Current Scopes: DNS Protocols: +DefaultRoute -LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported Current DNS Server: 127.0.0.153 DNS Servers: 127.0.0.153 DNS Domain: test.example The /etc/resolv.conf still has stub only, as expected:\n$ cat /etc/resolv.conf nameserver 127.0.0.53 options edns0 trust-ad search test.example Let’s make some queries:\n$ resolvectl query site.test.example site.test.example: 127.0.0.2 127.0.0.1 127.0.0.3 -- Information acquired via protocol DNS in 3.2ms. -- Data is authenticated: no; Data was acquired via local or encrypted transport: no -- Data from: network $ resolvectl query facebook.com facebook.com: 2a03:2880:f158:181:face:b00c:0:25de -- link: eth0 163.70.147.35 -- link: eth0 -- Information acquired via protocol DNS in 16.8ms. -- Data is authenticated: no; Data was acquired via local or encrypted transport: no -- Data from: network Query with search domain for site hostname:\n$ resolvectl query site site: 127.0.0.1 127.0.0.3 127.0.0.2 (site.test.example) -- Information acquired via protocol DNS in 3.4ms. -- Data is authenticated: no; Data was acquired via local or encrypted transport: no -- Data from: network For non-existing site2:\n$ resolvectl query site2 site3: Name \u0026#39;site2\u0026#39; not found In tcpdump we can see that the request was sent with search domain:\n$ sudo tcpdump -i any -s0 -A -n port 53 17:49:24.467909 lo In IP 127.0.0.1.36205 \u0026gt; 127.0.0.153.53: 17745+ [1au] A? site2.test.example. (47) 17:49:24.468073 lo In IP 127.0.0.1.46166 \u0026gt; 127.0.0.153.53: 40292+ [1au] AAAA? site2.test.example. (47) But if we ask for a hostname fb.com, we will see an interesting under the hood:\n$ dig +short @127.0.0.53 fb.com 157.240.214.35 $ sudo tcpdump -i any -s0 -A -n port 53 19:08:01.937480 eth0 Out IP 192.168.5.15.51825 \u0026gt; 1.1.1.1.53: 20123+ [1au] A? fb.com. (35) 19:08:01.937501 eth0 Out IP 192.168.5.15.45070 \u0026gt; 1.1.1.1.53: 23151+ [1au] AAAA? fb.com. (35) 19:08:01.937522 lo In IP 127.0.0.1.35601 \u0026gt; 127.0.0.153.53: 13671+ [1au] A? fb.com. (35) 19:08:01.937672 lo In IP 127.0.0.1.48931 \u0026gt; 127.0.0.153.53: 11867+ [1au] AAAA? fb.com. (35) 19:08:01.937888 lo In IP 127.0.0.153.53 \u0026gt; 127.0.0.1.35601: 13671 Refused- 0/0/1 (35) 19:08:01.937909 lo In IP 127.0.0.153.53 \u0026gt; 127.0.0.1.48931: 11867 Refused- 0/0/1 (35) 19:08:01.950784 eth0 In IP 1.1.1.1.53 \u0026gt; 192.168.5.15.51825: 20123 1/0/1 A 157.240.214.35 (51) 19:08:01.952483 eth0 In IP 1.1.1.1.53 \u0026gt; 192.168.5.15.45070: 23151 1/0/1 AAAA 2a03:2880:f158:181:face:b00c:0:25de (63) The query returns a correct answer, but in the tcpdump we can see that all name servers were asked to provide responses, including our link nameserve 127.0.0.153. This default behavior could lead to a very nasty DNS leak security flaw usually with VPN software:\nA DNS leak is a security flaw that allows DNS requests to be revealed to ISP DNS servers, despite the use of a VPN service to attempt to conceal them.[1] Although primarily of concern to VPN users, it is also possible to prevent it for proxy and direct internet users.\nIn order to address this issues we should leverage the Domains directive:\n[Resolve] DNS=1.1.1.1 Domains=~. Domains=\nA whitespace-separated list of domains which should be resolved using the DNS servers on this link. Each item in the list should be a domain name, optionally prefixed with a tilde (\u0026quot;~\u0026quot;). The domains with the prefix are called \u0026ldquo;routing-only domains\u0026rdquo;. The domains without the prefix are called \u0026ldquo;search domains\u0026rdquo; and are first used as search suffixes for extending single-label hostnames (hostnames containing no dots) to become fully qualified domain names (FQDNs). If a single-label hostname is resolved on this interface, each of the specified search domains are appended to it in turn, converting it into a fully qualified domain name, until one of them may be successfully resolved.\nBoth \u0026ldquo;search\u0026rdquo; and \u0026ldquo;routing-only\u0026rdquo; domains are used for routing of DNS queries: look-ups for hostnames ending in those domains (hence also single label names, if any \u0026ldquo;search domains\u0026rdquo; are listed), are routed to the DNS servers configured for this interface. The domain routing logic is particularly useful on multi-homed hosts with DNS servers serving particular private DNS zones on each interface.\nThe \u0026ldquo;routing-only\u0026rdquo; domain \u0026ldquo;~.\u0026rdquo; (the tilde indicating definition of a routing domain, the dot referring to the DNS root domain which is the implied suffix of all valid DNS names) has special effect. It causes all DNS traffic which does not match another configured domain routing entry to be routed to DNS servers specified for this interface. This setting is useful to prefer a certain set of DNS servers if a link on which they are connected is available.\nGet the new config:\n$ resolvectl Global Protocols: -LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported resolv.conf mode: stub DNS Servers: 1.1.1.1 127.0.0.153 DNS Domain: ~. Link 2 (eth0) Current Scopes: DNS Protocols: +DefaultRoute -LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported DNS Servers: 127.0.0.153 DNS Domain: test.example And now if we re-run our query:\n$ resolvectl query fb.com fb.com: 2a03:2880:f189:80:face:b00c:0:25de -- link: eth0 163.70.147.35 -- link: eth0 -- Information acquired via protocol DNS in 17.7ms. -- Data is authenticated: no; Data was acquired via local or encrypted transport: no -- Data from: network And in the tcpdump output we can see only requests to the global resolver:\n$ sudo tcpdump -i any -s0 -A -n port 53 19:10:38.312632 eth0 Out IP 192.168.5.15.41775 \u0026gt; 1.1.1.1.53: 37889+ [1au] AAAA? fb.com. (35) 19:10:38.312656 eth0 Out IP 192.168.5.15.50771 \u0026gt; 1.1.1.1.53: 14510+ [1au] A? fb.com. (35) 19:10:38.327208 eth0 In IP 1.1.1.1.53 \u0026gt; 192.168.5.15.41775: 37889 1/0/1 AAAA 2a03:2880:f189:80:face:b00c:0:25de (63) E..[ 19:10:38.329088 eth0 In IP 1.1.1.1.53 \u0026gt; 192.168.5.15.50771: 14510 1/0/1 A 163.70.147.35 (51) And local domains still work:\n$ resolvectl query site site: 127.0.0.2 127.0.0.1 127.0.0.3 (site.test.example) -- Information acquired via protocol DNS in 1.7ms. -- Data is authenticated: no; Data was acquired via local or encrypted transport: no -- Data from: network and only ask local name server with the correct search domain:\n19:11:51.498355 lo In IP 127.0.0.1.43769 \u0026gt; 127.0.0.153.53: 57337+ [1au] AAAA? site.test.example. (46) 19:11:51.498462 lo In IP 127.0.0.1.55548 \u0026gt; 127.0.0.153.53: 44925+ [1au] A? site.test.example. (46) 19:11:51.499115 lo In IP 127.0.0.153.53 \u0026gt; 127.0.0.1.55548: 44925*- 3/0/1 A 127.0.0.2, A 127.0.0.1, A 127.0.0.3 (145) 19:11:51.499149 lo In IP 127.0.0.153.53 \u0026gt; 127.0.0.1.43769: 57337*- 0/1/1 (128) 10.4 Useful commands # A handy monitor sub command allows us to monitor the ongoing queries:\n$ sudo resolvectl monitor → Q: cf.com IN AAAA ← S: success ← A: cf.com IN SOA merlin.ns.cloudflare.com dns.cloudflare.com 2343786646 10000 2400 604800 1800 → Q: google.com IN AAAA ← S: success ← A: google.com IN AAAA 2a00:1450:4009:823::200e As was mentioned above systemd-resolved provides a local cache:\n$ sudo resolvectl show-cache Scope protocol=dns ifindex=2 ifname=eth0 No entries. Scope protocol=dns google.com IN AAAA 2a00:1450:4009:823::200e google.com IN MX 10 smtp.google.com 10.5 Querying systemd-resolved # Using standard resolver methods like getaddrinfo() or connecting to /etc/resolv.conf nameservers over the DNS protocol will not provide the full power of per-link resolvers and search domains. Therefore, if your application truly needs this information, you can use two alternative interfaces provided by systemd-resolved:\nVarlink D-Bus 10.5.1 Varlink # Install dependencies:\n$ sudo apt install python3-varlink meson make $ python3 -m varlink.cli info unix:/run/systemd/resolve/io.systemd.Resolve Vendor: The systemd Project Product: systemd (systemd-resolved) Version: 255 (255.4-1ubuntu8.1) URL: https://systemd.io/ Interfaces: io.systemd io.systemd.Resolve org.varlink.service Build cli tool:\n$ git co https://github.com/varlink/libvarlink $ cd libvarlink $ make build Run:\n$ build/tool$ ./varlink help unix:/run/systemd/resolve/io.systemd.Resolve/io.systemd.Resolve $ ./varlink call unix:/run/systemd/resolve/io.systemd.Resolve/io.systemd.Resolve.ResolveHostname \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;site\u0026#34;}\u0026#39; { \u0026#34;addresses\u0026#34;: [ { \u0026#34;address\u0026#34;: [ 127, 0, 0, 1 ], \u0026#34;family\u0026#34;: 2 }, { \u0026#34;address\u0026#34;: [ 127, 0, 0, 2 ], \u0026#34;family\u0026#34;: 2 }, { \u0026#34;address\u0026#34;: [ 127, 0, 0, 3 ], \u0026#34;family\u0026#34;: 2 } ], \u0026#34;flags\u0026#34;: 8388609, \u0026#34;name\u0026#34;: \u0026#34;site.test.example\u0026#34; } The additional features can be called by specifying the interface by setting ifindex.\n10.5.2 D-Bus # Another way to interact with an extended systemd-resolved interface is via D-Bus.\n$ busctl status org.freedesktop.resolve1 PID=3478 PPID=1 TTY=n/a UID=991 EUID=991 SUID=991 FSUID=991 GID=991 EGID=991 SGID=991 FSGID=991 SupplementaryGIDs=991 Comm=systemd-resolve CommandLine=/usr/lib/systemd/systemd-resolved Label=unconfined CGroup=/system.slice/systemd-resolved.service Unit=systemd-resolved.service Slice=system.slice UserUnit=n/a UserSlice=n/a Session=n/a AuditLoginUID=n/a AuditSessionID=n/a UniqueName=:1.42 EffectiveCapabilities=cap_net_raw PermittedCapabilities=cap_net_raw Documentation for the API format could be found here: https://www.freedesktop.org/software/systemd/man/latest/org.freedesktop.resolve1.html.\nRun to resolve site hostname:\n$ busctl call org.freedesktop.resolve1 /org/freedesktop/resolve1 org.freedesktop.resolve1.Manager ResolveHostname isit 0 site 0 0 result:\na(iiay)st 3 0 2 4 127 0 0 3 0 2 4 127 0 0 1 0 2 4 127 0 0 2 \u0026#34;site.test.example\u0026#34; 8388609 Run to resolve fb.com hostname:\n$ busctl call org.freedesktop.resolve1 /org/freedesktop/resolve1 org.freedesktop.resolve1.Manager ResolveHostname isit 0 fb.com 0 0 Result:\na(iiay)st 2 2 2 4 157 240 221 35 2 10 16 42 3 40 128 241 88 0 130 250 206 176 12 0 0 37 222 \u0026#34;fb.com\u0026#34; 8388609 Read next chapter → "},{"id":26,"href":"/docs/page-cache/6-cgroup-v2-and-page-cache/","title":"cgroup v2 and Page Cache","section":"Linux Page Cache series","content":" cgroup v2 and Page Cache # Last updated: Oct 2025 Contents\nOverview Memory cgroup files Pressure Stall Information (PSI) Writeback and I/O Memory and I/O cgroup ownership Safe ad-hoc tasks The cgroup subsystem is the way to distribute and limit system resources fairly. It organizes all data in a hierarchy where the leaf nodes depend on their parents and inherit their settings. In addition, the cgroup provides a lot of helpful resource counters and statistics.\nThe control groups are everywhere. Even though you may not use them explicitly, they are already turned on by default in all modern GNU/Linux distributives and got integrated into systemd. It means that each service in a modern linux system runs under its own cgroup.\nOverview # We already touched the cgroup subsystem several times during this series, but let\u0026rsquo;s take a closer look at the entire picture now. The cgroup plays a critical role in the understanding Page Cache usage. It also helps to debug issues and configure software better by providing detailed stats. As was told earlier, the LRU lists use cgroup memory limits to make eviction decisions and to size the length of the LRU lists.\nAnother important topic in cgroup v2, which was unachievable with the previous v1, is a proper way of tracking Page Cache I/O writebacks. The v1 can\u0026rsquo;t understand which memory cgroup generates disk IOPS and therefore, it incorrectly tracks and limits disk operations. Fortunately, the new v2 version fixes these issues. It already provides a bunch of new features which can help with Page Cache writeback.\nThe simplest way to find out all cgroups and their limits are to go to the /sys/fs/cgroup. But you can use more convenient ways to get such info:\nsystemd-cgls and systemd-top to understand what cgroups systemd has; below a top-like tool for cgroups https://github.com/facebookincubator/below Memory cgroup files # Now let\u0026rsquo;s review the most important parts of the cgroup memory controller from the perspective of Page Cache.\nmemory.current – shows the total amount of memory currently used by the cgroup and its descendants. It, of course, includes Page Cache size. NOTE\nIt may be tempting to use this value in order to set your cgroup/container memory limit, but wait a bit for the following chapter.\nmemory.stat – shows a lot of memory counters, the most important for us can be filtered by file keyword: $ grep file /sys/fs/cgroup/user.slice/user-1000.slice/session-3.scope/memory.stat file 19804160 ❶ file_mapped 0 ❷ file_dirty 0 ❸ file_writeback 0 ❹ inactive_file 6160384 ❺ active_file 13643776 ❺ workingset_refault_file 0 ❻ workingset_activate_file 0 ❻ workingset_restore_file 0 ❻ where\n❶ file – the size of the Page Cache; ❷ file_mapped – mapped file memory size with mmap(); ❸ file_dirty – dirty pages size; ❹ file_writeback – how much data is being flushing at the moment; ❺ inactive_file and active_file – sizes of the LRU lists; ❻ workingset_refault_file, workingset_activate_file and workingset_restore_file – metrics to better understand memory thrashing and refault logic. memory.numa_stat – shows the above stats but for each NUMA node.\nmemory.min, memory.low, memory.high and memory.max – cgroup limits. I don\u0026rsquo;t want to repeat the cgroup v2 doc and recommend you to go and read it first. But what you need to keep in mind is that using the hard max or min limits is not the best strategy for your applications and systems. The better approach you can choose is to set only low and/or high limits closer to what you think is the working set size of your application. We will talk about measuring and predicting in the next section.\nmemory.events – shows how many times the cgroup hit the above limits:\nmemory.events low 0 high 0 max 0 oom 0 oom_kill 0 memory.pressure – this file contains Pressure Stall Information (PSI). It shows the general cgroup memory health by measuring the CPU time that was lost due to lack of memory. This file is the key to understanding the reclaiming process in the cgroup and, consequently, Page Cache. Let\u0026rsquo;s talk about PSI in more detail. Pressure Stall Information (PSI) # Back before PSI times, it was hard to tell whether a system and/or a cgroup has resource contention or not; whether a cgroup limits are overcommitted or under-provisioned. If the limit for a cgroup can be set lower, then where is its threshold? The PSI feature mitigates these confusions and not only allows us to get this information in real-time but also allows us to set up user-space triggers and get notifications to maximize hardware utilization without service degradation and OOM risks.\nThe PSI works for memory, CPU and I/O controllers. For example, the output for memory:\nsome avg10=0.00 avg60=0.00 avg300=0.00 total=0 full avg10=0.00 avg60=0.00 avg300=0.00 total=0 where\nsome – means that at least one task was stalled on memory for some average percentage of wall-time during 10, 60 and 300 seconds. The \u0026ldquo;total\u0026rdquo; field shows the absolute value in microseconds in order to reveal any spikes; full – means the same but for all tasks in the cgroup. This metric is a good indication of issues and usually means underprovisioning of the resource or wrong software settings. EXAMPLE\nsystemd-oom daemon, which is a part of modern GNU/Linux systems, uses the PSI to be more proactive than kernel\u0026rsquo;s OOM in recognition of memory scarcity and finding targets for killing.\nI also highly recommend reading the original PSI doc.\nWriteback and I/O # One of the most significant features of the cgroup v2 implementation is the possibility to track, observe and limit Page Cache async writeback for each cgroup. Nowadays, the kernel writeback process can identify which cgroup I/O limit to use in order to persist dirty pages to disks.\nBut what is also important is that it works in another direction too. If a cgroup experiences memory pressure and tries to reclaim some pages by flushing its dirty pages, it will use its own I/O limits and won\u0026rsquo;t harm the other cgroups. Thus the memory pressure translates into the disk I/O and if there is a lot of writes, eventually, into the disk pressure for the cgroup. Both controllers have the PSI files, which should be used for proactive management and tuning your software settings.\nIn order to control dirty pages flush frequency, the linux kernel has several sysctl knobs. If you want, you can make the background writeback process more or less aggressive:\n$ sudo sysctl -a | grep dirty vm.dirty_background_bytes = 0 vm.dirty_background_ratio = 10 vm.dirty_bytes = 0 vm.dirty_expire_centisecs = 3000 vm.dirty_ratio = 20 vm.dirty_writeback_centisecs = 500 vm.dirtytime_expire_seconds = 43200 Some of the above works for cgroups too. The kernel chooses and applies what reaches first for the entire system or for a cgroup.\nThe cgroup v2 also brings new I/O controllers: io.cost and io.latency. They provide 2 different approaches for limiting and guaranteeing disk operations. Please, read the cgroup v2 documentation for more details and distinctions. But I would say that if your setup is not complex, starting with less invasive io.latency makes sense.\nAs with the memory controller, the kernel also provides a bunch of files to control and observe I/O:\nio.stat – the stat file with per device data; io.latency – the latency target time in microseconds; io.pressure – the PSI file; io.weight – the target weight if io.cost was chosen; io.cost.qos and io.cost.model – the configuration file of the io.cost cgroup controller. Memory and I/O cgroup ownership # Several processes from multiple cgroups can obviously work with the same files. For example, cgroup1 can open and read the first 10 KiB of the file, and sometime later, another cgroup2 can append 2 KiB to the end of the same file and read the first 4KiB. The question is, whose memory and I/O limits will the kernel use?\nThe logic of memory ownership (therefore and Page Cache) is built based on each page. The ownership of a page is charged on the first access (page fault) and won\u0026rsquo;t switch to any other cgroup until this page will be completely reclaimed and evicted. The term ownership means that these pages will be used to calculate the cgroup Page Cache usage and will be included in all stats.\nFor example, cgroup1 is the owner of the first 10KiB, and cgroup2 – is the owner of the last 2KiB. No matter what cgroup1 will do with the file, it can even close it, cgroup1 remains the owner of the first 4KiB (not all 10KiB) as long as cgroup2 works with this first 4KiB of the file. In this situation, kernel keeps the pages in Page Caches and keeps updating LRU lists accordingly.\nFor the cgroup I/O, ownership works per inode. So for our example cgroup2 owns all writeback operations for the file. The inode is assigned to the cgroup on the first writeback, but unlike the memory ownership logic, the I/O ownership may migrate to another cgroup if the kernel notices that this other cgroup generates more dirty pages.\nIn order to troubleshoot memory ownership, we should use the pair of procfs files: /proc/pid/pagemap and /proc/kpagecgroup. The page-type tool supports showing per page cgroup information, but it\u0026rsquo;s hard to use it for a directory of files and get a well-formatted output. That\u0026rsquo;s why I wrote my own cgtouch tool in order to troubleshoot cgroup memory ownership.\n$ sudo go run ./main.go /var/tmp/ -v /var/tmp/file1.db cgroup inode percent pages path - 85.9% 28161 not charged 1781 14.1% 4608 /sys/fs/cgroup/user.slice/user-1000.slice/session-3.scope -- /var/tmp/ubuntu-21.04-live-server-amd64.iso cgroup inode percent pages pat - 0.0% 0 not charged 2453 100.0% 38032 /sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/app.slice/run-u10.service -- Files: 2 Directories: 7 Resident Pages: 42640/70801 166.6M/276.6M 60.2% cgroup inode percent pages path - 39.8% 28161 not charged 1781 6.5% 4608 /sys/fs/cgroup/user.slice/user-1000.slice/session-3.scope 2453 53.7% 38032 /sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/app.slice/run-u10.service Safe ad-hoc tasks # Let\u0026rsquo;s assume we need to run the wget command or manually install some packages by calling a configuration management system (e.g. saltstack). Both of these tasks can be unpredictably heavy for disk I/O. In order to run them safely and not interact with any production load, we should not run them in the root cgroup or the current terminal cgroup, because they usually don\u0026rsquo;t have any limits. So we need a new cgroup with some limits. It would be very tedious and cumbersome to manually create a cgroup for your task and manually configure it for every ad-hoc task. But fortunately, we don\u0026rsquo;t have to, so all modern GNU/Linux distributives come with the systemd out of the box with cgroup v2. The systemd-run with many other cool features from the systemd makes our life easier and saves a lot of time.\nSo, for example, wget task can be run in the following manner:\nsystemd-run --user -P -t -G --wait -p MemoryMax=12M wget http://ubuntu.ipacct.com/releases/21.04/ubuntu-21.04-live-server-amd64.iso Running as unit: run-u2.service ⬅ LOOK HERE Press ^] three times within 1s to disconnect TTY. --2021-09-11 19:53:33-- http://ubuntu.ipacct.com/releases/21.04/ubuntu-21.04-live-server-amd64.iso Resolving ubuntu.ipacct.com (ubuntu.ipacct.com)... 195.85.215.252, 2a01:9e40::252 Connecting to ubuntu.ipacct.com (ubuntu.ipacct.com)|195.85.215.252|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 1174243328 (1.1G) [application/octet-stream] Saving to: ‘ubuntu-21.04-live-server-amd64.iso.5’ ... The run-u2.service is my brand new cgroup with a memory limit. I can get its metrics:\n$ find /sys/fs/cgroup/ -name run-u2.service /sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/app.slice/run-u2.service $ cat /sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/app.slice/run-u2.service/memory.pressure some avg10=0.00 avg60=0.00 avg300=0.00 total=70234 full avg10=0.00 avg60=0.00 avg300=0.00 total=69717 $ grep file /sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/app.slice/run-u2.service/memory.stat file 11100160 file_mapped 0 file_dirty 77824 file_writeback 0 file_thp 0 inactive_file 5455872 active_file 5644288 workingset_refault_file 982 workingset_activate_file 0 workingset_restore_file 0 As you can see from the above we have near 12MiB file memory and some refault.\nTo get all power of systemd and cgroup please read its resource control doc.\nRead next chapter → "},{"id":27,"href":"/docs/resolver-dual-stack-application/11-querying-nameservers-on-dual-stack-hosts/","title":"Querying Nameservers on dual-stack hosts","section":"DNS resolvers and Dual-Stack applications","content":" 11. Querying Nameservers on dual-stack hosts # Last updated: Oct 2025 The already seen RFC 8305 Happy Eyeballs Version 2: Better Connectivity Using Concurrency force the same preference for IPv6 name servers as it does for establishing new connections:\nIf multiple DNS server addresses are configured for the current network, the client may have the option of sending its DNS queries over IPv4 or IPv6. In keeping with the Happy Eyeballs approach, queries SHOULD be sent over IPv6 first (note that this is not referring to the sending of AAAA or A queries, but rather the address of the DNS server itself and IP version used to transport DNS messages). If DNS queries sent to the IPv6 address do not receive responses, that address may be marked as penalized and queries can be sent to other DNS server addresses.\nBut unfortunately glibc resolver doesn’t support this logic. From man 5 resolv.conf:\nnameserver Name server IP address Internet address of a name server that the resolver should query, either an IPv4 address (in dot notation), or an IPv6 address in colon (and possibly dot) notation as per RFC 2373. Up to MAXNS (currently 3, see \u0026amp;lt;resolv.h\u0026gt;) name servers may be listed, one per keyword. If there are multiple servers, the resolver library queries them in the order listed. If no nameserver entries are present, the default is to use the name server on the local machine. (The algorithm used is to try a name server, and if the query times out, try the next, until out of name servers, then repeat trying all the name servers until a maximum number of retries are made.)\nThis essentially means using nameservers in order, regardless of which IP stack version they belong to.\nIf we remember, the musl version of getaddrinfo() sends all requests in parallel to all nameservers. Thus we can say that their implementation is close to the RFC 8305.\nUnfortunately, the systemd-resolved also doesn’t do this as well even though it allows to configure DNS servers in a more flexible way:\nDNS=\nA space-separated list of IPv4 and IPv6 addresses to use as system DNS servers.\nEach address can optionally take a port number separated with \u0026ldquo;:\u0026rdquo;, a network interface name or index separated with \u0026ldquo;%\u0026rdquo;, and a Server Name Indication (SNI) separated with \u0026ldquo;#\u0026rdquo;. When IPv6 address is specified with a port number, then the address must be in the square brackets. That is, the acceptable full formats are \u0026ldquo;111.222.333.444:9953%ifname#example.com\u0026rdquo; for IPv4 and \u0026ldquo;[1111:2222::3333]:9953%ifname#example.com\u0026rdquo; for IPv6. DNS requests are sent to one of the listed DNS servers in parallel to suitable per-link DNS servers acquired from systemd-networkd.service(8) or set at runtime by external applications. For compatibility reasons, if this setting is not specified, the DNS servers listed in /etc/resolv.conf are used instead, if that file exists and any servers are configured in it. This setting defaults to the empty list.\nBut the code of choosing a DNS server to query is a loop:\nDnsServer *manager_get_dns_server(Manager *m) { Link *l; assert(m); /* Try to read updates resolv.conf */ manager_read_resolv_conf(m); /* If no DNS server was chosen so far, pick the first one */ if (!m-\u0026gt;current_dns_server || /* In case m-\u0026gt;current_dns_server != m-\u0026gt;dns_servers */ manager_server_is_stub(m, m-\u0026gt;current_dns_server)) manager_set_dns_server(m, m-\u0026gt;dns_servers); while (m-\u0026gt;current_dns_server \u0026amp;\u0026amp; manager_server_is_stub(m, m-\u0026gt;current_dns_server)) { manager_next_dns_server(m, NULL); if (m-\u0026gt;current_dns_server == m-\u0026gt;dns_servers) manager_set_dns_server(m, NULL); } The Nginx allows to specify DNS servers in its config, but the addresses are asked in round-robin order without any preferences for IPv6.\nSyntax: resolver address ... [valid=time] [ipv4=on|off] [ipv6=on|off] [status_zone=zone]; Default: -- Context: http, server, location Configures name servers used to resolve names of upstream servers into addresses, for example: resolver 127.0.0.1 [::1]:5353; The address can be specified as a domain name or IP address, with an optional port (1.3.1, 1.2.2). If port is not specified, the port 53 is used. Name servers are queried in a round-robin fashion. The c-ares stub resolver library unfortunately also doesn\u0026rsquo;t prefer IPv6 over IPv4 DNS server:\nares_status_t ares__send_query(struct query *query, const ares_timeval_t *now) { … /* Choose the server to send the query to */ if (channel-\u0026gt;rotate) { /* Pull random server */ server = ares__random_server(channel); } else { /* Pull server with failover behavior */ server = ares__failover_server(channel); } … } Therefore, this is an area open for future improvement.\nRead next chapter → "},{"id":28,"href":"/docs/resolver-dual-stack-application/12-present-and-future-of-resolvers-and-dns-related-features/","title":"The Present and the future of resolvers and DNS related features","section":"DNS resolvers and Dual-Stack applications","content":" 12. The Present and the future of resolvers and DNS related features # Last updated: Oct 2025 Contents\n12.1 New DNS record: HTTPS 12.2. DNSSEC 12.3 DNS over TLS (DoT), DSN over HTTPS (DoH) and DNS over QUICK (DoQ) 12.4 Oblivious DNS 12.5. DNS Push Notifications Let me briefly review some important features and topics related to DNS, stub resolvers, and dual-stack applications. While these are beyond the scope of this series, they are worth mentioning.\n12.1 New DNS record: HTTPS # DNS has introduced a new and interesting resource record called HTTPS. This record addresses problems related to web service clients. Typically, when a client has only a domain name without additional information, it connects using plain text port 80. The server usually redirects to port 443 with HTTPS and sets an HSTS header. However, this initial plain text request-response has security issues and is vulnerable to Man-in-the-Middle (MitM) attacks.\nAnother problem is speed: additional redirects are not cheap, and the client often upgrades to HTTP/2 or even HTTP/3 afterward. All of this adds unnecessary latency to the client side.\nTo address these issues, the HTTPS record plays a canary role for existing HTTPS services. It provides supported ALPN protocols (such as HTTP/2 and HTTP/3) and returns A and/or AAAA addresses for connection in one response saving latency and packet rate.\nFor example;\n$ dig @1.1.1.1 cloudflare.com HTTPS ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.18.28-0ubuntu0.24.04.1-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; @1.1.1.1 cloudflare.com HTTPS ; (1 server found) ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 8180 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;cloudflare.com. IN HTTPS ;; ANSWER SECTION: cloudflare.com. 51 IN HTTPS 1 . alpn=\u0026#34;h3,h2\u0026#34; ipv4hint=104.16.132.229,104.16.133.229 ipv6hint=2606:4700::6810:84e5,2606:4700::6810:85e5 ;; Query time: 11 msec ;; SERVER: 1.1.1.1#53(1.1.1.1) (UDP) ;; WHEN: Fri Aug 02 01:18:53 BST 2024 ;; MSG SIZE rcvd: 116 12.2. DNSSEC # The Domain Name System Security Extensions (DNSSEC) is a feature of the Domain Name System (DNS) that authenticates responses to domain name lookups.\nWe can enable it with systemd-resolved for local stub resolver:\nNote that DNSSEC validation requires retrieval of additional DNS data, and thus results in a small DNS lookup time penalty.\nTo verify and query DNSSEC keys you can continue using dig, but the recommended and more convenient tool is delv (man 1 delv).\n$ delv @1.1.1.1 microsoft.com ; unsigned answer microsoft.com. 130 IN A 20.70.246.20 microsoft.com. 130 IN A 20.76.201.171 microsoft.com. 130 IN A 20.112.250.133 microsoft.com. 130 IN A 20.231.239.246 microsoft.com. 130 IN A 20.236.44.162 For failing test please use:\ndnssec-failed.org rhybar.cz $ delv @1.1.1.1 dnssec-failed.org ;; resolution failed: SERVFAIL In systemd-resolved the default for DNSSEC is \u0026ldquo;allow-downgrade\u0026rdquo;, but it could be hardened to yes.\n$ cat /etc/systemd/resolved.conf [Resolve] ... DNSSEC=yes # DNSSEC=allow-downgrade ... 12.3 DNS over TLS (DoT), DSN over HTTPS (DoH) and DNS over QUICK (DoQ) # DNSSEC provides data integrity and authentication, but the content of requests and responses remains in plain text. To address this issue, DNS over TLS (DoT) and later DNS over HTTPS (DoH) were introduced. The main difference between these two is the transport and port used. DoH is more private because it mimics regular HTTPS traffic (using 443 port and https transport), making it harder to detect and block.\nThe https://www.dnscrypt.org/ is a good starting point.\n12.4 Oblivious DNS # If we go further and consider privacy with DoT and DoH, there is a concern that a DNS forwarder knows too much about your requests. This led to the development of Oblivious DNS (oDNS), which introduces the concepts of an oDNS relay and an oDNS target. In this system, the client encrypts its request using the public key of the target and then proxies the encrypted request through the relay. Relay sends a request to the target without client identification information. This process helps to hide the client’s identity from the oDNS target and the request content from the oDNS relay.\nFor a great client with comprehensive documentation, see https://github.com/natesales/q\n12.5. DNS Push Notifications # There is an interesting and reasonable new RFC 8765 DNS Push Notifications, which suggests keeping an open connection between the DNS server and stub resolver to send push notifications when something changes. This idea could restore DNS as the mechanism for service discovery.\nRead next chapter → "},{"id":29,"href":"/docs/resolver-dual-stack-application/troubleshooting-tools-for-resolvers-and-dns/","title":"Troubleshooting tools for resolvers and DNS","section":"DNS resolvers and Dual-Stack applications","content":" Tools for troubleshooting in one place # Last updated: Oct 2025 Let me reiterate and consolidate all the tools that can be used to troubleshoot applications and systems when the stub resolver is under suspicion.\n• getent # man 1 getent\nWhen you need to query hostname via all NSS modules:\n$ getent host microsoft.com $ getent ahost microsoft.com • tcpdump # To dump in a user friendly format all requests to 53 port:\n$ sudo tcpdump -i any -s0 -A -n port 53 • dig # Query DNS recursor:\n$ dig @1.1.1.1 microsoft.com AAAA • delv # man 1 delv\nIf you need to debug or check DNSSEC:\n$ delv @1.1.1.1 dnssec-failed.org • resolvectl # With local systemd-resolved, query it for a hostname:\n$ resolvectl query facebook.com Get cache content:\n$ resolvectl show-cache Monitor current queries:\n$ resolvectl monitor • strace # Understand which files were opened to guess the resolver by the read config files:\n$ sudo strace -f -s0 -e openat app Check the network activity of an app:\n$ sudo strace -f -s0 -e trace=network app • gdb # It’s possible to set a breakpoint to libc stub resolver function getaddinfo() and/or other functions:\n$ gdb ./resolver (gdb) set args microsoft.com # ① (gdb) break getaddrinfo # ② Breakpoint 1 at 0x11aa90 (gdb) run # ③ ① – setting cli args if need;\n② – set a breakpoint for getaddinfo();\n③ – run binary.\nYou can also attach to a running process with gdb by:\n$ sudo gdb -p PID But be careful, it will suspend the execution of a process.\nThe one note here is to check whether you need debug info for the binary or not.\n"},{"id":30,"href":"/docs/page-cache/7-how-much-memory-my-program-uses-or-the-tale-of-working-set-size/","title":"Unique set size and working set size","section":"Linux Page Cache series","content":" How much memory my program uses or the tale of working set size # Last updated: Oct 2025 Contents\nIt’s all about who counts or the story of unique set size Idle pages and working set size Calculating memory limits with Pressure Stall Information (PSI) … and what about writeback? Currently, in the world of containers, auto-scaling, and on-demand clouds, it\u0026rsquo;s vital to understand the resource needs of services both in norman regular situations and under pressure near the software limits. But every time someone touches on the topic of memory usage, it becomes almost immediately unclear what and how to measure. RAM is a valuable and often expensive type of hardware. In some cases, its latency is even more important than disk latency. Therefore, the Linux kernel tries as hard as it can to optimize memory utilization, for instance by sharing the same pages among processes. In addition, the Linux Kernel has its Page Cache in order to improve storage IO speed by storing a subset of the disk data in memory. Page Cache not only, by its nature, performs implicit memory sharing, which usually confuses users, but also actively asynchronously works with the storage in the background. Thus, Page Cache brings even more complexity to the table of memory usage estimation.\nIn this chapter, I\u0026rsquo;m demonstrating some approaches you can use to determine your initial values for the memory (and thus Page Cache) limits and start your journey from a decent starting point.\nIt\u0026rsquo;s all about who counts or the story of unique set size # The 2 most common questions I\u0026rsquo;ve heard about memory and Linux are:\nWhere is all my free memory? How much memory does you/my/their application/service/database use? The first question\u0026rsquo;s answer should already be obvious to the reader (whispering \u0026ldquo;Page Cache\u0026rdquo;). But the second one is much more trickier. Usually, people think that the RSS column from the top or ps output is a good starting point to evaluate memory utilization. Although this statement may be correct in some cases, it can usually lead to misunderstanding of Page Cache importance and its impact on the service performance and reliability.\nLet\u0026rsquo;s take a well-known top (man 1 top) tool as an example in order to investigate its memory consumption. It\u0026rsquo;s written in C and it does nothing but prints process\u0026rsquo; stats in the loop. top doesn\u0026rsquo;t heavily work with disks and thus Page Cache. It doesn\u0026rsquo;t touch the network. Its only purpose is to read data from the procfs and to show it to the user in a friendly format. So it should be easy to understand its working set, shouldn\u0026rsquo;t it?\nLet\u0026rsquo;s start the top process in a new cgroup:\n$ systemd-run --user -P -t -G --wait top And in another terminal let\u0026rsquo;s start our learning. Begin with the ps:\n$ ps axu | grep top USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ... vagrant 611963 0.1 0.2 10836 4132 pts/4 Ss+ 11:55 0:00 /usr/bin/top ... ⬆ LOOK HERE As you can see from the above, the top process uses ~4MiB of memory according to the ps output.\nNow let\u0026rsquo;s get more details from the procfs and its /proc/pid/smaps_rollup file, which is basically a sum of all memory areas from the /proc/pid/smaps. For my PID:\n$ cat /proc/628011/smaps_rollup 55df25e91000-7ffdef5f7000 ---p 00000000 00:00 0 [rollup] Rss: 3956 kB ⓵ Pss: 1180 kB ⓶ Pss_Anon: 668 kB Pss_File: 512 kB Pss_Shmem: 0 kB Shared_Clean: 3048 kB ⓷ Shared_Dirty: 0 kB ⓸ Private_Clean: 240 kB Private_Dirty: 668 kB Referenced: 3956 kB ⓹ Anonymous: 668 kB ⓺ ... Where we mostly care about the following rows:\n⓵ – A well know RSS metric and what we\u0026rsquo;ve seen in the ps output. ⓶ – PSS stands for the process\u0026rsquo; proportional share memory. It\u0026rsquo;s an artificial memory metric and it should give you some insights about memory sharing: The \u0026ldquo;proportional set size\u0026rdquo; (PSS) of a process is the count of pages it has in memory, where each page is divided by the number of processes sharing it. So if a process has 1000 pages all to itself and 1000 shared with one other process, its PSS will be 1500.\n⓷ Shared_Clean – is an interesting metric. As we assumed earlier, our process should not use any Page Cache in theory, but it turns out it does use Page Cache. And as you can see, it\u0026rsquo;s the predominant part of memory usage. If you open a per area file /proc/pid/smaps, you can figure out that the reason is shared libs. All of them were opened with mmap() and are resident in Page Cache. ⓸ Shared_Dirty – If our process writes to files with mmap(), this line will show the amount of unsaved dirty Page Cache memory. ⓹ Referenced - indicates the amount of memory the process has marked as referenced or accessed so far. We touched on this metric in mmap() section. And if there is no memory pressure, it should be close to RSS. ⓺ Anonymous – shows the amount of memory that does not belong to any files. From the above, we can see that, although top\u0026rsquo;s RSS is 4MiB, most of its RSS is hidden in Page Cache. And in theory, if these pages become inactive for a while, the kernel can evict them from memory.\nLet\u0026rsquo;s take a look at the cgroup stats as well:\n$ cat /proc/628011/cgroup 0::/user.slice/user-1000.slice/user@1000.service/app.slice/run-u2.service $ cat /sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/app.slice/run-u2.service/memory.stat anon 770048 file 0 ... file_mapped 0 file_dirty 0 file_writeback 0 ... inactive_anon 765952 active_anon 4096 inactive_file 0 active_file 0 ... We can not see any file memory in the cgroup. That is another great example of the cgroup memory charging feature. Another cgroup has already accounted these libs.\nAnd to finish and recheck ourselves, let\u0026rsquo;s use the page-type tool:\n$ sudo ./page-types --pid 628011 --raw flags page-count MB symbolic-flags long-symbolic-flags 0x2000010100000800 1 0 ___________M_______________r_______f_____F__ mmap,reserved,softdirty,file 0xa000010800000868 39 0 ___U_lA____M__________________P____f_____F_1 uptodate,lru,active,mmap,private,softdirty,file,mmap_exclusive 0xa00001080000086c 21 0 __RU_lA____M__________________P____f_____F_1 referenced,uptodate,lru,active,mmap,private,softdirty,file,mmap_exclusive 0x200001080000086c 830 3 __RU_lA____M__________________P____f_____F__ referenced,uptodate,lru,active,mmap,private,softdirty,file 0x8000010000005828 187 0 ___U_l_____Ma_b____________________f_______1 uptodate,lru,mmap,anonymous,swapbacked,softdirty,mmap_exclusive 0x800001000000586c 1 0 __RU_lA____Ma_b____________________f_______1 referenced,uptodate,lru,active,mmap,anonymous,swapbacked,softdirty,mmap_exclusive total 1079 4 We can see that the memory of the top process has file mmap() areas and thus uses Page Cache.\nNow let\u0026rsquo;s get a unique memory set size for our top process. The unique memory set size or USS for the process is an amount of memory which only this target process uses. This memory could be sharable but still, be in the USS if no other processes use it.\nWe can use the page-types with -N flag and some shell magic to calculate the USS of the process:\n$ sudo ../vm/page-types --pid 628011 --raw -M -l -N | awk \u0026#39;{print $2}\u0026#39; | grep -E \u0026#39;^1$\u0026#39; | wc -l 248 The above means that 248 pages or 992 KiB is the unique set size (USS) of the top process.\nOr we can use our knowledge about /proc/pid/pagemap, /proc/kpagecount and /proc/pid/maps and write our own tool to get the unique set size. The full code of such tool can be found in the github repo.\nIf we run it, we should get the same output as page-type gave us:\n$ sudo go run ./main.go 628011 248 Now that we understand how it can be hard to estimate the memory usage and the importance of Page Cache in such calculations, we are ready to make a giant leap forward and start thinking about software with more active disk activities.\nIdle pages and working set size # Readers who have gotten this far may be curious about one more kernel file: /sys/kernel/mm/page_idle.\nYou can use it to estimate the working set size of a process. The main idea is to mark some pages with the special idle flag and, after some time, check the difference-making assumptions about the working data set size.\nYou can find great reference tools in Brendan Gregg\u0026rsquo;s repository.\nLet\u0026rsquo;s run it for our top process:\n$ sudo ./wss-v1 628011 60 Watching PID 628011 page references during 60.00 seconds... Est(s) Ref(MB) 60.117 2.00 The above means that from the 4MiB of RSS data, the process uses only 2 MiB during the 60-second interval.\nFor more information, you can also read this LWN article.\nThe drawbacks of this method are the following:\nit can be slow for a process with a huge memory footprint; all measurements happen in the user space and thus consume additional CPU; it completely detached from the possible writeback pressure your process can generate. Although it could be a reasonable starting limit for your containers, I will show you a better approach using cgroup stats and pressure stall information (PSI).\nCalculating memory limits with Pressure Stall Information (PSI) # As you can see throughout the series, I emphasize that running all services in their own cgroups with carefully configured limits is very important. It usually leads to better service performance and more uniform and correct use of system resources.\nBut what is still unclear is where to start. Which value to choose? Is it good to use the memory.current value? Or use the unique set size? Or estimate the working set size with the idle page flag? Though all these ways may be useful in some situations, I would suggest using the following PSI approach for a general case.\nOne more note about the memory.current before I continue with the PSI. If a cgroup doesn\u0026rsquo;t have a memory limit and the system has a lot of free memory for the process, the memory.current simply shows all the memory (including Page Cache) that your application has touched up to that point. It can include a lot of garbage your application doesn\u0026rsquo;t need for its runtime. For example, logs records, unneeded libs, etc. Using the memory.current value as a memory limit would be wasteful for the system and will not help you in capacity planning.\nThe modern approach to address this hard question is to use PSI in order to understand how a cgroup reacts to new memory allocations and Page Cache evictions. senapi is a simple automated script that collects and parses the PSI info and adjusts the memory.high:\nLet\u0026rsquo;s experiment with my test MongoDB installation. I have 2.6GiB of data:\n$ sudo du -hs /var/lib/mongodb/ 2.4G /var/lib/mongodb/ Now I need to generate some random read queries. In mongosh I can run an infinite while loop and read a random record every 500 ms:\nwhile (true) { printjson(db.collection.aggregate([{ $sample: { size: 1 } }])); sleep(500); } In the second terminal window, I start the senpai with the mongodb service cgroup:\nsudo python senpai.py /sys/fs/cgroup/system.slice/mongodb.service 2021-09-05 16:39:25 Configuration: 2021-09-05 16:39:25 cgpath = /sys/fs/cgroup/system.slice/mongodb.service 2021-09-05 16:39:25 min_size = 104857600 2021-09-05 16:39:25 max_size = 107374182400 2021-09-05 16:39:25 interval = 6 2021-09-05 16:39:25 pressure = 10000 2021-09-05 16:39:25 max_probe = 0.01 2021-09-05 16:39:25 max_backoff = 1.0 2021-09-05 16:39:25 coeff_probe = 10 2021-09-05 16:39:25 coeff_backoff = 20 2021-09-05 16:39:26 Resetting limit to memory.current. ... 2021-09-05 16:38:15 limit=503.90M pressure=0.030000 time_to_probe= 1 total=1999415 delta=601 integral=3366 2021-09-05 16:38:16 limit=503.90M pressure=0.030000 time_to_probe= 0 total=1999498 delta=83 integral=3449 2021-09-05 16:38:16 adjust: -0.000840646891233154 2021-09-05 16:38:17 limit=503.48M pressure=0.020000 time_to_probe= 5 total=2000010 delta=512 integral=512 2021-09-05 16:38:18 limit=503.48M pressure=0.020000 time_to_probe= 4 total=2001688 delta=1678 integral=2190 2021-09-05 16:38:19 limit=503.48M pressure=0.020000 time_to_probe= 3 total=2004119 delta=2431 integral=4621 2021-09-05 16:38:20 limit=503.48M pressure=0.020000 time_to_probe= 2 total=2006238 delta=2119 integral=6740 2021-09-05 16:38:21 limit=503.48M pressure=0.010000 time_to_probe= 1 total=2006238 delta=0 integral=6740 2021-09-05 16:38:22 limit=503.48M pressure=0.010000 time_to_probe= 0 total=2006405 delta=167 integral=6907 2021-09-05 16:38:22 adjust: -0.00020961438729431614 As you can see, according to the PSI, 503.48M of memory should be enough to support my reading work load without any problems.\nThis is obviously a preview of the PSI features and for real production services, you probably should think about io.pressure as well.\n\u0026hellip; and what about writeback? # To be honest, this question is more difficult to answer. As I write this article, I do not know of a good tool for evaluating and predicting writeback and IO usage. However, the rule of thumb is to start with io.latency and then try to use io.cost if needed.\nThere is also an interesting new project resctl-demo which can help in proper limits identification.\nRead next chapter → "},{"id":31,"href":"/docs/page-cache/8-direct-io-dio/","title":"Direct IO","section":"Linux Page Cache series","content":" Direct IO (DIO) (NOT READY) # Last updated: Oct 2025 Contents\nWhy it’s good Why it’s bad and io_uring alternative As usual, there is always an exception to any rule. And Page Cache is no different. So let\u0026rsquo;s talk about file reads and writes, which can ignore Page Cache content.\nWhy it’s good # Some applications require low-level access to the storage subsystem and the linux kernel gives such a feature by providing O_DIRECT file open flag. This IO is called the Direct IO or DIO. A program, which opens a file with this flag, bypasses the kernel Page Cache completely and directly communicates with the VFS and the underlying filesystem.\nThe pros are:\nLower CPU usage and thus higher throughput you can get; Linux Async IO (man 7 aio) works only with DIO (io_submit); zero-copy Avoiding double buffering () between Page Cache and user-space buffers; More control over the writeback. \u0026hellip; Why it’s bad and io_uring alternative # need to align read and writes to the block size; not all file systems are the same in implementing DIO; DIO without Linux AIO is slow and not useful at all; not cross-platform; DIO and buffered IO can\u0026rsquo;t be performed at the same time for the file. \u0026hellip; DIO usually makes no sense without AIO, but AIO has a lot of bad design decisions:\nSo I think this is ridiculously ugly.\nAIO is a horrible ad-hoc design, with the main excuse being \u0026ldquo;other, less gifted people, made that design, and we are implementing it for compatibility because database people - who seldom have any shred of taste - actually use it\u0026rdquo;.\nBut AIO was always really really ugly. Linus Torvalds\nHeads-up! With DIO still need to run fsync() on a file! Let\u0026rsquo;s write an example with golang and iouring-go library:\nTODO Read next chapter → "},{"id":32,"href":"/docs/page-cache/9-advanced-page-cache-observability-and-troubleshooting-tools/","title":"Advanced Page Cache observability and troubleshooting tools","section":"Linux Page Cache series","content":" Advanced Page Cache observability and troubleshooting tools # Last updated: Oct 2025 Contents\neBPF tools Writeback monitor Page Cache Top Cache stat bpftrace and kfunc trace perf tool Let\u0026rsquo;s touch on some advanced tools we can use to perform low-level kernel tracing and debugging.\neBPF tools # First of all, we can use eBPF tools. The [bcc]https://github.com/iovisor/bcc and bpftrace are your friends when you want to get some internal kernel information.\nLet\u0026rsquo;s take a look at some tools which come with it.\nWriteback monitor # $ sudo bpftrace ./writeback.bt Attaching 4 probes... Tracing writeback... Hit Ctrl-C to end. TIME DEVICE PAGES REASON ms 15:01:48 btrfs-1 7355 periodic 0.003 15:01:49 btrfs-1 7355 periodic 0.003 15:01:51 btrfs-1 7355 periodic 0.006 15:01:54 btrfs-1 7355 periodic 0.005 15:01:54 btrfs-1 7355 periodic 0.004 15:01:56 btrfs-1 7355 periodic 0.005 Page Cache Top # 19:49:52 Buffers MB: 0 / Cached MB: 610 / Sort: HITS / Order: descending PID UID CMD HITS MISSES DIRTIES READ_HIT% WRITE_HIT% 66229 vagrant vmtouch 44745 44032 0 50.4% 49.6% 66229 vagrant bash 205 0 0 100.0% 0.0% 66227 root cachetop 17 0 0 100.0% 0.0% 222 dbus dbus-daemon 16 0 0 100.0% 0.0% 317 vagrant tmux: server 4 0 0 100.0% 0.0% Cache stat # [vagrant@archlinux tools]$ sudo ./cachestat HITS MISSES DIRTIES HITRATIO BUFFERS_MB CACHED_MB 10 0 0 100.00% 0 610 4 0 0 100.00% 0 610 4 0 0 100.00% 0 610 21 0 0 100.00% 0 610 624 0 0 100.00% 0 438 2 0 0 100.00% 0 438 4 0 0 100.00% 0 438 0 0 0 0.00% 0 438 19 0 0 100.00% 0 438 0 428 0 0.00% 0 546 28144 16384 0 63.21% 0 610 0 0 0 0.00% 0 610 0 0 0 0.00% 0 610 17 0 0 100.00% 0 610 0 0 0 0.00% 0 610 bpftrace and kfunc trace # Other than that, eBPF and bpftrace have recently got a new great feature named kfunc. Thus, using it, you can trace some kernel functions without kernel debugging information installed.\nIt\u0026rsquo;s still close to experimental functionality, but it looks really promising.\nperf tool # But if you want to go deeper, I have something for you. perf allows you to set up dynamic tracing kernel probes almost at any kernel function. The only issue is the kernel debug information should be installed. Unfortunately, not all distributives provide it and, sometimes, you will need to recompile the kernel manually with some additional flags.\nBut when you get the debug info, you can perform really crazy investigations. For example, if we want to track the major page faults, we can find the kernel function which is in charge (https://elixir.bootlin.com/linux/latest/source and its search for help) and setup a probe:\nperf probe -f \u0026#34;do_read_fault vma-\u0026gt;vm_file-\u0026gt;f_inode-\u0026gt;i_ino\u0026#34; where do_read_fault is our kernel function and vma-\u0026gt;vm_file-\u0026gt;f_inode-\u0026gt;i_ino is an inode number of the file where the major page fault occurs.\nNow you can start recording events:\nperf record -e probe:do_read_fault -ag -- sleep 10 And after 10 seconds, we can grep out the inodes with perf script and bash magic:\nperf script | grep i_ino | cut -d \u0026#39; \u0026#39; -f 1,8| sed \u0026#39;s#i_ino=##g\u0026#39; | sort | uniq -c | sort -rn "}]